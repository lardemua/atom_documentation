<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><link rel="canonical" href="https://lardemua.github.io/atom_documentation/procedures/" />
      <link rel="shortcut icon" href="../img/favicon.ico" />
    <title>Calibration Procedures - ATOM Calibration Framework</title>
    <link rel="stylesheet" href="../css/theme.css" />
    <link rel="stylesheet" href="../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/styles/github.min.css" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "Calibration Procedures";
        var mkdocs_page_input_path = "procedures.md";
        var mkdocs_page_url = "/atom_documentation/procedures/";
      </script>
    
    <script src="../js/jquery-3.6.0.min.js" defer></script>
    <!--[if lt IE 9]>
      <script src="../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/highlight.min.js"></script>
      <script>hljs.initHighlightingOnLoad();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href=".." class="icon icon-home"> ATOM Calibration Framework
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="..">Introduction</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../getting_started/">Getting Started</a>
                </li>
              </ul>
              <ul class="current">
                <li class="toctree-l1 current"><a class="reference internal current" href="./">Calibration Procedures</a>
    <ul class="current">
    <li class="toctree-l2"><a class="reference internal" href="#create-a-calibration-package">Create a calibration package</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#configure-a-calibration-package">Configure a calibration package</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#recording-compressed-images">Recording compressed images</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#throttling-topics">Throttling topics</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#using-a-different-configuration-file">Using a different configuration file</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#using-tfs-instead-of-the-xacro-file">Using tfs instead of the xacro file</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#set-an-initial-estimate">Set an initial estimate</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#visualizing-sensor-fustrums">Visualizing sensor fustrums</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#collect-data">Collect data</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#rgb-camera-labeling">RGB camera labeling</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#3d-lidar-labeling">3D Lidar labeling</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#depth-camera-labeling">Depth camera labeling</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#2d-lidar-labeling">2D Lidar labeling</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#dataset-playback">Dataset playback</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#correcting-3d-lidar-labels">Correcting 3D Lidar labels</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#correcting-depth-labels">Correcting Depth labels</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#calibrate">Calibrate</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#calibrating-intrinsic-parameters">Calibrating intrinsic parameters</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#two-stage-calibration-for-robotic-systems-with-an-anchored-sensor">Two stage calibration for robotic systems with an anchored sensor</a>
    </li>
        </ul>
    </li>
    </ul>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../examples/">Calibration Examples</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../evaluations/">Evaluation Procedures</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../multimedia/">Multimedia</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../acknowledgment/">Acknowledgment</a>
                </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="..">ATOM Calibration Framework</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href=".." class="icon icon-home" alt="Docs"></a> &raquo;</li><li>Calibration Procedures</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>

          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h2 id="calibration-procedures">Calibration procedures</h2>
<ul>
<li><a href="#create-a-calibration-package">Create a calibration package</a></li>
<li><a href="#configure-a-calibration-package">Configure a calibration package</a><ul>
<li><a href="#recording-compressed-images">Recording compressed images</a></li>
<li><a href="#throttling-topics">Throttling topics</a></li>
<li><a href="#using-a-different-configuration-file">Using a different configuration file</a></li>
<li><a href="#using-tfs-instead-of-the-xacro-file">Using tfs instead of the xacro file</a></li>
</ul>
</li>
<li><a href="#set-an-initial-estimate">Set an initial estimate</a><ul>
<li><a href="#visualizing-sensor-fustrums">Visualizing sensor fustrums</a></li>
</ul>
</li>
<li><a href="#collect-data">Collect data</a><ul>
<li><a href="#rgb-camera-labeling">RGB camera labeling</a></li>
<li><a href="#3d-lidar-labeling">3D Lidar labeling</a></li>
<li><a href="#depth-camera-labeling">Depth camera labeling</a></li>
<li><a href="#2d-lidar-labeling">2D Lidar labeling</a></li>
</ul>
</li>
<li><a href="#dataset-playback">Dataset playback</a><ul>
<li><a href="#correcting-3d-lidar-labels">Correcting 3D Lidar labels</a></li>
<li><a href="#correcting-depth-labels">Correcting Depth labels</a></li>
</ul>
</li>
<li><a href="#calibrate">Calibrate</a><ul>
<li><a href="#calibrating-intrinsic-parameters">Calibrating intrinsic parameters</a></li>
<li><a href="#two-stage-calibration-for-robotic-systems-with-an-anchored-sensor">Two stage calibration for robotic systems with an anchored sensor</a></li>
</ul>
</li>
</ul>
<p>To calibrate your robot you must define your robotic system, (e.g. &lt;my_robot>). You should also have a <strong>system
description</strong> in the form of an <a href="http://wiki.ros.org/urdf">urdf</a> or a <a href="http://wiki.ros.org/xacro">xacro</a> file(s). This is normally stored in a ros package named <strong>&lt;my_robot>_description</strong>.
In addition to this, <strong>ATOM</strong> requires a bagfile with a recording of the data from the sensors you wish to calibrate.
This was covered in detail in <a href="..#calibration-pipeline">here</a>.</p>
<p>Transformations in the bagfile (i.e. topics /tf and /tf_static) will be ignored, so that they do not collide with the
ones being published by the <a href="http://wiki.ros.org/robot_state_publisher">robot_state_publisher</a>. Thus, if your robotic system contains moving parts, the bagfile should also record the <a href="http://docs.ros.org/en/lunar/api/sensor_msgs/html/msg/JointState.html">sensor_msgs/JointState</a> message.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>It is also possible to use the transformations in the bagfile instead of using the xacro description and the robot state publisher to produce them. 
See section on <a href="#using-tfs-instead-of-the-xacro-file">using tfs instead of a xacro file</a>.</p>
</div>
<p>To reduce the bag size, it may contain compressed images instead of raw images, since <strong>ATOM</strong> can decompress them while playing back the bagfile. 
Here is an example of a <a href="https://github.com/lardemua/atlascar2/blob/master/atlascar2_bringup/launch/record_sensor_data.launch">launch file</a>
which records compressed images.</p>
<p>We consider this to be part of the normal configuration of your robotic system in ROS, so ATOM assumes this is already done. 
In any case if you need inspiration you can take a look at the <a href="../examples/">calibration examples</a> and how we configured our systems.</p>
<h3 id="create-a-calibration-package">Create a calibration package</h3>
<p>Assuming you have your robotic system setup, you can start creating the calibration package.
You should create a calibration ros package specific for your robotic system. <strong>ATOM</strong> provides a script for this:</p>
<pre><code class="language-bash">rosrun atom_calibration create_calibration_pkg --name &lt;my_robot_calibration&gt;
</code></pre>
<p>This will create the ros package <my_robot_calibration> in the current folder, but you can also specify another folder,
e.g.:</p>
<pre><code class="language-bash">rosrun atom_calibration create_calibration_pkg --name ~/my/path/&lt;my_robot_calibration&gt;
</code></pre>
<h3 id="configure-a-calibration-package">Configure a calibration package</h3>
<p>Once your calibration package is created you will have to configure the calibration procedure by editing the
<em><my_robot_calibration>/calibration/config.yml</em> file with your system information.</p>
<p>The file contains several comments to provide clues on how to configure it.
Here are examples of calibration <strong>config.yml</strong> files for
an <a href="https://github.com/lardemua/atlascar2/blob/master/atlascar2_calibration/calibration/config.yml">autonomous vehicle</a>
and for <a href="https://github.com/miguelriemoliveira/mmtbot/blob/main/mmtbot_calibration/calibration/config.yml">MMTBot</a>, also shown below:</p>
<pre><code class="language-yaml">#
#           █████╗ ████████╗ ██████╗ ███╗   ███╗
#          ██╔══██╗╚══██╔══╝██╔═══██╗████╗ ████║
#          ███████║   ██║   ██║   ██║██╔████╔██║
#          ██╔══██║   ██║   ██║   ██║██║╚██╔╝██║
#   __     ██║  ██║   ██║   ╚██████╔╝██║ ╚═╝ ██║    _
#  / _|    ╚═╝  ╚═╝   ╚═╝    ╚═════╝ ╚═╝     ╚═╝   | |
#  | |_ _ __ __ _ _ __ ___   _____      _____  _ __| | __
#  |  _| '__/ _` | '_ ` _ \ / _ \ \ /\ / / _ \| '__| |/ /
#  | | | | | (_| | | | | | |  __/\ V  V / (_) | |  |   &lt;
#  |_| |_|  \__,_|_| |_| |_|\___| \_/\_/ \___/|_|  |_|\_\
#  https://github.com/lardemua/atom

# This yaml file describes your calibration!

# You can start by defining your robotic system.
# This is the URDF file (or xacro) that describes your robot.
# Every time a path to a file is requested you can use
#
#   - Absolute Path
#       Example 1: /home/user/ros_workspace/your_package/urdf/description.urdf.xacro
#       Example 2: file://home/user/ros_workspace/your_package/urdf/description.urdf.xacro
#
#   - Path Expansion
#       Example 1: ${HOME}/user/${YOUR_VARIABLE}/your_package/urdf/description.urdf.xacro
#       Example 2: ~/user/ros_workspace/your_package/urdf/description.urdf.xacro
#
#       NOTE: It is up to you to guarantee the environment variable exists.
#
#   - ROS Package Reference
#       Example: package://your_package/urdf/description.urdf.xacro
#
description_file: &quot;package://mmtbot_gazebo/urdf/mmtbot.urdf.xacro&quot;
#description_file: &quot;package://mmtbot_gazebo/urdf/mmtbot.urdf.xacro&quot;

# The calibration framework requires a bagfile to extract the necessary data for the calibration.
bag_file: &quot;$ROS_BAGS/mmtbot/test_depth2.bag&quot;

# You must define a frame of reference for the optimization process.
# It must exist in the transformation chains of all the sensors which are being calibrated.
world_link: &quot;world&quot;

# ATOM will calibrate the extrinsic parameters of your sensors.
# In this section you should discriminate the sensors that will be part of the calibrations.
sensors:
  # Each key will define a sensor and its name, which will be use throughout the calibration.
  # Each sensor definition must have the following properties:
  #       link:
  #           The frame of the sensor's data (i.e. the header.frame_id).
  #       parent_link:
  #           The parent link of the transformation (i.e. link) to be calibrated.
  #       child_link:
  #           This is the transformation (i.e. link) that we be optimized.
  #       topic_name:
  #           Name of the ROS topic that contains the data produced by this sensor.
  #           If you are calibrating an camera, you should use the raw image produced by the
  #           sensors. Additionally, it the topic is an image it will automatically use the
  #           respective `camera_info` topic.

  hand_camera:
    link: &quot;hand_camera_rgb_optical_frame&quot;
    parent_link: &quot;flange&quot;
    child_link: &quot;hand_camera_link&quot;
    topic_name: &quot;/hand_camera/rgb/image_raw&quot;
    throttle: 5
    modality: &quot;rgb&quot;

  world_camera_rgb:
    link: &quot;world_camera_rgb_optical_frame&quot;
    parent_link: &quot;world_camera_link&quot;
    child_link: &quot;world_camera_rgb_frame&quot;
    topic_name: &quot;/world_camera/rgb/image_raw&quot;
    modality: &quot;rgb&quot;

  lidar:
    link: &quot;lidar&quot;
    parent_link: &quot;tripod_left_support&quot;
    child_link: &quot;lidar_base_link&quot;
    topic_name: &quot;/lidar/points&quot;
    modality: &quot;lidar3d&quot;

# The calibration requires a detectable pattern.
# This section describes the properties of the calibration pattern used in th calibration.
calibration_pattern:
  # The frame id (or link) of the pattern.
  # This link/transformation will be optimized.
  link: &quot;pattern_link&quot;

  # The parent frame id (or link) of the pattern.
  # For example, in hand-eye calibration the parent link
  # of the pattern can be the end-effector or the base of the arm
  parent_link: &quot;world&quot;

  # Defines if the pattern link is the same in all collections (i.e. fixed=true),
  # or each collection will have its own estimative of the link transformation.
  fixed: false

  # The type of pattern used for the calibration.
  # Supported pattern are: chessboard, charuco
  pattern_type: &quot;charuco&quot;

  # If the pattern type is &quot;charuco&quot; you need to define
  # the aruco dictionary used by the pattern.
  # See https://docs.opencv.org/trunk/dc/df7/dictionary_8hpp.html
  dictionary: &quot;DICT_5X5_100&quot;

  # Mesh file (collada.dae or stl) for showing pattern on rviz. URI or regular path.
  mesh_file: &quot;package://mmtbot_gazebo/models/charuco_800x600/charuco_800x600.dae&quot;

  # The border width from the edge corner to the pattern physical edge.
  # Used for 3D sensors and lidars.
  # It can be a scalar (same border in x and y directions), or it can be {'x': ..., 'y': ,,,}
  border_size: { &quot;x&quot;: 0.04, &quot;y&quot;: 0.03 }

  # The number of corners the pattern has in the X and Y dimensions.
  # Note: The charuco detector uses the number of squares per dimension in its detector.
  # Internally we add a +1 to Y and X dimensions to account for that.
  # Therefore, the number of corners should be used even for the charuco pattern.
  dimension: { &quot;x&quot;: 11, &quot;y&quot;: 8 }

  # The length of the square edge.
  size: 0.06

  # The length of the charuco inner marker.
  inner_size: 0.045

# Miscellaneous configuration

# If your calibration problem is not fully constrained you should anchored one of the sensors.
# This makes it immovable during the optimization.
# This is typically referred to as gauge freedom.
anchored_sensor: &quot;world_camera_rgb&quot;

# Max time delta (in milliseconds) between sensor data messages when creating a collection.
max_duration_between_msgs: 1000

</code></pre>
<p>After filling the config.yml file, you should run the package configuration:</p>
<pre><code class="language-bash">rosrun &lt;my_robot_calibration&gt; configure 
</code></pre>
<p>This will go through a series of verifications to check if the configuration is valid, if the bagfile exists and contains the necessary transformations, among many others.
Once the verifications signal a correct calibration configuration, a set of files is automatically created inside your 
<strong>&lt;my_robot>_calibration</strong> ros package, as shown below:</p>
<pre><code class="language-bash">├&lt;my_robot&gt;_calibration
├── launch
│   ├── calibrate.launch
│   ├── collect_data.launch
│   ├── dataset_playback.launch
│   ├── playbag.launch
│   └── set_initial_estimate.launch
├── rviz
│   ├── calibrate.rviz
│   ├── collect_data.rviz
│   ├── dataset_playback.rviz
│   └── set_initial_estimate.rviz
├── scripts
│   └── configure
└── urdf
    ├── initial_estimate.urdf.xacro
├── calibration
│   ├── config.yml
│   └── summary.pdf

</code></pre>
<p>The <strong>launch</strong> directory contains automatically created launch files used to launch each of the calibration stages.
The <strong>rviz</strong> folder contains several rviz configuration files used to launch the visualization of each stage.
The <strong>urdf</strong> folder contains a symbolic link to the xacro file of &lt;my_robot> and, after calibration, will contain a calibrated urdf. 
Finally, the <strong>calibration</strong> folder contains the configuration file (config.yml). </p>
<p>This folder also contains an automatically produced schematic of the configuration of the calibration named <a href="../img/summary_example.pdf">summary.pdf</a>.
It is used to inspect the configuration of the calibration and assess if the configuration is doing what we intended.
Atomic transformations to be estimated are marked in blue, sensor coordinate systems, i.e. the coordinate systems in which sensors produce their data are marked in green, and the selected world coordinate frame is highlighted in red.</p>
<figure align="center">
<p><img alt="" src="../img/calibration_summary.png" width="120%" />
  </p>
<figcaption align="center">Summary of a calibration configuration (MMTBot).</figcaption>
</figure>
<h5 id="recording-compressed-images">Recording compressed images</h5>
<p>In case the topics from RGB camera(s) are published using the <a href="http://wiki.ros.org/image_transport">ROS image transport</a>, it is possible to record the compressed image topic instead of the raw image topic as in <a href="..#data-logging">this example</a>. During the configuration of your calibration package, ATOM will detect that these are compressed topics and automatically generate a decompressing mechanism in the launch files. </p>
<p>The advantage is that you get much smaller bag files since they contain compressed images.
This may be critical for large systems such as <a href="../examples/#larcc">LARCC</a>, where the data output may reach over 1GB per minute.</p>
<h5 id="throttling-topics">Throttling topics</h5>
<p>For an offline calibration procedure, speed is not critical. Thus, it is possible to configure a <a href="http://wiki.ros.org/topic_tools/throttle">ROS topic throttler</a> for your sensor messages, if they have been recorded in a higher than necessary frequency.
For that just add a field <strong>throttle</strong> to the section of the corresponding sensor in your calibration config.yaml file.</p>
<h5 id="using-a-different-configuration-file">Using a different configuration file</h5>
<p>It is also possible to configure your calibration package with a different configuration file, in the case you have
multiple configurations with multiple config.yml files. There are also other options to run a custom configuration, i.e.:</p>
<pre><code class="language-bash">usage: rosrun atom_calibration configure_calibration_pkg [-h] -n NAME [-utf] [-cfg CONFIG_FILE]

-h, --help            show this help message and exit
-n NAME, --name NAME  package name
-cfg CONFIG_FILE, --config_file CONFIG_FILE
                      Specify if you want to configure the calibration package with a specific configutation file.
                      If this flag is not given, the standard config.yml ill be used.
</code></pre>
<h5 id="using-tfs-instead-of-the-xacro-file">Using tfs instead of the xacro file</h5>
<p>If you prefer ATOM may use the tra
Sometimes it may be preferable to use the transformations in the bagfile instead of the ones produced by the xacro description. 
To do this use tehe --use_tfs option when configuring your package:</p>
<pre><code class="language-bash">usage: rosrun atom_calibration configure_calibration_pkg [-h] -n NAME [-utf] [-cfg CONFIG_FILE]

-h, --help            show this help message and exit
-n NAME, --name NAME  package name
-utf, --use_tfs       Use transformations in the bag file instead of generating new tfs from the xacro,
                      joint_state_msgs and robot state publisher.
</code></pre>
<h3 id="set-an-initial-estimate">Set an initial estimate</h3>
<p>Iterative optimization methods such as ATOM are sensitive to the initial parameter configuration. 
ATOM uses several groups of optimization parameters and uses distinct strategies to initialize each class. 
The most important group, sensor poses, may be manually initialized to ensure that the initial estimate is plausible and thus reduce the likelihood of encountering a local minima.</p>
<p>ATOM provides an interactive framework based on rviz which allows the user to set the pose of the sensors while having immediate visual feedback.
The system configures an <a href="http://wiki.ros.org/rviz/Tutorials/Interactive%20Markers%3A%20Getting%20Started">RViz interactive marker</a> for each sensor to be calibrated, so that it is possible to move the sensor in RViz.</p>
<p>During this procedure one may use several visual clues to better position the sensors, i.e. it is possible to see the CAD model of the sensors and the rest of the structure of the robotic system or one could observe the amount of overlap between sensors.</p>
<p>To set an initial estimate run:</p>
<pre><code class="language-bash">roslaunch &lt;my_robot_calibration&gt; set_initial_estimate.launch 
</code></pre>
<p>Here are a couple of examples of setting the initial estimate:</p>
<figure align="center">
<p><img alt="" src="../img/agrob_initial_estimate.gif" width="100%" />
  </p>
<figcaption align="center">Setting initial estimate of sensor poses in the AgrobV2.</figcaption>
</figure>
<figure align="center">
<p><img alt="" src="../img/set_initial_estimate_atlascar2.gif" width="100%" />
  </p>
<figcaption align="center">Setting initial estimate of sensor poses in the AtlasCar2.</figcaption>
</figure>
<figure align="center">
<p><img alt="" src="../img/ur10e_eye_in_hand_set_initial_estimate.gif" width="100%" />
  </p>
<figcaption align="center">Setting initial estimate of sensor poses in the IRIS UR10e.</figcaption>
</figure>
<h4 id="visualizing-sensor-fustrums">Visualizing sensor fustrums</h4>
<p>ATOM provides a way to visualize the fustrums of RGB and Depth cameras. These may be useful to get a clue about the overlap between sensors, or the ammount of coverage of a work volume. Below you can see the fustrum of two rgb cameras. One of the cameras is positioned on the end-effector of the manipulator, and when it moves, so does its fustrum.</p>
<p>This functionality is only available in the set initial estimate stage.</p>
<figure align="center">
<p><img alt="" src="../img/MMTBot_fustrum.gif" width="100%" />
  </p>
<figcaption align="center">Visualizing fustrum of RGB sensors in the MMTBot.</figcaption>
</figure>
<h3 id="collect-data">Collect data</h3>
<p>To run a system calibration, one requires data from the sensors collected at different time instants. We refer to these snapshots of data as <a href="..#what-is-a-collection">collections</a>, and a set of collections as an <a href="..#what-is-an-atom-dataset">ATOM dataset</a>. </p>
<p>To collect data, use:</p>
<pre><code class="language-bash">roslaunch &lt;my_robot_calibration&gt; collect_data.launch output_folder:=$ATOM_DATASETS/&lt;your_dataset_folder&gt;
</code></pre>
<p>The script launches a fully configured rviz window. The user observes the data playback and <strong>decides when a collection should be saved</strong> by clicking a green sphere in that appears in the scene. </p>
<p>The number of collections required to accurately calibrate a system vary according to the number, modality and positioning of the sensors. 
Empirically we found that a figure around 30 collections is usually sufficient for estimating an accurate calibration.
That's about the same number of images you need when calibrating a stereo system with OpenCV. Of course this highly depends on your system and the amount
of overlap there is between the sensors.</p>
<p>It is also possible to add additional parameters to configure several aspects of the script. See below all the options.</p>
<div class="admonition tip">
<p class="admonition-title">Additional parameters for collect_data.launch</p>
<table>
<thead>
<tr>
<th align="center">Argument</th>
<th align="center">Function</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">overwrite</td>
<td align="center">overwrites previous dataset without asking for confirmation</td>
</tr>
<tr>
<td align="center">bag_rate</td>
<td align="center">Defines the playback rate of the bagfile</td>
</tr>
<tr>
<td align="center">bag_start</td>
<td align="center">Start time for playback</td>
</tr>
<tr>
<td align="center">bag_file</td>
<td align="center">Name of bagfile to playback</td>
</tr>
<tr>
<td align="center">ssl</td>
<td align="center">A string to be evaluated that indicates if a sensor should be labelled.</td>
</tr>
</tbody>
</table>
<p>One example using all the parameters above:</p>
<pre><code>roslaunch &lt;my_robot_calibration&gt; collect_data.launch output_folder:=$ATOM_DATASETS/&lt;your_dataset_folder&gt; overwrite:=true bag_rate:=0.5 bag_start:=10 ssl:='lambda name: name in ["s1", "s2"]'
</code></pre>
</div>
<p>When you launch the data collection script, it automatically starts data labeling processes adequate for each sensor in your robotic system.
As such, the data is being continuously labeled as the bagfile is played.</p>
<p>Depending on the modalidity of the sensors in the system the labeling may be automatic or fully automatic.
Below we detail how each of the labelers operate.</p>
<h4 id="rgb-camera-labeling">RGB camera labeling</h4>
<p>RGB cameras have a fully automatic pattern detection. It uses off the shelf <a href="https://docs.opencv.org/4.x/d9/d0c/group__calib3d.html#ga93efa9b0aa890de240ca32b11253dd4a">chessboard</a> or <a href="https://docs.opencv.org/4.x/df/d4a/tutorial_charuco_detection.html">charuco</a> calibration pattern detectors.
ATOM provides an rviz configuration which subscribes annotated images received from the pattern detectors. You can check if the detection is working 
by observing the overlays of top of the images.</p>
<figure align="center">
<p><img alt="" src="../img/ur10e_eye_to_base_collect_data.gif" width="100%" />
  </p>
<figcaption align="center">Setting the seed point in 2D Lidar data for semi-automatic labeling (AtlasCar2).
</figcaption>
</figure>
<div class="admonition note">
<p class="admonition-title">Use charuco boards</p>
<p>Charuco boards are preferable to chessboard patterns, because of two main reasons: the first is that the charuco detection is more more efficient when compared to the chessboard detection; the second is that the charuco pattern is detected even if it is only partially visible in the image, which is very usefull when the sensors in your system have small overlapping fields of view.</p>
</div>
<h4 id="3d-lidar-labeling">3D Lidar labeling</h4>
<p>3D Lidar labeling is a semi-automatic procedure. The idea is that the user moves an rviz marker close to where the pattern is present in the lidar point cloud.  </p>
<figure align="center">
<p><img alt="" src="../img/MMTBot3DLidarLabeling.gif" width="100%" />
  </p>
<figcaption align="center">Setting the seed point in 3D Lidar data for semi-automatic labeling (MMTBot).</figcaption>
</figure>
<p>After setting this seed position, the system continues to track the patterns pose over the next frames, even if it moves,
as you can see below:</p>
<figure align="center">
<p><img alt="" src="../img/agrob_data_collection.gif" width="100%" />
  </p>
<figcaption align="center">Automatic 3D Lidar labeling and creating an ATOM dataset (AgrobV2).</figcaption>
</figure>
<div class="admonition warning">
<p class="admonition-title">Tracking limitations</p>
<p>The tracking procedure may fail if the pattern is too close to another object, as for example the ground plane. This can be solved by making sure the pattern is sufficiently far from all other objects, or during the dataset playback stage. </p>
</div>
<h4 id="depth-camera-labeling">Depth camera labeling</h4>
<p>The labeling of depth cameras a semi-automatic procedure. It is done by clicking the depth image in rviz. The user should click somewhere inside the pattern, and then the system carries on the tracking of the pattern even if it moves. The user may reset the procedure by reclicking the image.</p>
<figure align="center">
<p><img alt="" src="../img/ATOMDepthLabeling.gif" width="100%" />
  </p>
<figcaption align="center">Labeling of depth data (LARCC).</figcaption>
</figure>
<div class="admonition warning">
<p class="admonition-title">RViz fork required</p>
<p>Learn about this <a href="../getting_started/#clone-rviz-fork">here</a>.</p>
</div>
<h4 id="2d-lidar-labeling">2D Lidar labeling</h4>
<p>The labeling of the 2D Lidars is very similar to the labeling of 3D Lidars. The user sets the seed point where the lidar points are observing the pattern, and then the pattern is tracked.</p>
<figure align="center">
<p><img alt="" src="../img/collect_data_atlascar2.gif" width="100%" />
  </p>
<figcaption align="center">Setting the seed point in 2D Lidar data for semi-automatic labeling (AtlasCar2).</figcaption>
</figure>
<div class="admonition warning">
<p class="admonition-title">May be deprecated</p>
<p>The 2D Lidar semi-automatic labeling was last used in 2019, so it may be deprecated. If you are interested on having this functionality <a href="https://github.com/lardemua/atom/issues">create an issue</a> with a request.</p>
</div>
<h3 id="dataset-playback">Dataset playback</h3>
<p>The dataset playback is used to review and eventually correct the automatic labels produced during the <a href="#collect-data">collection of data</a>.
Ideally, the bulk of the labels produced automatically during the collect data stage should be correct, but a few incorrect annotations will disrupt the calibration. As such, a review of the annotations is recommended. You may skip it if you feel that the automatic labeling went very well.</p>
<p>To run the dataset playback, first launch the visualization:</p>
<pre><code>roslaunch &lt;my_robot_calibration&gt; dataset_playback.launch
</code></pre>
<p>and then the dataset_playback node:</p>
<pre><code>clear &amp;&amp; rosrun atom_calibration dataset_playback -json &lt;my_dataset_file&gt;.json -ow
</code></pre>
<p>This will create a new json file called <my_dataset_file_corrected/>.json. There are other available options listed below:</p>
<pre><code class="language-bash">usage: dataset_playback [-h] -json JSON_FILE [-csf COLLECTION_SELECTION_FUNCTION] [-ow]

optional arguments:
  -h, --help            show this help message and exit
  -json JSON_FILE, --json_file JSON_FILE
                        Json file containing input dataset.
  -csf COLLECTION_SELECTION_FUNCTION, --collection_selection_function COLLECTION_SELECTION_FUNCTION
                        A string to be evaluated into a lambda function that receives a collection name as input and returns True or False to indicate if the collection should be loaded (and used in the optimization). The Syntax is
                        lambda name: f(x), where f(x) is the function in python language. Example: lambda name: int(name) &gt; 5 , to load only collections 6, 7, and onward.
  -ow, --overwrite      Overwrites the data_corrected.json without asking for permission
</code></pre>
<p>Check the <a href="../multimedia/#navigating-collections">video tutorial</a>.</p>
<figure align="center">
<p><img alt="" src="../img/AtomDatasetPlaybackNavigation.gif" width="100%" />
  </p>
<figcaption align="center">Navigating through collections in a dataset (LARCC).</figcaption>
</figure>
<h4 id="correcting-3d-lidar-labels">Correcting 3D Lidar labels</h4>
<p>Correcting 3D Lidar labels is done by selecting points in the point cloud displayed by RViz and pressing keys in order to add these points as pattern points or boundary points.</p>
<figure align="center">
<p><img alt="" src="../img/ATOMDatasetPlaybackCorrecting3DLidarLabels.gif" width="100%" />
  </p>
<figcaption align="center">Correcting 3D Lidar labels in a dataset (LARCC).</figcaption>
</figure>
<p>Check the <a href="../multimedia/#correcting-3d-lidar-labels">video tutorial</a>.</p>
<div class="admonition tip">
<p class="admonition-title">Do not forget to compile your catkin workspace</p>
<p>For selecting points from point clouds, we use an rviz plugin in cpp that must be compiled.</p>
</div>
<h4 id="correcting-depth-labels">Correcting Depth labels</h4>
<p>To correct depth modality labels the user draws a polygon around the pattern in the depth image.</p>
<figure align="center">
<p><img alt="" src="../img/ATOMDatasetPlaybackCorrectingDepthLabels.gif" width="100%" />
  </p>
<figcaption align="center">Correcting depth labels in a dataset (LARCC).</figcaption>
</figure>
<p>Check the <a href="../multimedia/#correcting-depth-labels">video tutorial</a>.</p>
<div class="admonition warning">
<p class="admonition-title">RViz fork required</p>
<p>Learn about this <a href="../getting_started/#clone-rviz-fork">here</a>.</p>
</div>
<h3 id="calibrate">Calibrate</h3>
<p>Finally, a system calibration is called through:</p>
<pre><code class="language-bash">roslaunch &lt;my_robot_calibration&gt; calibrate.launch 
</code></pre>
<p>Then, in a second terminal, run the calibrate script:</p>
<pre><code class="language-bash">rosrun atom_calibration calibrate -json $ATOM_DATASETS/&lt;my_robot_dataset&gt;/dataset_corrected.json -v -rv -si 
</code></pre>
<p>There are several options to use in the calibrate script, one common usage is:</p>
<pre><code class="language-bash">rosrun atom_calibration calibrate -json $ATOM_DATASETS/&lt;my_robot_dataset&gt;/dataset_corrected.json -v -rv -si -uic -csf 'lambda x: int(x)&lt; 5'  -ssf 'lambda name: name in [&quot;camera_2&quot;,&quot;camera_3&quot;]'
</code></pre>
<p>which would run a calibration in verbose mode (-v), using ros visualization (-rv), showing images (-si), using incomplete collections, using collections with index smaller than 5, considering only sensors  camera_2 and camera_3.</p>
<p>You can see all the options listed below:</p>
<pre><code>usage: calibrate [-h] [-vo] -json JSON_FILE [-v] [-rv] [-si] [-oi] [-sr SAMPLE_RESIDUALS] [-ss SAMPLE_SEED]
                 [-slr SAMPLE_LONGITUDINAL_RESIDUALS] [-ajf] [-oas] [-ap] [-uic] [-ias] [-rpd]
                 [-nig translation rotation] [-ssf SENSOR_SELECTION_FUNCTION] [-csf COLLECTION_SELECTION_FUNCTION]
                 [-phased] [-ipg] [-oj OUTPUT_JSON]

optional arguments:
  -h, --help            show this help message and exit
  -vo, --view_optimization
                        ...
  -json JSON_FILE, --json_file JSON_FILE
                        Json file containing input dataset.
  -v, --verbose         Be verbose
  -rv, --ros_visualization
                        Publish ros visualization markers.
  -si, --show_images    shows images for each camera
  -oi, --optimize_intrinsics
                        Adds camera instrinsics to the ptimization
  -sr SAMPLE_RESIDUALS, --sample_residuals SAMPLE_RESIDUALS
                        Samples residuals
  -ss SAMPLE_SEED, --sample_seed SAMPLE_SEED
                        Sampling seed
  -slr SAMPLE_LONGITUDINAL_RESIDUALS, --sample_longitudinal_residuals SAMPLE_LONGITUDINAL_RESIDUALS
                        Samples residuals
  -ajf, --all_joints_fixed
                        Assume all joints are fixed and because of that draw a single robot mesh.Overrides automatic
                        detection of static robot.
  -oas, --only_anchored_sensor
                        Runs optimization only using the anchored sensor and discarding all others.
  -ap, --anchor_patterns
                        Runs optimization without changing the poses of the patterns.
  -uic, --use_incomplete_collections
                        Remove any collection which does not have a detection for all sensors.
  -ias, --ignore_anchored_sensor
                        Ignore the anchored sensor information in the dataset.
  -rpd, --remove_partial_detections
                        Remove detected labels which are only partial.Used or the Charuco.
  -nig translation rotation, --noisy_initial_guess translation rotation
                        Percentage of noise to add to the initial guess atomic transformations set before.
  -ssf SENSOR_SELECTION_FUNCTION, --sensor_selection_function SENSOR_SELECTION_FUNCTION
                        A string to be evaluated into a lambda function that receives a sensor name as input and
                        returns True or False to indicate if the sensor should be loaded (and used in the
                        optimization). The Syntax is lambda name: f(x), where f(x) is the function in python
                        language. Example: lambda name: name in [&quot;left_laser&quot;, &quot;frontal_camera&quot;] , to load only
                        sensors left_laser and frontal_camera
  -csf COLLECTION_SELECTION_FUNCTION, --collection_selection_function COLLECTION_SELECTION_FUNCTION
                        A string to be evaluated into a lambda function that receives a collection name as input and
                        returns True or False to indicate if the collection should be loaded (and used in the
                        optimization). The Syntax is lambda name: f(x), where f(x) is the function in python
                        language. Example: lambda name: int(name) &gt; 5 , to load only collections 6, 7, and onward.
  -phased, --phased_execution
                        Stay in a loop before calling optimization, and in another after calling the optimization.
                        Good for debugging.
  -ipg, --initial_pose_ghost
                        Draw a ghost mesh with the systems initial pose. Good for debugging.
  -oj OUTPUT_JSON, --output_json OUTPUT_JSON
                        Full path to output json file.
</code></pre>
<p>If you use the --verbose option, the script will periodically print a table containing information about the errors per sensor and per collection, e.g.:</p>
<pre><code class="language-bash">Errors per collection (anchored sensor,  max error per sensor, not detected as &quot;---&quot;)
+------------+----------+----------+----------+----------------+---------+---------+---------+
| Collection | camera_2 | camera_3 | camera_4 | depth_camera_1 | lidar_1 | lidar_2 | lidar_3 |
+------------+----------+----------+----------+----------------+---------+---------+---------+
|     11     | 191.1838 |   ---    |  0.3000  |     0.1321     |  0.2606 |  0.1153 |  1.4850 |
|     12     | 194.3039 | 98.7729  |  0.4031  |     0.1591     |  0.1043 |  0.0924 |  0.5050 |
|     13     |   ---    | 94.6346  |  0.3795  |     0.1942     |  0.1657 |  0.1191 |  0.4210 |
|     14     | 199.6314 |   ---    |  0.4001  |     0.1569     |  0.1897 |  0.0685 |  0.7067 |
|     15     | 199.8989 |   ---    |  0.5995  |     0.1561     |  0.1956 |  0.0670 |  0.7074 |
|     16     |   ---    | 146.3350 |  0.2370  |     0.1516     |  0.4412 |  0.1316 |  0.3175 |
|     17     |   ---    | 150.6509 |  0.2204  |     0.1924     |  0.1747 |  0.0298 |  0.4698 |
|     18     | 202.8877 |   ---    |  0.5371  |     0.0978     |  0.2200 |  0.1526 |  0.8007 |
|     19     | 211.7066 | 133.3410 |  0.3598  |     0.2179     |  0.0909 |  0.0367 |  0.6065 |
|     20     | 213.7645 |   ---    |  0.5272  |     0.2152     |  0.2647 |  0.2008 |  0.7670 |
|     21     | 212.6482 |   ---    |  0.6358  |     0.2444     |  0.3512 |  0.0518 |  1.7215 |
|     22     | 212.3743 |   ---    |  0.5967  |     0.2321     |  0.3342 |  0.0591 |  0.7347 |
|     23     | 209.1305 |   ---    |  0.4488  |     0.1161     |  0.1601 |  0.0431 |  0.7198 |
|     24     | 201.1378 | 139.1949 |  0.2539  |     0.1802     |  0.1114 |  0.0517 |  0.5524 |
|     25     |   ---    | 139.2066 |  0.2994  |     0.0869     |  0.4260 |  0.1768 |  0.1162 |
|     26     | 209.7005 |   ---    |  0.4192  |     0.1315     |  0.1400 |  0.0443 |  0.6932 |
|     27     | 210.5517 |   ---    |  0.4042  |     0.2189     |  0.2926 |  0.0495 |  0.7068 |
|     28     |   ---    | 142.4085 |  0.2628  |     0.1500     |  0.2662 |  0.1419 |  0.3405 |
|  Averages  | 205.3015 | 130.5680 |  0.4047  |     0.1685     |  0.2327 |  0.0907 |  0.6873 |
+------------+----------+----------+----------+----------------+---------+---------+---------+
</code></pre>
<p>Here's an example of a system being calibrated.</p>
<figure align="center">
<p><img alt="" src="../img/agrob_calibration.gif" width="100%" />
  </p>
<figcaption align="center">Calibration of AgrobV2.</figcaption>
</figure>
<h4 id="calibrating-intrinsic-parameters">Calibrating intrinsic parameters</h4>
<p>ATOM also supports intrinsic camera calibration (for now just RGB modality), but requires a first guess for these parameters. </p>
<p>We compute a first guess using the <a href="http://wiki.ros.org/camera_calibration/Tutorials/MonocularCalibration">ROS monocular camera calibration</a> for rgb cameras, and the <a href="http://wiki.ros.org/openni_launch/Tutorials/IntrinsicCalibration">depth camera intrinsic calibration</a> for depth cameras. After these calibrations are carried out, the sensor drivers publish calibrated <a href="http://docs.ros.org/en/noetic/api/sensor_msgs/html/msg/CameraInfo.html">camera_info messages</a> which are saved to the bag files that is used for calibration, and then used a as first guess for ATOM's intrinsic parameter estimation. Intrinsic calibration runs simultaneous with the extrinsic calibration. </p>
<h4 id="two-stage-calibration-for-robotic-systems-with-an-anchored-sensor">Two stage calibration for robotic systems with an anchored sensor</h4>
<p>When one sensor is set to be anchored in the calibration/config.yml file, i.e. this <a href="https://github.com/lardemua/atlascar2/blob/6850dfe2209e3f5e9c7a3ca66a2b98054ebed256/atlascar2_calibration/calibration/config.yml#L99">file</a> for the AtlaCar2, we recommend a two stage procedure to achieve a more accurate calibration:</p>
<p>First, run a calibration using parameter <strong>--only_anchored_sensor</strong> (<strong>-oas</strong>) which will exclude from the optimization all sensors which are not the anchored one. This optimization will position the patterns correctly w.r.t. the anchored sensor. For example:</p>
<pre><code>rosrun atom_calibration calibrate -json $ATOM_DATASETS/larcc_real/ dataset_train/dataset_corrected.json -uic -nig 0.0 0.0 -ipg -si -rv -v -oas
</code></pre>
<p>The output is stored in the <strong>atom_calibration.json</strong>, which is used and the input for the second stage, where all sensors are used. In this second stage the poses of the patterns are frozen using the parameter <strong>--anchor_patterns</strong> (<strong>-ap</strong>). To avoid overwriting atom_calibration.json, you should also define the output json file (<strong>-oj</strong>). For example:</p>
<pre><code>rosrun atom_calibration calibrate -json $ATOM_DATASETS/larcc_real/ dataset_train/atom_calibration.json -uic -nig 0.0 0.0 -ipg -si -rv -v -ap -oj atom_anchored_calibration.json
</code></pre>
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="../getting_started/" class="btn btn-neutral float-left" title="Getting Started"><span class="icon icon-circle-arrow-left"></span> Previous</a>
        <a href="../examples/" class="btn btn-neutral float-right" title="Calibration Examples">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
      <span><a href="../getting_started/" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
      <span><a href="../examples/" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script>var base_url = '..';</script>
    <script src="../js/theme_extra.js" defer></script>
    <script src="../js/theme.js" defer></script>
      <script src="../search/main.js" defer></script>
    <script defer>
        window.onload = function () {
            SphinxRtdTheme.Navigation.enable(true);
        };
    </script>

</body>
</html>
