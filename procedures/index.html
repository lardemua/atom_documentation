<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><link rel="canonical" href="https://lardemua.github.io/atom_documentation/procedures/" />
      <link rel="shortcut icon" href="../img/favicon.ico" />
    <title>Calibration Procedures - ATOM Calibration Framework</title>
    <link rel="stylesheet" href="../css/theme.css" />
    <link rel="stylesheet" href="../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/styles/github.min.css" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "Calibration Procedures";
        var mkdocs_page_input_path = "procedures.md";
        var mkdocs_page_url = "/atom_documentation/procedures/";
      </script>
    
    <script src="../js/jquery-3.6.0.min.js" defer></script>
    <!--[if lt IE 9]>
      <script src="../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/highlight.min.js"></script>
      <script>hljs.initHighlightingOnLoad();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href=".." class="icon icon-home"> ATOM Calibration Framework
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="..">Introduction</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../getting_started/">Getting Started</a>
                </li>
              </ul>
              <ul class="current">
                <li class="toctree-l1 current"><a class="reference internal current" href="./">Calibration Procedures</a>
    <ul class="current">
    <li class="toctree-l2"><a class="reference internal" href="#create-a-calibration-package">Create a calibration package</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#configure-a-calibration-package">Configure a calibration package</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#set-an-initial-estimate">Set an initial estimate</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#visualizing-sensor-fustrums">Visualizing sensor fustrums</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#collect-data">Collect data</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#rgb-camera-labeling">RGB camera labeling</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#3d-lidar-labeling">3D Lidar labeling</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#depth-camera-labeling">Depth camera labeling</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#2d-lidar-labeling">2D Lidar labeling</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#dataset-playback">Dataset playback</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#calibrate">Calibrate</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#advanced-usage-running-calibration-script-in-separate-terminal">Advanced usage - running calibration script in separate terminal</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#advanced-usage-two-stage-calibration-for-robotic-systems-with-an-anchored-sensor">Advanced usage - two stage calibration for robotic systems with an anchored sensor</a>
    </li>
        </ul>
    </li>
    </ul>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../examples/">Calibration Examples</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../evaluations/">Evaluation Procedures</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../multimedia/">Multimedia</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../acknowledgment/">Acknowledgment</a>
                </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="..">ATOM Calibration Framework</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href=".." class="icon icon-home" alt="Docs"></a> &raquo;</li><li>Calibration Procedures</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>

          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h2 id="calibration-procedures">Calibration procedures</h2>
<p>To calibrate your robot you must define your robotic system, (e.g. &lt;your_robot>). You should also have a <strong>system
description</strong> in the form of an <a href="http://wiki.ros.org/urdf">urdf</a> or a <a href="http://wiki.ros.org/xacro">xacro</a> file(s). This is normally stored in a ros package named <strong>&lt;your_robot>_description</strong>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>We recommend using xacro files instead of urdfs. </p>
</div>
<p>Finally, <strong>ATOM</strong> requires a bagfile with a recording of the data from the sensors you wish to calibrate.</p>
<p>Transformations in the bagfile (i.e. topics /tf and /tf_static) will be ignored, so that they do not collide with the
ones being published by the <a href="http://wiki.ros.org/robot_state_publisher">robot_state_publisher</a>. Thus, if your robotic system contains moving parts, the bagfile should also record the <a href="http://docs.ros.org/en/lunar/api/sensor_msgs/html/msg/JointState.html">sensor_msgs/JointState</a> message.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>It is also possible to use the transformations in the bagfile instead of using the xacro description and the robot state publisher to produce them. 
See section on <a href="#configuring-a-calibration-package">configuring a calibration package</a>.</p>
</div>
<p>To reduce the bag size, it may contain compressed images instead of raw images, since <strong>ATOM</strong> can decompress them while playing back the bagfile. 
Here is an example of a <a href="https://github.com/lardemua/atlascar2/blob/master/atlascar2_bringup/launch/record_sensor_data.launch">launch file</a>
which records compressed images.</p>
<p>We consider this to be part of the normal configuration of your robotic system in ROS, so ATOM assumes this is already done. 
In any case if you need inspiration you can take a look at the <a href="../examples/">calibration examples</a> and how we configured our systems.</p>
<h3 id="create-a-calibration-package">Create a calibration package</h3>
<p>Assuming you have your robotic system setup, you can start creating the calibration package.
You should create a calibration ros package specific for your robotic system. <strong>ATOM</strong> provides a script for this:</p>
<pre><code class="language-bash">rosrun atom_calibration create_calibration_pkg --name &lt;your_robot_calibration&gt;
</code></pre>
<p>This will create the ros package <your_robot_calibration> in the current folder, but you can also specify another folder,
e.g.:</p>
<pre><code class="language-bash">rosrun atom_calibration create_calibration_pkg --name ~/my/path/&lt;your_robot_calibration&gt;
</code></pre>
<h3 id="configure-a-calibration-package">Configure a calibration package</h3>
<p>Once your calibration package is created you will have to configure the calibration procedure by editing the
<em><your_robot_calibration>/calibration/config.yml</em> file with your system information.</p>
<p>Here are examples of calibration <strong>config.yml</strong> files for
an <a href="https://github.com/lardemua/atlascar2/blob/master/atlascar2_calibration/calibration/config.yml">autonomous vehicle</a>
and for
a <a href="https://github.com/miguelriemoliveira/mmtbot/blob/main/mmtbot_calibration/calibration/config.yml">simulated hand eye system</a>.
Also, the file contains several comments to provide clues on how to configure it.</p>
<p>After filling the config.yml file, you should run the package configuration:</p>
<pre><code class="language-bash">rosrun &lt;your_robot_calibration&gt; configure 
</code></pre>
<p>This will go through a series of varifications, and create a set of files for launching the system, configuring rviz,
etc.</p>
<p>It is also possible to configure your calibration package with a different configuration file, in the case you have
multiple configurations with multiple config.yml files. There are also other options to run a custom configuration, i.e.:</p>
<pre><code class="language-bash">usage: rosrun atom_calibration configure_calibration_pkg [-h] -n NAME [-utf] [-cfg CONFIG_FILE]

-h, --help            show this help message and exit
-n NAME, --name NAME  package name
-utf, --use_tfs       Use transformations in the bag file instead of generating new tfs from the xacro,
                      joint_state_msgs and robot state publisher.
-cfg CONFIG_FILE, --config_file CONFIG_FILE
                      Specify if you want to configure the calibration package with a specific configutation file.
                      If this flag is not given, the standard config.yml ill be used.
</code></pre>
<h3 id="set-an-initial-estimate">Set an initial estimate</h3>
<p>Iterative optimization methods are often sensitive to the initial parameter configuration. There are several optimization parameters. 
However, the ones we refer to in this case are those that represent the poses of each sensor. <strong>ATOM</strong> provides an interactive framework based on rviz which allows the
user to set the pose of the sensors while having immediate visual feedback.</p>
<p>To set an initial estimate run:</p>
<pre><code class="language-bash">roslaunch &lt;your_robot_calibration&gt; set_initial_estimate.launch 
</code></pre>
<p>Here are a couple of examples of setting the initial estimate:</p>
<figure align="center">
<p><img alt="" src="../img/agrob_initial_estimate.gif" width="100%" />
  </p>
<figcaption align="center">Setting initial estimate of sensor poses in the AgrobV2.</figcaption>
</figure>
<figure align="center">
<p><img alt="" src="../img/set_initial_estimate_atlascar2.gif" width="100%" />
  </p>
<figcaption align="center">Setting initial estimate of sensor poses in the AtlasCar2.</figcaption>
</figure>
<figure align="center">
<p><img alt="" src="../img/ur10e_eye_in_hand_set_initial_estimate.gif" width="100%" />
  </p>
<figcaption align="center">Setting initial estimate of sensor poses in the IRIS UR10e.</figcaption>
</figure>
<h4 id="visualizing-sensor-fustrums">Visualizing sensor fustrums</h4>
<p>ATOM provides a way to visualize the fustrums of RGB and Depth cameras. These may be useful to get a clue about the overlap between sensors, or the ammount of coverage of a work volume. Below you can see the fustrum of two rgb cameras. One of the cameras is positioned on the end-effector of the manipulator, and when it moves, so does its fustrum.</p>
<figure align="center">
<p><img alt="" src="../img/MMTBot_fustrum.gif" width="100%" />
  </p>
<figcaption align="center">Visualizing fustrum of RGB sensors in the MMTBot.</figcaption>
</figure>
<h3 id="collect-data">Collect data</h3>
<p>To run a system calibration, one requires data from the sensors collected at different time instants. We refer to these snapshots of data as <a href="concepts.md#collections">collections</a>, and a set of collections as an <a href="concepts.md#atom-datasets">ATOM dataset</a>. </p>
<p>To collect data, use:</p>
<pre><code class="language-bash">roslaunch &lt;your_robot_calibration&gt; collect_data.launch output_folder:=$ATOM_DATASETS/&lt;your_dataset_folder&gt;
</code></pre>
<p>The script launches an rviz window already configured. The user observes the data playback and <strong>decides when a collection should be saved</strong> by clicking a green sphere in that appears in the scene.</p>
<p>It is also possible to add additional parameters to configure several aspects of the script. See below all the options.</p>
<div class="admonition tip">
<p class="admonition-title">Additional parameters for collect_data.launch</p>
<table>
<thead>
<tr>
<th align="center">Argument</th>
<th align="center">Function</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">overwrite</td>
<td align="center">overwrites previous dataset without asking for confirmation</td>
</tr>
<tr>
<td align="center">bag_rate</td>
<td align="center">Defines the playback rate of the bagfile</td>
</tr>
<tr>
<td align="center">bag_start</td>
<td align="center">Start time for playback</td>
</tr>
<tr>
<td align="center">bag_file</td>
<td align="center">Name of bagfile to playback</td>
</tr>
<tr>
<td align="center">ssl</td>
<td align="center">A string to be evaluated that indicates if a sensor should be labelled.</td>
</tr>
</tbody>
</table>
<p>One example using all the parameters above:</p>
<pre><code>roslaunch &lt;your_robot_calibration&gt; collect_data.launch output_folder:=$ATOM_DATASETS/&lt;your_dataset_folder&gt; overwrite:=true bag_rate:=0.5 bag_start:=10 ssl:='lambda name: name in ["s1", "s2"]'
</code></pre>
</div>
<p>When you launch the data collection script, it automatically starts data labeling processes adequate for each sensor in your robotic system.
As such, the data is being continuously labeled as the bagfile is played.</p>
<p>Depending on the modalidity of the sensors in the system the labelling may be automatic or fully automatic.
Below we detail how each of the labelers operate.</p>
<h4 id="rgb-camera-labeling">RGB camera labeling</h4>
<p>RGB cameras have a fully automatic pattern detection. It uses off the shelf <a href="https://docs.opencv.org/4.x/d9/d0c/group__calib3d.html#ga93efa9b0aa890de240ca32b11253dd4a">chessboard</a> or <a href="https://docs.opencv.org/4.x/df/d4a/tutorial_charuco_detection.html">charuco</a> calibration pattern detectors.
ATOM provides an rviz configuration which subscribes annotated images received from the pattern detectors. You can check if the detection is working 
by observing the overlays of top of the images.</p>
<figure align="center">
<p><img alt="" src="../img/ur10e_eye_to_base_collect_data.gif" width="100%" />
  </p>
<figcaption align="center">Setting the seed point in 2D Lidar data for semi-automatic labeling (AtlasCar2).
</figcaption>
</figure>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Charuco boards are preferable to chessboard patterns, because of two main reasons: the first is that the charuco detection is more more efficient when compared to the chessboard detection; the second is that the charuco pattern is detected even if it is only partially visible in the image, which is very usefull when the sensors in your system have small overlapping fields of view.</p>
</div>
<h4 id="3d-lidar-labeling">3D Lidar labeling</h4>
<p>3D Lidar labelling is a semi-automatic procedure. The idea is that the user moves an rviz marker close to where the pattern is present in the lidar point cloud.  </p>
<figure align="center">
<p><img alt="" src="../img/MMTBot3DLidarLabeling.gif" width="100%" />
  </p>
<figcaption align="center">Setting the seed point in 3D Lidar data for semi-automatic labeling (MMTBot).</figcaption>
</figure>
<p>After setting this seed position, the system continues to track the patterns pose over the next frames, even if it moves,
as you can see below:</p>
<figure align="center">
<p><img alt="" src="../img/agrob_data_collection.gif" width="100%" />
  </p>
<figcaption align="center">Automatic 3D Lidar labeling and creating an ATOM dataset (AgrobV2).</figcaption>
</figure>
<div class="admonition warning">
<p class="admonition-title">Tracking limitations</p>
<p>The tracking procedure may fail if the pattern is too close to another object, as for example the ground plane. This can be solved by making sure the pattern is sufficiently far from all other objects, or during the dataset playback stage. </p>
</div>
<h4 id="depth-camera-labeling">Depth camera labeling</h4>
<p>The labeling of depth cameras a semi-automatic procedure. It is done by clicking the depth image in rviz. The user should click somewhere inside the pattern, and then the system carries on the tracking of the pattern even if it moves. The user may reset the procedure by reclicking the image.</p>
<figure align="center">
<p><img alt="" src="../img/ATOMDepthLabeling.gif" width="100%" />
  </p>
<figcaption align="center">Labeling of depth data (LARCC).</figcaption>
</figure>
<div class="admonition warning">
<p class="admonition-title">RViz fork required</p>
<p>This functionality is only available using a special RViz fork at:</p>
<p><a href="https://github.com/miguelriemoliveira/rviz">https://github.com/miguelriemoliveira/rviz</a> </p>
<p>which extends the image display to suport mouse clicking. We are working on integrating this in the RViz main branch, but this is not available yet.
More information here:</p>
<p><a href="https://github.com/ros-visualization/rviz/issues/916">https://github.com/ros-visualization/rviz/issues/916</a></p>
<p><a href="https://github.com/ros-visualization/rviz/pull/1737">https://github.com/ros-visualization/rviz/pull/1737</a></p>
</div>
<h4 id="2d-lidar-labeling">2D Lidar labeling</h4>
<p>The labeling of the 2D Lidars is very similar to the labeling of 3D Lidars. The user sets the seed point where the lidar points are observing the pattern, and then the pattern is tracked.</p>
<figure align="center">
<p><img alt="" src="../img/collect_data_atlascar2.gif" width="100%" />
  </p>
<figcaption align="center">Setting the seed point in 2D Lidar data for semi-automatic labeling (AtlasCar2).</figcaption>
</figure>
<div class="admonition warning">
<p class="admonition-title">May be deprecated</p>
<p>The 2D Lidar semi-automatic labeling was last used in 2019, so it may be deprecated. If you are interested on having this functionality create an issue with a request.</p>
</div>
<h3 id="dataset-playback">Dataset playback</h3>
<p>The dataset playback is used to review and eventually correct the automatic labels produced during the <a href="#collect-data">collection of data</a>.
Ideally, the bulk of the annotations should be correct, but a few incorrect labels will disruot the calibration. As such, a review of the annotations is recommended by default.</p>
<p>To run the dataset playback, first launch the visualization:</p>
<pre><code>roslaunch &lt;your_robot_calibration&gt; dataset_playback.launch
</code></pre>
<p>and then s</p>
<pre><code>clear &amp;&amp; rosrun atom_calibration dataset_playback -json $ATOM_DATASETS/larcc/larcc_real/test_dataset/dataset_corrected.json -csf "lambda x: int(x) &lt;= 55" -uic -si  -ow
</code></pre>
<div class="admonition warning">
<p class="admonition-title">RViz fork required</p>
<p>This functionality is only available using a special RViz fork at:</p>
<p><a href="https://github.com/miguelriemoliveira/rviz">https://github.com/miguelriemoliveira/rviz</a> </p>
<p>which extends the image display to suport mouse clicking. We are working on integrating this in the RViz main branch, but this is not available yet.
More information here:</p>
<p><a href="https://github.com/ros-visualization/rviz/issues/916">https://github.com/ros-visualization/rviz/issues/916</a></p>
<p><a href="https://github.com/ros-visualization/rviz/pull/1737">https://github.com/ros-visualization/rviz/pull/1737</a></p>
</div>
<h3 id="calibrate">Calibrate</h3>
<p>Finally, a system calibration is called through:</p>
<pre><code class="language-bash">roslaunch &lt;your_robot_calibration&gt; calibrate.launch dataset_file:=~/datasets/&lt;my_dataset&gt;/dataset.json 
</code></pre>
<p>You can use a couple of launch file arguments to configure the calibration procedure, as seen below:</p>
<div class="admonition tip">
<p class="admonition-title">Additional parameters for calibrate.launch</p>
<table>
<thead>
<tr>
<th align="center">Argument</th>
<th align="center">Function</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">use_incomplete_collections</td>
<td align="center">Remove collections which do not have a detection for all sensors</td>
</tr>
<tr>
<td align="center">ssf</td>
<td align="center">A string to be evaluated into a lambda function that receives  a sensor <br> name as input and returns True or False to indicate if the sensor <br>should be used in the optimization</td>
</tr>
<tr>
<td align="center">csf</td>
<td align="center">A string to be evaluated into a lambda function that receives a <br>collection name as input and returns True or False to indicate <br>if that collection should be used in the optimization.</td>
</tr>
</tbody>
</table>
<p>One example using all the parameters above:</p>
<pre><code>roslaunch &lt;your_robot_calibration&gt; calibrate.launch dataset_file:=$ATOM_DATASETS/&lt;my_dataset&gt;/dataset.json  use_incomplete_collections:=true ssf:='lambda name: name in ["camera1, "lidar2"]' csf:='lambda name: int(name) &lt; 7'
</code></pre>
</div>
<h5 id="advanced-usage-running-calibration-script-in-separate-terminal">Advanced usage - running calibration script in separate terminal</h5>
<p>Alternatively, for debugging the calibrate script it is better not to have it executed with a bunch of other scripts
which is what happens when you call the launch file. You can run everything with the launch excluding without the
calibrate script using the <strong>run_calibration:=false</strong> option, e.g.:</p>
<pre><code class="language-bash">roslaunch &lt;your_robot_calibration&gt; calibrate.launch dataset_file:=~/datasets/&lt;my_dataset&gt;/dataset.json run_calibration:=false 
</code></pre>
<p>and then launch the calibrate script in standalone mode:</p>
<pre><code class="language-bash">rosrun atom_calibration calibrate -json dataset_file:=~/datasets/&lt;my_dataset&gt;/dataset.json 
</code></pre>
<p>There are several additional command line arguments to use with the <strong>calibrate</strong> script, run calibrate --help to get the complete list:</p>
<pre><code class="language-bash">usage: calibrate [-h] [-sv SKIP_VERTICES] [-z Z_INCONSISTENCY_THRESHOLD]
                 [-vpv] [-vo] -json JSON_FILE [-v] [-rv] [-si] [-oi] [-pof]
                 [-sr SAMPLE_RESIDUALS] [-ss SAMPLE_SEED] [-od] [-fec] [-uic]
                 [-rpd] [-ssf SENSOR_SELECTION_FUNCTION]
                 [-csf COLLECTION_SELECTION_FUNCTION]

optional arguments:
  -h, --help            show this help message and exit
  -json JSON_FILE, --json_file JSON_FILE
                        Json file containing input dataset.
  -vo, --view_optimization
                        Displays generic total error and residuals graphs.
  -v, --verbose         Be verbose
  -rv, --ros_visualization
                        Publish ros visualization markers.
  -si, --show_images    shows images for each camera
  -oi, --optimize_intrinsics
                        Adds camera instrinsics and distortion to the optimization
  -sr SAMPLE_RESIDUALS, --sample_residuals SAMPLE_RESIDUALS
                        Samples residuals
  -ss SAMPLE_SEED, --sample_seed SAMPLE_SEED
                        Sampling seed
  -uic, --use_incomplete_collections
                        Remove any collection which does not have a detection
                        for all sensors.
  -rpd, --remove_partial_detections
                        Remove detected labels which are only partial. Used or
                        the Charuco.
  -ssf SENSOR_SELECTION_FUNCTION, --sensor_selection_function SENSOR_SELECTION_FUNCTION
                        A string to be evaluated into a lambda function that
                        receives a sensor name as input and returns True or
                        False to indicate if the sensor should be loaded (and
                        used in the optimization). The Syntax is lambda name:
                        f(x), where f(x) is the function in python language.
                        Example: lambda name: name in [&quot;left_laser&quot;,
                        &quot;frontal_camera&quot;] , to load only sensors left_laser
                        and frontal_camera
  -csf COLLECTION_SELECTION_FUNCTION, --collection_selection_function COLLECTION_SELECTION_FUNCTION
                        A string to be evaluated into a lambda function that
                        receives a collection name as input and returns True
                        or False to indicate if the collection should be
                        loaded (and used in the optimization). The Syntax is
                        lambda name: f(x), where f(x) is the function in
                        python language. Example: lambda name: int(name) &gt; 5 ,
                        to load only collections 6, 7, and onward.
</code></pre>
<p>It is also possible to call some of these through the launch file. Check the launch file to see how.</p>
<h5 id="advanced-usage-two-stage-calibration-for-robotic-systems-with-an-anchored-sensor">Advanced usage - two stage calibration for robotic systems with an anchored sensor</h5>
<p>When one sensor is set to be anchored in the calibration/config.yml file, i.e. this <a href="https://github.com/lardemua/atlascar2/blob/6850dfe2209e3f5e9c7a3ca66a2b98054ebed256/atlascar2_calibration/calibration/config.yml#L99">file</a> for the AtlaCar2, we recommend a two stage procedure to achieve a more accurate calibration:</p>
<p>First, run a calibration using parameter <strong>--only_anchored_sensor</strong> (<strong>-oas</strong>) which will exclude from the optimization all sensors which are not the anchored one. This optimization will position the patterns correctly w.r.t. the anchored sensor. For example:</p>
<pre><code>rosrun atom_calibration calibrate -json $ATOM_DATASETS/larcc_real/ dataset_train/dataset_corrected.json -uic -nig 0.0 0.0 -ipg -si -rv -v -oas
</code></pre>
<p>The output is stored in the <strong>atom_calibration.json</strong>, which is used and the input for the second stage, where all sensors are used. In this second stage the poses of the patterns are frozen using the parameter <strong>--anchor_patterns</strong> (<strong>-ap</strong>). To avoid overwritting atom_calibration.json, you should also define the output json file (<strong>-oj</strong>). For example:</p>
<pre><code>rosrun atom_calibration calibrate -json $ATOM_DATASETS/larcc_real/ dataset_train/atom_calibration.json -uic -nig 0.0 0.0 -ipg -si -rv -v -ap -oj atom_anchored_calibration.json
</code></pre>
<figure align="center">
<p><img alt="" src="../img/agrob_calibration.gif" width="100%" />
  </p>
<figcaption align="center">Calibration of AgrobV2.</figcaption>
</figure>
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="../getting_started/" class="btn btn-neutral float-left" title="Getting Started"><span class="icon icon-circle-arrow-left"></span> Previous</a>
        <a href="../examples/" class="btn btn-neutral float-right" title="Calibration Examples">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
      <span><a href="../getting_started/" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
      <span><a href="../examples/" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script>var base_url = '..';</script>
    <script src="../js/theme_extra.js" defer></script>
    <script src="../js/theme.js" defer></script>
      <script src="../search/main.js" defer></script>
    <script defer>
        window.onload = function () {
            SphinxRtdTheme.Navigation.enable(true);
        };
    </script>

</body>
</html>
