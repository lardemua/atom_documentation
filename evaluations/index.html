<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><link rel="canonical" href="https://lardemua.github.io/atom_documentation/evaluations/" />
      <link rel="shortcut icon" href="../img/favicon.ico" />
    <title>Evaluation Procedures - ATOM Calibration Framework</title>
    <link rel="stylesheet" href="../css/theme.css" />
    <link rel="stylesheet" href="../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/styles/github.min.css" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "Evaluation Procedures";
        var mkdocs_page_input_path = "evaluations.md";
        var mkdocs_page_url = "/atom_documentation/evaluations/";
      </script>
    
    <script src="../js/jquery-3.6.0.min.js" defer></script>
    <!--[if lt IE 9]>
      <script src="../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/highlight.min.js"></script>
      <script>hljs.initHighlightingOnLoad();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href=".." class="icon icon-home"> ATOM Calibration Framework
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="..">Introduction</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../getting_started/">Getting Started</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../procedures/">Calibration Procedures</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../examples/">Calibration Examples</a>
                </li>
              </ul>
              <ul class="current">
                <li class="toctree-l1 current"><a class="reference internal current" href="./">Evaluation Procedures</a>
    <ul class="current">
    <li class="toctree-l2"><a class="reference internal" href="#annotation-of-rgb-images">Annotation of rgb images</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#full-evaluation">Full Evaluation</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#individual-evaluation">Individual Evaluation</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#lidar-to-depth-evaluation">LiDAR to Depth evaluation</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#lidar-to-lidar-evaluation">LiDAR to LiDAR evaluation</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#lidar-to-rgb-evaluation">LiDAR to RGB evaluation</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#depth-to-depth-evaluation">Depth to Depth evaluation</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#depth-to-rgb-evaluation">Depth to RGB evaluation</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#rgb-to-rgb-evaluation">RGB to RGB evaluation</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#point-cloud-image-projection">Point cloud image projection</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#ground-truth-frame-evaluation">Ground truth frame evaluation</a>
    </li>
        </ul>
    </li>
    </ul>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../multimedia/">Multimedia</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../acknowledgment/">Acknowledgment</a>
                </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="..">ATOM Calibration Framework</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href=".." class="icon icon-home" aria-label="Docs"></a> &raquo;</li>
      <li>Evaluation Procedures</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h2 id="evaluation-procedures">Evaluation Procedures</h2>
<ul>
<li><a href="#evaluation-procedures">Evaluation Procedures</a><ul>
<li><a href="#annotation-of-rgb-images">Annotation of rgb images</a></li>
</ul>
</li>
<li><a href="#full-evaluation">Full Evaluation</a></li>
<li><a href="#individual-evaluation">Individual Evaluation</a><ul>
<li><a href="#lidar-to-depth-evaluation">LiDAR to Depth evaluation</a></li>
<li><a href="#lidar-to-lidar-evaluation">LiDAR to LiDAR evaluation</a></li>
<li><a href="#lidar-to-rgb-evaluation">LiDAR to RGB evaluation</a></li>
<li><a href="#depth-to-depth-evaluation">Depth to Depth evaluation</a></li>
<li><a href="#depth-to-rgb-evaluation">Depth to RGB evaluation</a></li>
<li><a href="#rgb-to-rgb-evaluation">RGB to RGB evaluation</a></li>
<li><a href="#point-cloud-image-projection">Point cloud image projection</a></li>
<li><a href="#ground-truth-frame-evaluation">Ground truth frame evaluation</a></li>
</ul>
</li>
</ul>
<p>After the system is calibrated one common concern is to be able to assess the accuracy of the produced calibration. ATOM provides several evaluation scripts for this purpose.</p>
<p>Unlike ATOM which calibrates all sensors simultaneously, evaluations are performed in pairs of sensors, which facilitates comparisons with other calibration approaches (which are mostly pairwise),
e.g. <a href="https://docs.opencv.org/4.x/d9/d0c/group__calib3d.html">opencv's stereo calibration</a>. Thus, there is a different evaluation script for each combination of modalities.</p>
<p>Every sensor evaluation script has two different variations: </p>
<ul>
<li>
<p>Intra-collection evaluation: This type of evaluation involves comparing the detections made by a pair of sensors within the same collection. It is utilized to assess the performance of sensor to sensor calibration.</p>
</li>
<li>
<p>Inter-collection evaluation: In this type of evaluation, we compare the detections made by a pair of sensors in different collections. To do this, we project the detections from the source sensor's frame to the world frame of the source collection and then project them to the target sensor's frame in the target collection. It evaluates not only the sensor to sensor calibration but also the sensor to frame calibration.</p>
</li>
</ul>
<p>The ATOM evaluation scripts often request for a train dataset and a test dataset. 
The train dataset is the dataset which is used to conduct the calibration, while the test dataset is a dataset which was not used for calibration.
Results are reported for the test dataset using the geometric transformations estimated during the calibration which were copied to the train dataset json.
These are meant to be two separate datasets, but if you can also use a single dataset by indicating it both as the train and the test dataset.
In this case, however, you should be aware that the results are being computed with data that was used for training.</p>
<p>The ATOM evaluation produces results presented in tabular format. This table displays the evaluation outcomes for each collection or collection pair, along with detailed error metrics. The ATOM evaluation output can be displayed in two distinct formats:</p>
<ul>
<li>
<p>Terminal Table: The results are printed directly to the terminal, providing an immediate overview of the evaluation outcomes</p>
</li>
<li>
<p>CSV File: You can save the evaluation results as a CSV file by using the <code>-sfr</code> flag when running the evaluations. This allows for easy storage and further analysis of the evaluation data. 
These files are saved by default in the subdirectory <code>results</code> in the test dataset folder.
To change this default, use the <code>-sfrn</code> flag</p>
</li>
</ul>
<h4 id="annotation-of-rgb-images">Annotation of rgb images</h4>
<p>To evaluate calibration between range sensors and cameras, it is necessary to annotate the physical limits on the calibration pattern in the images of the collection, to allow a comparison with physical labelings as measured by range sensors .</p>
<pre><code class="language-bash">rosrun atom_evaluation  annotate_pattern_borders_in_rgb -d &lt;path_to_train_file&gt; -cs &lt;rgb_sensor_name&gt; 

optional arguments:
  -h, --help            show this help message and exit
  -d DATASET_FILE, --dataset_file DATASET_FILE
                        Json file containing input dataset.
  -cs RGB_SENSOR, --rgb_sensor RGB_SENSOR
                        Source transformation sensor.
  -si, --show_images    If true the script shows images.
  -ww WINDOW_WIDTH, --window_width WINDOW_WIDTH
                        Width of the window.
  -ps POINT_SIZE, --point_size POINT_SIZE
                        Size of points to draw on image.
  -ppp POINTS_PER_PIXEL, --points_per_pixel POINTS_PER_PIXEL
                        How many points per pixel to sample between annotated points.

</code></pre>
<p>Note: you must annotate each camera sensor present in your calibration system. These annotation will be used to evaluate both the lidar-camera pairs and depth-camera.</p>
<p>How to annotate:</p>
<ul>
<li><strong>click</strong> to add points in the currently selected pattern side (text in top left corner)</li>
<li>if the pattern limit is viewed as a straight line in the image you may click only the corners, if needed you can click more points</li>
<li>once a side is complete, move on to the <strong>n</strong>ext side by pressing "n"</li>
<li>when the four sides are complete move to the next collection image by pressing "."</li>
</ul>
<p>The complete list of keys is printed when "h" is pressed. Be sure to label the corners in both intersected edges, ie, each corner should have two different coloured points.</p>
<p>The result should be something like this (for each image):</p>
<figure align="center">
<p><img alt="Image title" src="../img/annotation_rgb_images.png" width="90%" />
  </p>
<figcaption align="center">Annotation of the boundaries of the pattern in RGB images.</figcaption>
</figure>
<p>Here is a <a href="https://www.youtube.com/watch?v=DSYyKU-nDcs">video tutorial</a>.</p>
<h3 id="full-evaluation">Full Evaluation</h3>
<p>ATOM streamlines the evaluation process by generating a launch file called <code>full_evaluation.launch</code> during its configuration. 
This file automates the execution of all necessary evaluations to comprehensively assess the user's system. 
It is designed to configure and execute every possible evaluation the system may require. 
To run the full system evaluation, use the following command:</p>
<pre><code class="language-bash">roslaunch softbot2_calibration full_evaluation.launch train_json:=&lt;path_to_train_file&gt; test_json:=&lt;path_to_test_file&gt;
</code></pre>
<p>The <code>-sfr</code> flag is used in this command, resulting in the presentation of evaluation results in CSV files located in the <code>results</code> subdirectory of the test dataset directory.</p>
<h3 id="individual-evaluation">Individual Evaluation</h3>
<p>For users who prefer to run evaluations separately, ATOM provides the option to run each evaluation individually. 
To do so, determine the modalities of the sensors to be assessed and choose one of the following scripts that align with your sensor modalities:</p>
<h4 id="lidar-to-depth-evaluation">LiDAR to Depth evaluation</h4>
<p>How to run:</p>
<ul>
<li>Intra-collection</li>
</ul>
<pre><code class="language-bash">rosrun atom_evaluation lidar_to_depth_evaluation -train_json &lt;path_to_train_file&gt; -test_json &lt;path_to_test_file&gt; -ds &lt;depth_sensor_name&gt; -rs &lt;lidar_sensor_name&gt; -sfr

optional arguments:
  -h, --help            show this help message and exit
  -train_json TRAIN_JSON_FILE, --train_json_file TRAIN_JSON_FILE
                        Json file containing input training dataset.
  -test_json TEST_JSON_FILE, --test_json_file TEST_JSON_FILE
                        Json file containing input testing dataset.
  -rs RANGE_SENSOR, --range_sensor RANGE_SENSOR
                        Source transformation sensor.
  -ds DEPTH_SENSOR, --depth_sensor DEPTH_SENSOR
                        Target transformation sensor.
  -si, --show_images    If true the script shows images.
  -bt BORDER_TOLERANCE, --border_tolerance BORDER_TOLERANCE
                        Define the percentage of pixels to use to create a
                        border. Lidar points outside that border will not
                        count for the error calculations
  -csf COLLECTION_SELECTION_FUNCTION, --collection_selection_function COLLECTION_SELECTION_FUNCTION
                        A string to be evaluated into a lambda function that
                        receives a collection name as input and returns True
                        or False to indicate if the collection should be
                        loaded (and used in the optimization). The Syntax is
                        lambda name: f(x), where f(x) is the function in
                        python language. Example: lambda name: int(name) &gt; 5
                        , to load only collections 6, 7, and onward.
  -uic, --use_incomplete_collections
                        Remove any collection which does not have a detection
                        for all sensors.
  -rpd, --remove_partial_detections
                        Remove detected labels which are only partial.Used or
                        the Charuco.
  -sfr, --save_file_results
                        Store the results
  -sfrn SAVE_FILE_RESULTS_NAME, --save_file_results_name SAVE_FILE_RESULTS_NAME
                        Name of csv file to save the results. Default: -test_
                        json/results/{name_of_dataset}_{sensor_source}_to_{se
                        nsor_target}_results.csv
</code></pre>
<ul>
<li>Inter-collection</li>
</ul>
<pre><code class="language-bash">rosrun atom_evaluation inter_collection_lidar_to_depth_evaluation -train_json &lt;path_to_train_file&gt; -test_json &lt;path_to_test_file&gt; -ds &lt;depth_sensor_name&gt; -rs &lt;lidar_sensor_name&gt; -wf &lt;world_frame&gt; -sfr

optional arguments:
  -h, --help            show this help message and exit
  -train_json TRAIN_JSON_FILE, --train_json_file TRAIN_JSON_FILE
                        Json file containing input training dataset.
  -test_json TEST_JSON_FILE, --test_json_file TEST_JSON_FILE
                        Json file containing input testing dataset.
  -rs RANGE_SENSOR, --range_sensor RANGE_SENSOR
                        Source transformation sensor.
  -ds DEPTH_SENSOR, --depth_sensor DEPTH_SENSOR
                        Target transformation sensor.
  -si, --show_images    If true the script shows images.
  -wf WORLD_FRAME, --world_frame WORLD_FRAME
                        Fixed frame between collections.
  -bt BORDER_TOLERANCE, --border_tolerance BORDER_TOLERANCE
                        Define the percentage of pixels to use to create a
                        border. Lidar points outside that border will not
                        count for the error calculations
  -csf COLLECTION_SELECTION_FUNCTION, --collection_selection_function COLLECTION_SELECTION_FUNCTION
                        A string to be evaluated into a lambda function that
                        receives a collection name as input and returns True
                        or False to indicate if the collection should be
                        loaded (and used in the optimization). The Syntax is
                        lambda name: f(x), where f(x) is the function in
                        python language. Example: lambda name: int(name) &gt; 5
                        , to load only collections 6, 7, and onward.
  -uic, --use_incomplete_collections
                        Remove any collection which does not have a detection
                        for all sensors.
  -rpd, --remove_partial_detections
                        Remove detected labels which are only partial.Used or
                        the Charuco.
  -sfr, --save_file_results
                        Store the results
  -sfrn SAVE_FILE_RESULTS_NAME, --save_file_results_name SAVE_FILE_RESULTS_NAME
                        Name of csv file to save the results. Default: -test_
                        json/results/{name_of_dataset}_inter_collection_{sensor_source}_to_{se
                        nsor_target}_results.csv
</code></pre>
<h4 id="lidar-to-lidar-evaluation">LiDAR to LiDAR evaluation</h4>
<p>How to run:</p>
<ul>
<li>Intra-collection:</li>
</ul>
<pre><code class="language-bash">rosrun atom_evaluation lidar_to_lidar_evaluation -train_json &lt;path_to_train_file&gt; -test_json &lt;path_to_test_file&gt; -ss &lt;source_lidar_sensor_name&gt; -st &lt;target_lidar_sensor_name&gt; -sfr

optional arguments:
 -h, --help            show this help message and exit
  -train_json TRAIN_JSON_FILE, --train_json_file TRAIN_JSON_FILE
                        Json file containing input training dataset.
  -test_json TEST_JSON_FILE, --test_json_file TEST_JSON_FILE
                        Json file containing input testing dataset.
  -ss SENSOR_SOURCE, --sensor_source SENSOR_SOURCE
                        Source transformation sensor.
  -st SENSOR_TARGET, --sensor_target SENSOR_TARGET
                        Target transformation sensor.
  -csf COLLECTION_SELECTION_FUNCTION, --collection_selection_function COLLECTION_SELECTION_FUNCTION
                        A string to be evaluated into a lambda function that receives a collection name as input and returns True or False to indicate if the
                        collection should be loaded (and used in the optimization). The Syntax is lambda name: f(x), where f(x) is the function in python
                        language. Example: lambda name: int(name) &gt; 5 , to load only collections 6, 7, and onward.
  -uic, --use_incomplete_collections
                        Remove any collection which does not have a detection for all sensors.
  -rpd, --remove_partial_detections
                        Remove detected labels which are only partial.Used or the Charuco.
  -sfr, --save_file_results
                        Store the results
  -sfrn SAVE_FILE_RESULTS_NAME, --save_file_results_name SAVE_FILE_RESULTS_NAME
                        Name of csv file to save the results. Default: -test_
                        json/results/{name_of_dataset}_{sensor_source}_to_{se
                        nsor_target}_results.csv

</code></pre>
<ul>
<li>Inter-collection:</li>
</ul>
<pre><code class="language-bash">rosrun atom_evaluation inter_collection_lidar_to_lidar_evaluation -train_json &lt;path_to_train_file&gt; -test_json &lt;path_to_test_file&gt; -ss &lt;source_lidar_sensor_name&gt; -st &lt;target_lidar_sensor_name&gt; -wf &lt;world_frame&gt; -sfr

optional arguments:
 -h, --help            show this help message and exit
  -train_json TRAIN_JSON_FILE, --train_json_file TRAIN_JSON_FILE
                        Json file containing input training dataset.
  -test_json TEST_JSON_FILE, --test_json_file TEST_JSON_FILE
                        Json file containing input testing dataset.
  -ss SENSOR_SOURCE, --sensor_source SENSOR_SOURCE
                        Source transformation sensor.
  -st SENSOR_TARGET, --sensor_target SENSOR_TARGET
                        Target transformation sensor.
  -wf WORLD_FRAME, --world_frame WORLD_FRAME
                        Fixed frame between collections.
  -csf COLLECTION_SELECTION_FUNCTION, --collection_selection_function COLLECTION_SELECTION_FUNCTION
                        A string to be evaluated into a lambda function that receives a collection name as input and returns True or False to indicate if the
                        collection should be loaded (and used in the optimization). The Syntax is lambda name: f(x), where f(x) is the function in python
                        language. Example: lambda name: int(name) &gt; 5 , to load only collections 6, 7, and onward.
  -uic, --use_incomplete_collections
                        Remove any collection which does not have a detection for all sensors.
  -rpd, --remove_partial_detections
                        Remove detected labels which are only partial.Used or the Charuco.
  -sfr, --save_file_results
                        Store the results
  -sfrn SAVE_FILE_RESULTS_NAME, --save_file_results_name SAVE_FILE_RESULTS_NAME
                        Name of csv file to save the results. Default: -test_
                        json/results/{name_of_dataset}_inter_collection{sensor_source}_to_{se
                        nsor_target}_results.csv

</code></pre>
<h4 id="lidar-to-rgb-evaluation">LiDAR to RGB evaluation</h4>
<p>Evaluates the LiDAR-to-Camera calibration through the reprojection of the pattern limit 3D points into the image using
the following metrics:</p>
<ul>
<li>X and Y errors</li>
<li>Root mean squared error</li>
</ul>
<p>This process requires the annotation of the pattern limit points in the image.</p>
<p>How to run:</p>
<ul>
<li>Intra-collection:</li>
</ul>
<pre><code class="language-bash">rosrun atom_evaluation lidar_to_rgb_evaluation -train_json &lt;path_to_train_file&gt; -test_json &lt;path_to_test_file&gt; -rs &lt;lidar_sensor_name&gt; -cs &lt;rgb_sensor_name&gt; -sfr

optional arguments:
  -h, --help            show this help message and exit
  -train_json TRAIN_JSON_FILE, --train_json_file TRAIN_JSON_FILE
                        Json file containing input training dataset.
  -test_json TEST_JSON_FILE, --test_json_file TEST_JSON_FILE
                        Json file containing input testing dataset.
  -rs RANGE_SENSOR, --range_sensor RANGE_SENSOR
                        Source transformation sensor.
  -cs CAMERA_SENSOR, --camera_sensor CAMERA_SENSOR
                        Target transformation sensor.
  -si, --show_images    If true the script shows images.
  -csf COLLECTION_SELECTION_FUNCTION, --collection_selection_function COLLECTION_SELECTION_FUNCTION
                        A string to be evaluated into a lambda function that receives a collection name as input and returns True or False to indicate if the
                        collection should be loaded (and used in the optimization). The Syntax is lambda name: f(x), where f(x) is the function in python
                        language. Example: lambda name: int(name) &gt; 5 , to load only collections 6, 7, and onward.
  -uic, --use_incomplete_collections
                        Remove any collection which does not have a detection for all sensors.
  -rpd, --remove_partial_detections
                        Remove detected labels which are only partial.Used or the Charuco.
  -sfr, --save_file_results
                        Store the results
  -sfrn SAVE_FILE_RESULTS_NAME, --save_file_results_name SAVE_FILE_RESULTS_NAME
                        Name of csv file to save the results. Default: -test_json/results/{name_of_dataset}_{sensor_source}_to_{sensor_target}_results.csv

</code></pre>
<ul>
<li>Inter-collection:</li>
</ul>
<pre><code class="language-bash">rosrun atom_evaluation inter_collection_lidar_to_rgb_evaluation -train_json &lt;path_to_train_file&gt; -test_json &lt;path_to_test_file&gt; -rs &lt;lidar_sensor_name&gt; -cs &lt;rgb_sensor_name&gt; -wf &lt;world_frame&gt; -sfr

optional arguments:
  -h, --help            show this help message and exit
  -train_json TRAIN_JSON_FILE, --train_json_file TRAIN_JSON_FILE
                        Json file containing input training dataset.
  -test_json TEST_JSON_FILE, --test_json_file TEST_JSON_FILE
                        Json file containing input testing dataset.
  -rs RANGE_SENSOR, --range_sensor RANGE_SENSOR
                        Source transformation sensor.
  -cs CAMERA_SENSOR, --camera_sensor CAMERA_SENSOR
                        Target transformation sensor.
  -si, --show_images    If true the script shows images.
  -wf WORLD_FRAME, --world_frame WORLD_FRAME
                        Fixed frame between collections.
  -csf COLLECTION_SELECTION_FUNCTION, --collection_selection_function COLLECTION_SELECTION_FUNCTION
                        A string to be evaluated into a lambda function that receives a collection name as input and returns True or False to indicate if the
                        collection should be loaded (and used in the optimization). The Syntax is lambda name: f(x), where f(x) is the function in python
                        language. Example: lambda name: int(name) &gt; 5 , to load only collections 6, 7, and onward.
  -uic, --use_incomplete_collections
                        Remove any collection which does not have a detection for all sensors.
  -rpd, --remove_partial_detections
                        Remove detected labels which are only partial.Used or the Charuco.
  -sfr, --save_file_results
                        Store the results
  -sfrn SAVE_FILE_RESULTS_NAME, --save_file_results_name SAVE_FILE_RESULTS_NAME
                        Name of csv file to save the results. Default: -test_json/results/{name_of_dataset}_inter_collection_{sensor_source}_to_{sensor_target}_results.csv

</code></pre>
<h4 id="depth-to-depth-evaluation">Depth to Depth evaluation</h4>
<p>How to run:</p>
<ul>
<li>Intra-collection:</li>
</ul>
<pre><code class="language-bash">rosrun atom_evaluation depth_to_depth_evaluation -train_json &lt;path_to_train_file&gt; -test_json &lt;path_to_test_file&gt; -ss &lt;source_depth_sensor_name&gt; -st &lt;target_depth_sensor_name&gt; -sfr

  -h, --help            show this help message and exit
  -train_json TRAIN_JSON_FILE, --train_json_file TRAIN_JSON_FILE
                        Json file containing input training dataset.
  -test_json TEST_JSON_FILE, --test_json_file TEST_JSON_FILE
                        Json file containing input testing dataset.
  -ss SENSOR_SOURCE, --sensor_source SENSOR_SOURCE
                        Source transformation sensor.
  -st SENSOR_TARGET, --sensor_target SENSOR_TARGET
                        Target transformation sensor.
  -si, --show_images    If true the script shows images.
  -bt BORDER_TOLERANCE, --border_tolerance BORDER_TOLERANCE
                        Define the percentage of pixels to use to create a border. Lidar points outside that border will not count for the error calculations
  -csf COLLECTION_SELECTION_FUNCTION, --collection_selection_function COLLECTION_SELECTION_FUNCTION
                        A string to be evaluated into a lambda function that receives a collection name as input and returns True or False to indicate if the
                        collection should be loaded (and used in the optimization). The Syntax is lambda name: f(x), where f(x) is the function in python
                        language. Example: lambda name: int(name) &gt; 5 , to load only collections 6, 7, and onward.
  -uic, --use_incomplete_collections
                        Remove any collection which does not have a detection for all sensors.
  -rpd, --remove_partial_detections
                        Remove detected labels which are only partial.Used or the Charuco.
  -sfr, --save_file_results
                        Store the results
  -sfrn SAVE_FILE_RESULTS_NAME, --save_file_results_name SAVE_FILE_RESULTS_NAME
                        Name of csv file to save the results. Default: -test_json/results/{name_of_dataset}_{sensor_source}_to_{sensor_target}_results.csv

</code></pre>
<ul>
<li>Inter-collection:</li>
</ul>
<pre><code class="language-bash">rosrun atom_evaluation inter_collection_depth_to_depth_evaluation -train_json &lt;path_to_train_file&gt; -test_json &lt;path_to_test_file&gt; -ss &lt;source_depth_sensor_name&gt; -st &lt;target_depth_sensor_name&gt; -wf &lt;world_frame&gt; -sfr

  -h, --help            show this help message and exit
  -train_json TRAIN_JSON_FILE, --train_json_file TRAIN_JSON_FILE
                        Json file containing input training dataset.
  -test_json TEST_JSON_FILE, --test_json_file TEST_JSON_FILE
                        Json file containing input testing dataset.
  -ss SENSOR_SOURCE, --sensor_source SENSOR_SOURCE
                        Source transformation sensor.
  -st SENSOR_TARGET, --sensor_target SENSOR_TARGET
                        Target transformation sensor.
  -si, --show_images    If true the script shows images.
  -wf WORLD_FRAME, --world_frame WORLD_FRAME
                        Fixed frame between collections.
  -bt BORDER_TOLERANCE, --border_tolerance BORDER_TOLERANCE
                        Define the percentage of pixels to use to create a border. Lidar points outside that border will not count for the error calculations
  -csf COLLECTION_SELECTION_FUNCTION, --collection_selection_function COLLECTION_SELECTION_FUNCTION
                        A string to be evaluated into a lambda function that receives a collection name as input and returns True or False to indicate if the
                        collection should be loaded (and used in the optimization). The Syntax is lambda name: f(x), where f(x) is the function in python
                        language. Example: lambda name: int(name) &gt; 5 , to load only collections 6, 7, and onward.
  -uic, --use_incomplete_collections
                        Remove any collection which does not have a detection for all sensors.
  -rpd, --remove_partial_detections
                        Remove detected labels which are only partial.Used or the Charuco.
  -sfr, --save_file_results
                        Store the results
  -sfrn SAVE_FILE_RESULTS_NAME, --save_file_results_name SAVE_FILE_RESULTS_NAME
                        Name of csv file to save the results. Default: -test_json/results/{name_of_dataset}_inter_collection_{sensor_source}_to_{sensor_target}_results.csv

</code></pre>
<h4 id="depth-to-rgb-evaluation">Depth to RGB evaluation</h4>
<p>How to run:</p>
<ul>
<li>Intra-collection:</li>
</ul>
<pre><code class="language-bash">rosrun atom_evaluation depth_to_rgb_evaluation -train_json &lt;path_to_train_file&gt; -test_json &lt;path_to_test_file&gt; -ds &lt;depth_sensor_name&gt; -cs &lt;rgb_sensor_name&gt; -sfr 

optional arguments:
  -h, --help            show this help message and exit
  -train_json TRAIN_JSON_FILE, --train_json_file TRAIN_JSON_FILE
                        Json file containing input training dataset.
  -test_json TEST_JSON_FILE, --test_json_file TEST_JSON_FILE
                        Json file containing input testing dataset.
  -ds DEPTH_SENSOR, --depth_sensor DEPTH_SENSOR
                        Source transformation sensor.
  -cs CAMERA_SENSOR, --camera_sensor CAMERA_SENSOR
                        Target transformation sensor.
  -si, --show_images    If true the script shows images.
  -csf COLLECTION_SELECTION_FUNCTION, --collection_selection_function COLLECTION_SELECTION_FUNCTION
                        A string to be evaluated into a lambda function that receives a collection name as input and returns True or False to indicate if the
                        collection should be loaded (and used in the optimization). The Syntax is lambda name: f(x), where f(x) is the function in python
                        language. Example: lambda name: int(name) &gt; 5 , to load only collections 6, 7, and onward.
  -uic, --use_incomplete_collections
                        Remove any collection which does not have a detection for all sensors.
  -rpd, --remove_partial_detections
                        Remove detected labels which are only partial.Used or the Charuco.
  -sfr, --save_file_results
                        Store the results
  -sfrn SAVE_FILE_RESULTS_NAME, --save_file_results_name SAVE_FILE_RESULTS_NAME
                        Name of csv file to save the results. Default: -test_json/results/{name_of_dataset}_{sensor_source}_to_{sensor_target}_results.csv

</code></pre>
<ul>
<li>Inter-collection:</li>
</ul>
<pre><code class="language-bash">rosrun atom_evaluation inter_collection_depth_to_rgb_evaluation -train_json &lt;path_to_train_file&gt; -test_json &lt;path_to_test_file&gt; -ds &lt;depth_sensor_name&gt; -cs &lt;rgb_sensor_name&gt; -wf &lt;world_frame&gt; -sfr 

optional arguments:
  -h, --help            show this help message and exit
  -train_json TRAIN_JSON_FILE, --train_json_file TRAIN_JSON_FILE
                        Json file containing input training dataset.
  -test_json TEST_JSON_FILE, --test_json_file TEST_JSON_FILE
                        Json file containing input testing dataset.
  -ds DEPTH_SENSOR, --depth_sensor DEPTH_SENSOR
                        Source transformation sensor.
  -cs CAMERA_SENSOR, --camera_sensor CAMERA_SENSOR
                        Target transformation sensor.
  -si, --show_images    If true the script shows images.
  -wf WORLD_FRAME, --world_frame WORLD_FRAME
                        Fixed frame between collections.
  -csf COLLECTION_SELECTION_FUNCTION, --collection_selection_function COLLECTION_SELECTION_FUNCTION
                        A string to be evaluated into a lambda function that receives a collection name as input and returns True or False to indicate if the
                        collection should be loaded (and used in the optimization). The Syntax is lambda name: f(x), where f(x) is the function in python
                        language. Example: lambda name: int(name) &gt; 5 , to load only collections 6, 7, and onward.
  -uic, --use_incomplete_collections
                        Remove any collection which does not have a detection for all sensors.
  -rpd, --remove_partial_detections
                        Remove detected labels which are only partial.Used or the Charuco.
  -sfr, --save_file_results
                        Store the results
  -sfrn SAVE_FILE_RESULTS_NAME, --save_file_results_name SAVE_FILE_RESULTS_NAME
                        Name of csv file to save the results. Default: -test_json/results/{name_of_dataset}_inter_collection_{sensor_source}_to_{sensor_target}_results.csv

</code></pre>
<h4 id="rgb-to-rgb-evaluation">RGB to RGB evaluation</h4>
<p>Evaluates de camera-to-camera reprojection error with the following metrics:</p>
<ul>
<li>X and Y errors</li>
<li>Root mean squared error</li>
<li>Translation and rotation errors</li>
</ul>
<p>How to run:</p>
<ul>
<li>Intra collection:</li>
</ul>
<pre><code class="language-bash">rosrun atom_evaluation rgb_to_rgb_evaluation -train_json &lt;path_to_train_file&gt; -test_json &lt;path_to_test_file&gt; -ss &lt;source_rgb_sensor_name&gt; -st &lt;target_rgb_sensor_name&gt; -sfr

optional arguments:
  -h, --help            show this help message and exit
  -train_json TRAIN_JSON_FILE, --train_json_file TRAIN_JSON_FILE
                        Json file containing train input dataset.
  -test_json TEST_JSON_FILE, --test_json_file TEST_JSON_FILE
                        Json file containing test input dataset.
  -ss SENSOR_SOURCE, --sensor_source SENSOR_SOURCE
                        Source transformation sensor.
  -st SENSOR_TARGET, --sensor_target SENSOR_TARGET
                        Target transformation sensor.
  -si, --show_images    If true the script shows images.
  -csf COLLECTION_SELECTION_FUNCTION, --collection_selection_function COLLECTION_SELECTION_FUNCTION
                        A string to be evaluated into a lambda function that receives a collection name as input and returns True or False to indicate if the
                        collection should be loaded (and used in the optimization). The Syntax is lambda name: f(x), where f(x) is the function in python
                        language. Example: lambda name: int(name) &gt; 5 , to load only collections 6, 7, and onward.
  -uic, --use_incomplete_collections
                        Remove any collection which does not have a detection for all sensors.
  -rpd, --remove_partial_detections
                        Remove detected labels which are only partial.Used or the Charuco.
  -sfr, --save_file_results
                        Store the results
  -sfrn SAVE_FILE_RESULTS_NAME, --save_file_results_name SAVE_FILE_RESULTS_NAME
                        Name of csv file to save the results. Default: -test_json/results/{name_of_dataset}_{sensor_source}_to_{sensor_target}_results.csv
</code></pre>
<ul>
<li>Inter collection:</li>
</ul>
<pre><code class="language-bash">rosrun atom_evaluation inter_collection_rgb_to_rgb_evaluation -train_json &lt;path_to_train_file&gt; -test_json &lt;path_to_test_file&gt; -ss &lt;source_rgb_sensor_name&gt; -st &lt;target_rgb_sensor_name&gt; -wf &lt;world_frame&gt; -sfr

optional arguments:
  -h, --help            show this help message and exit
  -train_json TRAIN_JSON_FILE, --train_json_file TRAIN_JSON_FILE
                        Json file containing train input dataset.
  -test_json TEST_JSON_FILE, --test_json_file TEST_JSON_FILE
                        Json file containing test input dataset.
  -ss SENSOR_SOURCE, --sensor_source SENSOR_SOURCE
                        Source transformation sensor.
  -st SENSOR_TARGET, --sensor_target SENSOR_TARGET
                        Target transformation sensor.
  -si, --show_images    If true the script shows images.
  -wf WORLD_FRAME, --world_frame WORLD_FRAME
                        Fixed frame between collections.
  -csf COLLECTION_SELECTION_FUNCTION, --collection_selection_function COLLECTION_SELECTION_FUNCTION
                        A string to be evaluated into a lambda function that receives a collection name as input and returns True or False to indicate if the
                        collection should be loaded (and used in the optimization). The Syntax is lambda name: f(x), where f(x) is the function in python
                        language. Example: lambda name: int(name) &gt; 5 , to load only collections 6, 7, and onward.
  -uic, --use_incomplete_collections
                        Remove any collection which does not have a detection for all sensors.
  -rpd, --remove_partial_detections
                        Remove detected labels which are only partial.Used or the Charuco.
  -sfr, --save_file_results
                        Store the results
  -sfrn SAVE_FILE_RESULTS_NAME, --save_file_results_name SAVE_FILE_RESULTS_NAME
                        Name of csv file to save the results. Default: -test_json/results/{name_of_dataset}_inter_collection_{sensor_source}_to_{sensor_target}_results.csv
</code></pre>
<h4 id="point-cloud-image-projection">Point cloud image projection</h4>
<p><code>atom_evaluation</code> also allows the user to visualize the point cloud projected into an image to check the calibration.</p>
<pre><code class="language-bash">usage: point_cloud_to_image.py [-h] -json JSON_FILE -ls LIDAR_SENSOR -cs CAMERA_SENSOR
optional arguments:
  -h, --help            show this help message and exit
  -json JSON_FILE, --json_file JSON_FILE
                        Json file containing input dataset.
  -ls LIDAR_SENSOR, --lidar_sensor LIDAR_SENSOR
                        LiDAR sensor name.
  -cs CAMERA_SENSOR, --camera_sensor CAMERA_SENSOR
                        Camera sensor name.
</code></pre>
<p>How to run:</p>
<pre><code class="language-bash">rosrun atom_evaluation point_cloud_to_image.py -json &lt;path_to_test_json&gt; -ls &lt;lidar_sensor_name&gt; -cs &lt;camera_sensor_name&gt;
</code></pre>
<h4 id="ground-truth-frame-evaluation">Ground truth frame evaluation</h4>
<p>When using simulation it is possible to know that a given dataset contains "perfect" value for the transformations to be estimated.
Using this, we can run a comparison between the ground_truth dataset and the calibrated dataset, and assess if the estimated transformations are accurate.</p>
<p>To run use:</p>
<pre><code>rosrun atom_evaluation ground_truth_frame_evaluation -train_json &lt;calibrated_dataset&gt; -test_json &lt;ground_truth_dataset&gt;
</code></pre>
<p>The script produces a table where you can inspect the errors.
It is also possible to configure which frames are analyzed. Use --help to see the options.</p>
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="../examples/" class="btn btn-neutral float-left" title="Calibration Examples"><span class="icon icon-circle-arrow-left"></span> Previous</a>
        <a href="../multimedia/" class="btn btn-neutral float-right" title="Multimedia">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
      <span><a href="../examples/" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
      <span><a href="../multimedia/" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script>var base_url = '..';</script>
    <script src="../js/theme_extra.js" defer></script>
    <script src="../js/theme.js" defer></script>
      <script src="../search/main.js" defer></script>
    <script defer>
        window.onload = function () {
            SphinxRtdTheme.Navigation.enable(true);
        };
    </script>

</body>
</html>
