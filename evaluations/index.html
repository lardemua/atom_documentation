<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><link rel="canonical" href="https://lardemua.github.io/atom_documentation/evaluations/" />
      <link rel="shortcut icon" href="../img/favicon.ico" />
    <title>Evaluations - ATOM Calibration Framework</title>
    <link rel="stylesheet" href="../css/theme.css" />
    <link rel="stylesheet" href="../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/styles/github.min.css" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "Evaluations";
        var mkdocs_page_input_path = "evaluations.md";
        var mkdocs_page_url = "/atom_documentation/evaluations/";
      </script>
    
    <script src="../js/jquery-3.6.0.min.js" defer></script>
    <!--[if lt IE 9]>
      <script src="../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/highlight.min.js"></script>
      <script>hljs.initHighlightingOnLoad();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href=".." class="icon icon-home"> ATOM Calibration Framework
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="..">Home</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../quick_start/">Quick Start</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../concepts/">Concepts</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../procedures/">Calibration Procedures</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../examples/">Calibration Examples</a>
                </li>
              </ul>
              <ul class="current">
                <li class="toctree-l1 current"><a class="reference internal current" href="./">Evaluations</a>
    <ul class="current">
    <li class="toctree-l2"><a class="reference internal" href="#annotation-of-rgb-images">Annotation of rgb images</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#rgb-to-rgb-camera-evaluation">RGB to RGB camera evaluation</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#lidar-to-depth-camera-evaluation">LiDAR to Depth Camera evaluation</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#rgb-to-depth-camera-evaluation">RGB to Depth camera evaluation</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#lidar-to-lidar-evaluation">LiDAR to LiDAR evaluation</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#lidar-to-rgb-camera-evaluation">LiDAR to RGB camera evaluation</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#point-cloud-image-projection">Point cloud image projection</a>
    </li>
    </ul>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../publications/">Publications</a>
                </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="..">ATOM Calibration Framework</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href=".." class="icon icon-home" alt="Docs"></a> &raquo;</li><li>Evaluations</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>

          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h2 id="evaluating-your-calibration">Evaluating your calibration</h2>
<p>After the system is calibrated one common concern is to be able to assess the accuracy of the produced calibration. ATOM provides several evaluation scripts for this purpose.</p>
<p>Unlike ATOM which calibrates all sensors simultaneously, evaluations are performed in pairs of sensors, which facilitates comparisons with other calibration approaches (which are mostly pairwise),
e.g. <a href="https://docs.opencv.org/4.x/d9/d0c/group__calib3d.html">opencv's stereo calibration</a>. Thus, there is a different evaluation script for each combination of modalities.</p>
<h4 id="annotation-of-rgb-images">Annotation of rgb images</h4>
<p>To evaluate calibration between range sensors and cameras, it is necessary to annotate the physical limits on the calibration pattern in the images of the collection, to allow a comparison with physical labellings as measured by range sensors .</p>
<pre><code class="language-bash">rosrun atom_evaluation  annotate_pattern_borders_in_rgb.py [-h] -d DATASET_FILE -cs CAMERA_SENSOR [-si] [-ww WINDOW_WIDTH] [-ps POINT_SIZE] [-ppp POINTS_PER_PIXEL]

optional arguments:
  -h, --help            show this help message and exit
  -d DATASET_FILE, --dataset_file DATASET_FILE
                        Json file containing input dataset.
  -cs CAMERA_SENSOR, --camera_sensor CAMERA_SENSOR
                        Source transformation sensor.
  -si, --show_images    If true the script shows images.
  -ww WINDOW_WIDTH, --window_width WINDOW_WIDTH
                        Width of the window.
  -ps POINT_SIZE, --point_size POINT_SIZE
                        Size of points to draw on image.
  -ppp POINTS_PER_PIXEL, --points_per_pixel POINTS_PER_PIXEL
                        How many points per pixel to sample between annotated points.

</code></pre>
<p>Note: you must annotate each camera sensor present in your calibration system. These annotation will be used to evaluate both the lidar-camera pairs and depth-camera.</p>
<p>How to annotate:</p>
<ul>
<li><strong>click</strong> to add points in the currently selected pattern side (text in top left corner)</li>
<li>if the pattern limit is viewed as a straight line in the image you may click only the corners, if needed you can click more points</li>
<li>once a side is complete, move on to the <strong>n</strong>ext side by pressing "n"</li>
<li>when the four sides are complete move to the next collection image by pressing "."</li>
</ul>
<p>The complete list of keys is printed when "h" is pressed. Be sure to label the corners in both intersected edges, ie, each corner should have two different coloured points.</p>
<p>The result should be someting like this (for each image):</p>
<p><img align="center" src="docs/annotation_rgb_images.png" width="450"/></p>
<p>Here is a <a href="https://www.youtube.com/watch?v=DSYyKU-nDcs">video tutorial</a>.</p>
<h4 id="rgb-to-rgb-camera-evaluation">RGB to RGB camera evaluation</h4>
<p>Evaluates de camera-to-camera reprojection error with the following metrics:</p>
<ul>
<li>X and Y errors</li>
<li>Root mean squared error</li>
<li>Translation and rotation errors</li>
</ul>
<pre><code>usage: rgb_to_rgb_evaluation [-h] -train_json TRAIN_JSON_FILE -test_json TEST_JSON_FILE -ss SENSOR_SOURCE -st SENSOR_TARGET [-si] [-sg]

optional arguments:
  -h, --help            show this help message and exit
  -train_json TRAIN_JSON_FILE, --train_json_file TRAIN_JSON_FILE
                        Json file containing train input dataset.
  -test_json TEST_JSON_FILE, --test_json_file TEST_JSON_FILE
                        Json file containing test input dataset.
  -ss SENSOR_SOURCE, --sensor_source SENSOR_SOURCE
                        Source transformation sensor.
  -st SENSOR_TARGET, --sensor_target SENSOR_TARGET
                        Target transformation sensor.
  -si, --show_images    If true the script shows images.
  -sg, --save_graphics  Save reprojection error graphics.
</code></pre>
<p>How to run:</p>
<pre><code class="language-bash">rosrun atom_evaluation rgb_to_rgb_evalutation.py -train_json &lt;path_to_train_file&gt; -test_json &lt;path_to_test_file&gt; -ss &lt;source_sensor_name&gt; -ts &lt;target_sensor_name&gt;

</code></pre>
<h4 id="lidar-to-depth-camera-evaluation">LiDAR to Depth Camera evaluation</h4>
<p>How to run:</p>
<pre><code class="language-bash">rosrun atom_evaluation lidar_to_depth_evaluation.py -train_json &lt;path_to_train_file&gt; -test_json &lt;path_to_test_file&gt; -cs &lt;camera_sensor_name&gt; -rs &lt;lidar_sensor_name&gt; -si

optional arguments:
  -h, --help            show this help message and exit
  -train_json TRAIN_JSON_FILE, --train_json_file TRAIN_JSON_FILE
                        Json file containing train input dataset.
  -test_json TEST_JSON_FILE, --test_json_file TEST_JSON_FILE
                        Json file containing test input dataset.
  -ld SOURCE_SENSOR, --lidar_sensor SOURCE_SENSOR
                        Source transformation sensor.
  -ds TARGET_SENSOR, --depth_sensor TARGET_SENSOR
                        Target transformation sensor.
  -si, --show_images    If true the script shows images.

</code></pre>
<h4 id="rgb-to-depth-camera-evaluation">RGB to Depth camera evaluation</h4>
<p>How to run:</p>
<pre><code class="language-bash">rosrun atom_evaluation depth_sensor_to_camera_evaluation.py -train_json &lt;path_to_train_file&gt; -test_json &lt;path_to_test_file&gt; -cs &lt;camera_sensor_name&gt; -ds &lt;depth_sensor_name&gt;

optional arguments:
  -h, --help            show this help message and exit
  -train_json TRAIN_JSON_FILE, --train_json_file TRAIN_JSON_FILE
                        Json file containing train input dataset.
  -test_json TEST_JSON_FILE, --test_json_file TEST_JSON_FILE
                        Json file containing test input dataset.
  -cs SOURCE_SENSOR, --camera_sensor SOURCE_SENSOR
                        Source transformation sensor.
  -ds TARGET_SENSOR, --depth_sensor TARGET_SENSOR
                        Target transformation sensor.
  -si, --show_images    If true the script shows images.

</code></pre>
<h4 id="lidar-to-lidar-evaluation">LiDAR to LiDAR evaluation</h4>
<p>How to run:</p>
<pre><code class="language-bash">rosrun atom_evaluation lidar_to_lidar.py -train_json &lt;path_to_train_file&gt; -test_json &lt;path_to_test_file&gt; -ld1 &lt;source_lidar_sensor_name&gt; -ld2 &lt;target_lidar_sensor_name&gt;

optional arguments:
  -h, --help            show this help message and exit
  -train_json TRAIN_JSON_FILE, --train_json_file TRAIN_JSON_FILE
                        Json file containing train input dataset.
  -test_json TEST_JSON_FILE, --test_json_file TEST_JSON_FILE
                        Json file containing test input dataset.
  -ld1 SOURCE_SENSOR, --lidar_sensor_1 SOURCE_SENSOR
                        Source transformation sensor.
  -ld2 TARGET_SENSOR, --lidar_sensor_2 TARGET_SENSOR
                        Target transformation sensor.

</code></pre>
<h4 id="lidar-to-rgb-camera-evaluation">LiDAR to RGB camera evaluation</h4>
<p>Evaluates the LiDAR-to-Camera calibration through the reprojection of the pattern limit 3D points into the image using
the following metrics:</p>
<ul>
<li>X and Y errors</li>
<li>Root mean squared error</li>
</ul>
<p>This process requires the annotation of the pattern limit points in the image.</p>
<p>After annotating once, if the user wish to repeat the process, the saved json file with the annotations can be loaded.
For that the <code>-ua</code> flag has to be disabled.</p>
<pre><code>usage: range_sensor_to_camera_evaluation.py [-h] -train_json TRAIN_JSON_FILE -test_json TEST_JSON_FILE -ss SOURCE_SENSOR -ts TARGET_SENSOR [-si] -ef EVAL_FILE [-ua]

optional arguments:
  -h, --help            show this help message and exit
  -train_json TRAIN_JSON_FILE, --train_json_file TRAIN_JSON_FILE
                        Json file containing input training dataset.
  -test_json TEST_JSON_FILE, --test_json_file TEST_JSON_FILE
                        Json file containing input testing dataset.
  -ss SOURCE_SENSOR, --source_sensor SOURCE_SENSOR
                        Source transformation sensor.
  -ts TARGET_SENSOR, --target_sensor TARGET_SENSOR
                        Target transformation sensor.
  -si, --show_images    If true the script shows images.
  -ef EVAL_FILE, --eval_file EVAL_FILE
                        Path to file to read and/or write the evalutation data.
  -ua, --use_annotation
                        If true, the limit points will be manually annotated.
</code></pre>
<h4 id="point-cloud-image-projection">Point cloud image projection</h4>
<p><code>atom_evaluation</code> also allows the user to visualize the point cloud projected into an image to check the calibration.</p>
<pre><code class="language-bash">usage: point_cloud_to_image.py [-h] -json JSON_FILE -ls LIDAR_SENSOR -cs CAMERA_SENSOR
optional arguments:
  -h, --help            show this help message and exit
  -json JSON_FILE, --json_file JSON_FILE
                        Json file containing input dataset.
  -ls LIDAR_SENSOR, --lidar_sensor LIDAR_SENSOR
                        LiDAR sensor name.
  -cs CAMERA_SENSOR, --camera_sensor CAMERA_SENSOR
                        Camera sensor name.
</code></pre>
<p>How to run:</p>
<pre><code class="language-bash">rosrun atom_evaluation point_cloud_to_image.py -json &lt;path_to_test_json&gt; -ls &lt;lidar_sensor_name&gt; -cs &lt;camera_sensor_name&gt;
</code></pre>
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="../examples/" class="btn btn-neutral float-left" title="Calibration Examples"><span class="icon icon-circle-arrow-left"></span> Previous</a>
        <a href="../publications/" class="btn btn-neutral float-right" title="Publications">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
      <span><a href="../examples/" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
      <span><a href="../publications/" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script>var base_url = '..';</script>
    <script src="../js/theme_extra.js" defer></script>
    <script src="../js/theme.js" defer></script>
      <script src="../search/main.js" defer></script>
    <script defer>
        window.onload = function () {
            SphinxRtdTheme.Navigation.enable(true);
        };
    </script>

</body>
</html>
