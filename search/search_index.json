{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"ATOM ATOM is a calibration framework using A tomic T ransformations O ptimization M ethod. https://github.com/lardemua/atom It contains a set of calibration tools for multi-sensor, multi-modal, robotic systems, based on the optimization of atomic transformations, as provided by a ROS based robot description. Moreover, it provides several scripts to facilitate all the steps of a calibration procedure. If this work is helpful for you please cite these publications . Multimedia Take a look at the ATOM youtube playlist . Installation Clone the atom repository to a directory inside your catkin workspace: git clone https://github.com/lardemua/atom then install requirements. sudo pip3 install -r requirements.txt Configure environment variables We often use two enviroment variables to allow for easy cross machine access to bagfiles and datasets. If you want to use these you can also add these lines to your .bashrc or .zhsrc , adjusting the paths according to your case: export ROS_BAGS=\"$HOME/<bagfiles\" export ATOM_DATASETS=\"$HOME/datasets\" and then you can refer to these environment variables when providing paths to atom scripts, e.g.: roslaunch <your_robot_calibration> calibrate.launch dataset_file:=$ATOM_DATASETS/<my_dataset>/dataset.json and you can also refer to them inside the calibration configuration file Contributors Miguel Riem Oliveira - University of Aveiro Afonso Castro - University of Aveiro Eurico Pedrosa - University of Aveiro Tiago Madeira - University of Aveiro Andr\u00e9 Aguiar - INESC TEC Daniela Rato - University of Aveiro Current Maintainers Miguel Riem Oliveira - University of Aveiro Daniela Rato - University of Aveiro Manuel Gomes - University of Aveiro","title":"Home"},{"location":"#atom","text":"ATOM is a calibration framework using A tomic T ransformations O ptimization M ethod. https://github.com/lardemua/atom It contains a set of calibration tools for multi-sensor, multi-modal, robotic systems, based on the optimization of atomic transformations, as provided by a ROS based robot description. Moreover, it provides several scripts to facilitate all the steps of a calibration procedure. If this work is helpful for you please cite these publications .","title":"ATOM"},{"location":"#multimedia","text":"Take a look at the ATOM youtube playlist .","title":"Multimedia"},{"location":"#installation","text":"Clone the atom repository to a directory inside your catkin workspace: git clone https://github.com/lardemua/atom then install requirements. sudo pip3 install -r requirements.txt","title":"Installation"},{"location":"#configure-environment-variables","text":"We often use two enviroment variables to allow for easy cross machine access to bagfiles and datasets. If you want to use these you can also add these lines to your .bashrc or .zhsrc , adjusting the paths according to your case: export ROS_BAGS=\"$HOME/<bagfiles\" export ATOM_DATASETS=\"$HOME/datasets\" and then you can refer to these environment variables when providing paths to atom scripts, e.g.: roslaunch <your_robot_calibration> calibrate.launch dataset_file:=$ATOM_DATASETS/<my_dataset>/dataset.json and you can also refer to them inside the calibration configuration file","title":"Configure environment variables"},{"location":"#contributors","text":"Miguel Riem Oliveira - University of Aveiro Afonso Castro - University of Aveiro Eurico Pedrosa - University of Aveiro Tiago Madeira - University of Aveiro Andr\u00e9 Aguiar - INESC TEC Daniela Rato - University of Aveiro","title":"Contributors"},{"location":"#current-maintainers","text":"Miguel Riem Oliveira - University of Aveiro Daniela Rato - University of Aveiro Manuel Gomes - University of Aveiro","title":"Current Maintainers"},{"location":"detailed_description/","text":"System calibration - Detailed Description To calibrate your robot you must define your robotic system, (e.g. <your_robot>). You should also have a system description in the form of an urdf or a xacro file(s). This is normally stored in a ros package named <your_robot>_description . Note We recommend using xacro files instead of urdfs. Finally, ATOM requires a bagfile with a recording of the data from the sensors you wish to calibrate. Transformations in the bagfile (i.e. topics /tf and /tf_static) will be ignored, so that they do not collide with the ones being published by the robot_state_publisher . Thus, if your robotic system contains moving parts, the bagfile should also record the sensor_msgs/JointState message. Note It is also possible to use the transformations in the bagfile instead of using the xacro description and the robot state publisher to produce them. To reduce the bag size it may contain compressed images, since ATOM can decompress them while playing back the bagfile. Here is an example of a launch file which records compressed images. Creating a calibration package To start you should create a calibration ros package specific for your robot. ATOM provides a script for this: rosrun atom_calibration create_calibration_pkg --name <your_robot_calibration> This will create the ros package in the current folder, but you can also specify the folder, e.g.: rosrun atom_calibration create_calibration_pkg --name ~/my/path/<your_robot_calibration> Configuring a calibration package Once your calibration package is created you will have to configure the calibration procedure by editing the /calibration/config.yml file with your system information. Here are examples of calibration config.yml files for an autonomous vehicle and for a simulated hand eye system After filling the config.yml file, you can run the package configuration: rosrun <your_robot_calibration> configure This will go through a series of varifications, and create a set of files for launching the system, configuring rviz, etc. It is also possible to configure your calibration package with a different configuration file, in the case you have multiple configurations with multiple config.yml files. To do this, you can use: rosrun <your_robot_calibration> configure -c new_config_file.yml If you want to use other arguments of the calibration package configuration you may run: rosrun atom_calibration configure_calibration_package --name <your_robot_calibration> <other options> usage: configure_calibration_pkg [-h] -n NAME [-utf] [-cfg CONFIG_FILE] -h, --help show this help message and exit -n NAME, --name NAME package name -utf, --use_tfs Use transformations in the bag file instead of generating new tfs from the xacro, joint_state_msgs and robot state publisher. -cfg CONFIG_FILE, --config_file CONFIG_FILE Specify if you want to configure the calibration package with a specific configutation file. If this flag is not given, the standard config.yml ill be used. Set initial estimate Iterative optimization methods are often sensitive to the initial parameter configuration. Here, the optimization parameters represent the poses of each sensor. ATOM provides an interactive framework based on rviz which allows the user to set the pose of the sensors while having immediate visual feedback. To set an initial estimate run: roslaunch <your_robot_calibration> set_initial_estimate.launch Here are a couple of examples: Atlascar2 AgrobV2 UR10e eye in hand Collect data the extraction of snapshots of data (a.k.a., collections) which constitute an ATOM dataset: Note When calibrating a stereo system one collects several images from both cameras in order to calibrate. Collections are a similar concept applied to multi-modal systems. A collection is a recording of the data from all the sensors in the system at a particular time instant. To run a system calibration, one requires sensor data collected at different time instants. We refer to these as data collections or simply collections . To collect data, the user should launch: roslaunch <your_robot_calibration> collect_data.launch output_folder:=<your_dataset_folder> Depending on the size and number of topics in the bag file, it may be necessary (it often is) to reduce the playback rate of the bag file. roslaunch <your_robot_calibration> collect_data.launch output_folder:=<your_dataset_folder> bag_rate:=<playback_rate> You can use a couple of launch file arguments to configure the calibration procedure, namely overwrite [false] - if the dataset folder is the same as one previously recorded, it overwrites the previous dataset ssl [false] - S kip S ensor L abelling: A string to be evaluated into a lambda function that receives a sensor name as input and returns True or False to indicate if that sensor should be labelled. An example: ``` roslaunch collect_data.launch output_folder:=$ATOM_DATASETS/ / ssl:='lambda name: name in [\"lidar_1\", \"lidar_2\", \"lidar_3\"]' Here are some examples of the system collecting data: Atlascar2 AgrobV2 UR10e eye to_base A dataset is a folder which contains a set of collections. There, a dataset.json file stores all the information required for the calibration. There are also in the folder images and point clouds associated with each collection. Calibrate sensors Finally, a system calibration is called through: roslaunch <your_robot_calibration> calibrate.launch dataset_file:=~/datasets/<my_dataset>/dataset.json You can use a couple of launch file arguments to configure the calibration procedure, namely single_pattern [false] - use_incomplete_collections [false] - Remove any collection which does not have a detection for all sensors. ssf [false] - S ensor S election F unction: A string to be evaluated into a lambda function that receives a sensor name as input and returns True or False to indicate if the sensor should be loaded (and used in the optimization). An example: roslaunch <your_robot_calibration> calibrate.launch dataset_file:=$ATOM_DATASETS/<my_dataset>/dataset.json ssf:='lambda name: name in [\"camera1, \"lidar2\"]' csf [false] - C ollection S election F unction: A string to be evaluated into a lambda function that receives a collection name as input and returns True or False to indicate if that collection should be loaded (and used in the optimization). An example: roslaunch <your_robot_calibration> calibrate.launch dataset_file:=$ATOM_DATASETS/<my_dataset>/dataset.json csf:='lambda name: int(name) < 7' Advanced usage - running calibration script in separate terminal Alternatively, for debugging the calibrate script it is better not to have it executed with a bunch of other scripts which is what happens when you call the launch file. You can run everything with the launch excluding without the calibrate script roslaunch <your_robot_calibration> calibrate.launch dataset_file:=~/datasets/<my_dataset>/dataset.json run_calibration:=false and then launch the script in standalone mode rosrun atom_calibration calibrate -json dataset_file:=~/datasets/<my_dataset>/dataset.json There are several additional command line arguments to use with the calibrate script, run calibrate --help to get the complete list: usage: calibrate [-h] [-sv SKIP_VERTICES] [-z Z_INCONSISTENCY_THRESHOLD] [-vpv] [-vo] -json JSON_FILE [-v] [-rv] [-si] [-oi] [-pof] [-sr SAMPLE_RESIDUALS] [-ss SAMPLE_SEED] [-od] [-fec] [-uic] [-rpd] [-ssf SENSOR_SELECTION_FUNCTION] [-csf COLLECTION_SELECTION_FUNCTION] optional arguments: -h, --help show this help message and exit -json JSON_FILE, --json_file JSON_FILE Json file containing input dataset. -vo, --view_optimization Displays generic total error and residuals graphs. -v, --verbose Be verbose -rv, --ros_visualization Publish ros visualization markers. -si, --show_images shows images for each camera -oi, --optimize_intrinsics Adds camera instrinsics and distortion to the optimization -sr SAMPLE_RESIDUALS, --sample_residuals SAMPLE_RESIDUALS Samples residuals -ss SAMPLE_SEED, --sample_seed SAMPLE_SEED Sampling seed -uic, --use_incomplete_collections Remove any collection which does not have a detection for all sensors. -rpd, --remove_partial_detections Remove detected labels which are only partial. Used or the Charuco. -ssf SENSOR_SELECTION_FUNCTION, --sensor_selection_function SENSOR_SELECTION_FUNCTION A string to be evaluated into a lambda function that receives a sensor name as input and returns True or False to indicate if the sensor should be loaded (and used in the optimization). The Syntax is lambda name: f(x), where f(x) is the function in python language. Example: lambda name: name in [\"left_laser\", \"frontal_camera\"] , to load only sensors left_laser and frontal_camera -csf COLLECTION_SELECTION_FUNCTION, --collection_selection_function COLLECTION_SELECTION_FUNCTION A string to be evaluated into a lambda function that receives a collection name as input and returns True or False to indicate if the collection should be loaded (and used in the optimization). The Syntax is lambda name: f(x), where f(x) is the function in python language. Example: lambda name: int(name) > 5 , to load only collections 6, 7, and onward. It is also possible to call some of these through the launch file. Check the launch file to see how. Advanced usage - two stage calibration for robotic systems with an anchored sensor When one sensor is set to be acnhored in the calibration/config.yml file, i.e. this file for the AtlaCar2, we recommend a two stage procedure to achieve a more accurate calibration: First, run a calibration using parameter --only_anchored_sensor ( -oas ) which will exclude from the optimization all sensors which are not the anchored one. This optimization will position the patterns correctly w.r.t. the anchored sensor. For example: rosrun atom_calibration calibrate -json $ATOM_DATASETS/larcc_real/ dataset_train/dataset_corrected.json -uic -nig 0.0 0.0 -ipg -si -rv -v -oas The output is stored in the atom_calibration.json , which is used and the input for the second stage, where all sensors are used. In this second stage the poses of the patterns are frozen using the parameter --anchor_patterns ( -ap ). To avoid overwritting atom_calibration.json, you should also define the output json file ( -oj ). For example: rosrun atom_calibration calibrate -json $ATOM_DATASETS/larcc_real/ dataset_train/atom_calibration.json -uic -nig 0.0 0.0 -ipg -si -rv -v -ap -oj atom_anchored_calibration.json","title":"Detailed Description"},{"location":"detailed_description/#system-calibration-detailed-description","text":"To calibrate your robot you must define your robotic system, (e.g. <your_robot>). You should also have a system description in the form of an urdf or a xacro file(s). This is normally stored in a ros package named <your_robot>_description . Note We recommend using xacro files instead of urdfs. Finally, ATOM requires a bagfile with a recording of the data from the sensors you wish to calibrate. Transformations in the bagfile (i.e. topics /tf and /tf_static) will be ignored, so that they do not collide with the ones being published by the robot_state_publisher . Thus, if your robotic system contains moving parts, the bagfile should also record the sensor_msgs/JointState message. Note It is also possible to use the transformations in the bagfile instead of using the xacro description and the robot state publisher to produce them. To reduce the bag size it may contain compressed images, since ATOM can decompress them while playing back the bagfile. Here is an example of a launch file which records compressed images.","title":"System calibration - Detailed Description"},{"location":"detailed_description/#creating-a-calibration-package","text":"To start you should create a calibration ros package specific for your robot. ATOM provides a script for this: rosrun atom_calibration create_calibration_pkg --name <your_robot_calibration> This will create the ros package in the current folder, but you can also specify the folder, e.g.: rosrun atom_calibration create_calibration_pkg --name ~/my/path/<your_robot_calibration>","title":"Creating a calibration package"},{"location":"detailed_description/#configuring-a-calibration-package","text":"Once your calibration package is created you will have to configure the calibration procedure by editing the /calibration/config.yml file with your system information. Here are examples of calibration config.yml files for an autonomous vehicle and for a simulated hand eye system After filling the config.yml file, you can run the package configuration: rosrun <your_robot_calibration> configure This will go through a series of varifications, and create a set of files for launching the system, configuring rviz, etc. It is also possible to configure your calibration package with a different configuration file, in the case you have multiple configurations with multiple config.yml files. To do this, you can use: rosrun <your_robot_calibration> configure -c new_config_file.yml If you want to use other arguments of the calibration package configuration you may run: rosrun atom_calibration configure_calibration_package --name <your_robot_calibration> <other options> usage: configure_calibration_pkg [-h] -n NAME [-utf] [-cfg CONFIG_FILE] -h, --help show this help message and exit -n NAME, --name NAME package name -utf, --use_tfs Use transformations in the bag file instead of generating new tfs from the xacro, joint_state_msgs and robot state publisher. -cfg CONFIG_FILE, --config_file CONFIG_FILE Specify if you want to configure the calibration package with a specific configutation file. If this flag is not given, the standard config.yml ill be used.","title":"Configuring a calibration package"},{"location":"detailed_description/#set-initial-estimate","text":"Iterative optimization methods are often sensitive to the initial parameter configuration. Here, the optimization parameters represent the poses of each sensor. ATOM provides an interactive framework based on rviz which allows the user to set the pose of the sensors while having immediate visual feedback. To set an initial estimate run: roslaunch <your_robot_calibration> set_initial_estimate.launch Here are a couple of examples: Atlascar2 AgrobV2 UR10e eye in hand","title":"Set initial estimate"},{"location":"detailed_description/#collect-data","text":"the extraction of snapshots of data (a.k.a., collections) which constitute an ATOM dataset: Note When calibrating a stereo system one collects several images from both cameras in order to calibrate. Collections are a similar concept applied to multi-modal systems. A collection is a recording of the data from all the sensors in the system at a particular time instant. To run a system calibration, one requires sensor data collected at different time instants. We refer to these as data collections or simply collections . To collect data, the user should launch: roslaunch <your_robot_calibration> collect_data.launch output_folder:=<your_dataset_folder> Depending on the size and number of topics in the bag file, it may be necessary (it often is) to reduce the playback rate of the bag file. roslaunch <your_robot_calibration> collect_data.launch output_folder:=<your_dataset_folder> bag_rate:=<playback_rate> You can use a couple of launch file arguments to configure the calibration procedure, namely overwrite [false] - if the dataset folder is the same as one previously recorded, it overwrites the previous dataset ssl [false] - S kip S ensor L abelling: A string to be evaluated into a lambda function that receives a sensor name as input and returns True or False to indicate if that sensor should be labelled. An example: ``` roslaunch collect_data.launch output_folder:=$ATOM_DATASETS/ / ssl:='lambda name: name in [\"lidar_1\", \"lidar_2\", \"lidar_3\"]' Here are some examples of the system collecting data: Atlascar2 AgrobV2 UR10e eye to_base A dataset is a folder which contains a set of collections. There, a dataset.json file stores all the information required for the calibration. There are also in the folder images and point clouds associated with each collection.","title":"Collect data"},{"location":"detailed_description/#calibrate-sensors","text":"Finally, a system calibration is called through: roslaunch <your_robot_calibration> calibrate.launch dataset_file:=~/datasets/<my_dataset>/dataset.json You can use a couple of launch file arguments to configure the calibration procedure, namely single_pattern [false] - use_incomplete_collections [false] - Remove any collection which does not have a detection for all sensors. ssf [false] - S ensor S election F unction: A string to be evaluated into a lambda function that receives a sensor name as input and returns True or False to indicate if the sensor should be loaded (and used in the optimization). An example: roslaunch <your_robot_calibration> calibrate.launch dataset_file:=$ATOM_DATASETS/<my_dataset>/dataset.json ssf:='lambda name: name in [\"camera1, \"lidar2\"]' csf [false] - C ollection S election F unction: A string to be evaluated into a lambda function that receives a collection name as input and returns True or False to indicate if that collection should be loaded (and used in the optimization). An example: roslaunch <your_robot_calibration> calibrate.launch dataset_file:=$ATOM_DATASETS/<my_dataset>/dataset.json csf:='lambda name: int(name) < 7'","title":"Calibrate sensors"},{"location":"detailed_description/#advanced-usage-running-calibration-script-in-separate-terminal","text":"Alternatively, for debugging the calibrate script it is better not to have it executed with a bunch of other scripts which is what happens when you call the launch file. You can run everything with the launch excluding without the calibrate script roslaunch <your_robot_calibration> calibrate.launch dataset_file:=~/datasets/<my_dataset>/dataset.json run_calibration:=false and then launch the script in standalone mode rosrun atom_calibration calibrate -json dataset_file:=~/datasets/<my_dataset>/dataset.json There are several additional command line arguments to use with the calibrate script, run calibrate --help to get the complete list: usage: calibrate [-h] [-sv SKIP_VERTICES] [-z Z_INCONSISTENCY_THRESHOLD] [-vpv] [-vo] -json JSON_FILE [-v] [-rv] [-si] [-oi] [-pof] [-sr SAMPLE_RESIDUALS] [-ss SAMPLE_SEED] [-od] [-fec] [-uic] [-rpd] [-ssf SENSOR_SELECTION_FUNCTION] [-csf COLLECTION_SELECTION_FUNCTION] optional arguments: -h, --help show this help message and exit -json JSON_FILE, --json_file JSON_FILE Json file containing input dataset. -vo, --view_optimization Displays generic total error and residuals graphs. -v, --verbose Be verbose -rv, --ros_visualization Publish ros visualization markers. -si, --show_images shows images for each camera -oi, --optimize_intrinsics Adds camera instrinsics and distortion to the optimization -sr SAMPLE_RESIDUALS, --sample_residuals SAMPLE_RESIDUALS Samples residuals -ss SAMPLE_SEED, --sample_seed SAMPLE_SEED Sampling seed -uic, --use_incomplete_collections Remove any collection which does not have a detection for all sensors. -rpd, --remove_partial_detections Remove detected labels which are only partial. Used or the Charuco. -ssf SENSOR_SELECTION_FUNCTION, --sensor_selection_function SENSOR_SELECTION_FUNCTION A string to be evaluated into a lambda function that receives a sensor name as input and returns True or False to indicate if the sensor should be loaded (and used in the optimization). The Syntax is lambda name: f(x), where f(x) is the function in python language. Example: lambda name: name in [\"left_laser\", \"frontal_camera\"] , to load only sensors left_laser and frontal_camera -csf COLLECTION_SELECTION_FUNCTION, --collection_selection_function COLLECTION_SELECTION_FUNCTION A string to be evaluated into a lambda function that receives a collection name as input and returns True or False to indicate if the collection should be loaded (and used in the optimization). The Syntax is lambda name: f(x), where f(x) is the function in python language. Example: lambda name: int(name) > 5 , to load only collections 6, 7, and onward. It is also possible to call some of these through the launch file. Check the launch file to see how.","title":"Advanced usage - running calibration script in separate terminal"},{"location":"detailed_description/#advanced-usage-two-stage-calibration-for-robotic-systems-with-an-anchored-sensor","text":"When one sensor is set to be acnhored in the calibration/config.yml file, i.e. this file for the AtlaCar2, we recommend a two stage procedure to achieve a more accurate calibration: First, run a calibration using parameter --only_anchored_sensor ( -oas ) which will exclude from the optimization all sensors which are not the anchored one. This optimization will position the patterns correctly w.r.t. the anchored sensor. For example: rosrun atom_calibration calibrate -json $ATOM_DATASETS/larcc_real/ dataset_train/dataset_corrected.json -uic -nig 0.0 0.0 -ipg -si -rv -v -oas The output is stored in the atom_calibration.json , which is used and the input for the second stage, where all sensors are used. In this second stage the poses of the patterns are frozen using the parameter --anchor_patterns ( -ap ). To avoid overwritting atom_calibration.json, you should also define the output json file ( -oj ). For example: rosrun atom_calibration calibrate -json $ATOM_DATASETS/larcc_real/ dataset_train/atom_calibration.json -uic -nig 0.0 0.0 -ipg -si -rv -v -ap -oj atom_anchored_calibration.json","title":"Advanced usage - two stage calibration for robotic systems with an anchored sensor"},{"location":"evaluations/","text":"Evaluating your calibration After the system is calibrated one common concern is to be able to assess the accuracy of the produced calibration. ATOM provides several evaluation scripts for this purpose. Unlike ATOM which calibrates all sensors simultaneously, evaluations are performed in pairs of sensors, which facilitates comparisons with other calibration approaches (which are mostly pairwise), e.g. opencv's stereo calibration . Thus, there is a different evaluation script for each combination of modalities. Annotation of rgb images To evaluate calibration between range sensors and cameras, it is necessary to annotate the physical limits on the calibration pattern in the images of the collection, to allow a comparison with physical labellings as measured by range sensors . rosrun atom_evaluation annotate_pattern_borders_in_rgb.py [-h] -d DATASET_FILE -cs CAMERA_SENSOR [-si] [-ww WINDOW_WIDTH] [-ps POINT_SIZE] [-ppp POINTS_PER_PIXEL] optional arguments: -h, --help show this help message and exit -d DATASET_FILE, --dataset_file DATASET_FILE Json file containing input dataset. -cs CAMERA_SENSOR, --camera_sensor CAMERA_SENSOR Source transformation sensor. -si, --show_images If true the script shows images. -ww WINDOW_WIDTH, --window_width WINDOW_WIDTH Width of the window. -ps POINT_SIZE, --point_size POINT_SIZE Size of points to draw on image. -ppp POINTS_PER_PIXEL, --points_per_pixel POINTS_PER_PIXEL How many points per pixel to sample between annotated points. Note: you must annotate each camera sensor present in your calibration system. These annotation will be used to evaluate both the lidar-camera pairs and depth-camera. How to annotate: click to add points in the currently selected pattern side (text in top left corner) if the pattern limit is viewed as a straight line in the image you may click only the corners, if needed you can click more points once a side is complete, move on to the n ext side by pressing \"n\" when the four sides are complete move to the next collection image by pressing \".\" The complete list of keys is printed when \"h\" is pressed. Be sure to label the corners in both intersected edges, ie, each corner should have two different coloured points. The result should be someting like this (for each image): Here is a video tutorial . RGB to RGB camera evaluation Evaluates de camera-to-camera reprojection error with the following metrics: X and Y errors Root mean squared error Translation and rotation errors usage: rgb_to_rgb_evaluation [-h] -train_json TRAIN_JSON_FILE -test_json TEST_JSON_FILE -ss SENSOR_SOURCE -st SENSOR_TARGET [-si] [-sg] optional arguments: -h, --help show this help message and exit -train_json TRAIN_JSON_FILE, --train_json_file TRAIN_JSON_FILE Json file containing train input dataset. -test_json TEST_JSON_FILE, --test_json_file TEST_JSON_FILE Json file containing test input dataset. -ss SENSOR_SOURCE, --sensor_source SENSOR_SOURCE Source transformation sensor. -st SENSOR_TARGET, --sensor_target SENSOR_TARGET Target transformation sensor. -si, --show_images If true the script shows images. -sg, --save_graphics Save reprojection error graphics. How to run: rosrun atom_evaluation rgb_to_rgb_evalutation.py -train_json <path_to_train_file> -test_json <path_to_test_file> -ss <source_sensor_name> -ts <target_sensor_name> LiDAR to Depth Camera evaluation How to run: rosrun atom_evaluation lidar_to_depth_evaluation.py -train_json <path_to_train_file> -test_json <path_to_test_file> -cs <camera_sensor_name> -rs <lidar_sensor_name> -si optional arguments: -h, --help show this help message and exit -train_json TRAIN_JSON_FILE, --train_json_file TRAIN_JSON_FILE Json file containing train input dataset. -test_json TEST_JSON_FILE, --test_json_file TEST_JSON_FILE Json file containing test input dataset. -ld SOURCE_SENSOR, --lidar_sensor SOURCE_SENSOR Source transformation sensor. -ds TARGET_SENSOR, --depth_sensor TARGET_SENSOR Target transformation sensor. -si, --show_images If true the script shows images. RGB to Depth camera evaluation How to run: rosrun atom_evaluation depth_sensor_to_camera_evaluation.py -train_json <path_to_train_file> -test_json <path_to_test_file> -cs <camera_sensor_name> -ds <depth_sensor_name> optional arguments: -h, --help show this help message and exit -train_json TRAIN_JSON_FILE, --train_json_file TRAIN_JSON_FILE Json file containing train input dataset. -test_json TEST_JSON_FILE, --test_json_file TEST_JSON_FILE Json file containing test input dataset. -cs SOURCE_SENSOR, --camera_sensor SOURCE_SENSOR Source transformation sensor. -ds TARGET_SENSOR, --depth_sensor TARGET_SENSOR Target transformation sensor. -si, --show_images If true the script shows images. LiDAR to LiDAR evaluation How to run: rosrun atom_evaluation lidar_to_lidar.py -train_json <path_to_train_file> -test_json <path_to_test_file> -ld1 <source_lidar_sensor_name> -ld2 <target_lidar_sensor_name> optional arguments: -h, --help show this help message and exit -train_json TRAIN_JSON_FILE, --train_json_file TRAIN_JSON_FILE Json file containing train input dataset. -test_json TEST_JSON_FILE, --test_json_file TEST_JSON_FILE Json file containing test input dataset. -ld1 SOURCE_SENSOR, --lidar_sensor_1 SOURCE_SENSOR Source transformation sensor. -ld2 TARGET_SENSOR, --lidar_sensor_2 TARGET_SENSOR Target transformation sensor. LiDAR to RGB camera evaluation Evaluates the LiDAR-to-Camera calibration through the reprojection of the pattern limit 3D points into the image using the following metrics: X and Y errors Root mean squared error This process requires the annotation of the pattern limit points in the image. After annotating once, if the user wish to repeat the process, the saved json file with the annotations can be loaded. For that the -ua flag has to be disabled. usage: range_sensor_to_camera_evaluation.py [-h] -train_json TRAIN_JSON_FILE -test_json TEST_JSON_FILE -ss SOURCE_SENSOR -ts TARGET_SENSOR [-si] -ef EVAL_FILE [-ua] optional arguments: -h, --help show this help message and exit -train_json TRAIN_JSON_FILE, --train_json_file TRAIN_JSON_FILE Json file containing input training dataset. -test_json TEST_JSON_FILE, --test_json_file TEST_JSON_FILE Json file containing input testing dataset. -ss SOURCE_SENSOR, --source_sensor SOURCE_SENSOR Source transformation sensor. -ts TARGET_SENSOR, --target_sensor TARGET_SENSOR Target transformation sensor. -si, --show_images If true the script shows images. -ef EVAL_FILE, --eval_file EVAL_FILE Path to file to read and/or write the evalutation data. -ua, --use_annotation If true, the limit points will be manually annotated. Point cloud image projection atom_evaluation also allows the user to visualize the point cloud projected into an image to check the calibration. usage: point_cloud_to_image.py [-h] -json JSON_FILE -ls LIDAR_SENSOR -cs CAMERA_SENSOR optional arguments: -h, --help show this help message and exit -json JSON_FILE, --json_file JSON_FILE Json file containing input dataset. -ls LIDAR_SENSOR, --lidar_sensor LIDAR_SENSOR LiDAR sensor name. -cs CAMERA_SENSOR, --camera_sensor CAMERA_SENSOR Camera sensor name. How to run: rosrun atom_evaluation point_cloud_to_image.py -json <path_to_test_json> -ls <lidar_sensor_name> -cs <camera_sensor_name>","title":"Evaluations"},{"location":"evaluations/#evaluating-your-calibration","text":"After the system is calibrated one common concern is to be able to assess the accuracy of the produced calibration. ATOM provides several evaluation scripts for this purpose. Unlike ATOM which calibrates all sensors simultaneously, evaluations are performed in pairs of sensors, which facilitates comparisons with other calibration approaches (which are mostly pairwise), e.g. opencv's stereo calibration . Thus, there is a different evaluation script for each combination of modalities.","title":"Evaluating your calibration"},{"location":"evaluations/#annotation-of-rgb-images","text":"To evaluate calibration between range sensors and cameras, it is necessary to annotate the physical limits on the calibration pattern in the images of the collection, to allow a comparison with physical labellings as measured by range sensors . rosrun atom_evaluation annotate_pattern_borders_in_rgb.py [-h] -d DATASET_FILE -cs CAMERA_SENSOR [-si] [-ww WINDOW_WIDTH] [-ps POINT_SIZE] [-ppp POINTS_PER_PIXEL] optional arguments: -h, --help show this help message and exit -d DATASET_FILE, --dataset_file DATASET_FILE Json file containing input dataset. -cs CAMERA_SENSOR, --camera_sensor CAMERA_SENSOR Source transformation sensor. -si, --show_images If true the script shows images. -ww WINDOW_WIDTH, --window_width WINDOW_WIDTH Width of the window. -ps POINT_SIZE, --point_size POINT_SIZE Size of points to draw on image. -ppp POINTS_PER_PIXEL, --points_per_pixel POINTS_PER_PIXEL How many points per pixel to sample between annotated points. Note: you must annotate each camera sensor present in your calibration system. These annotation will be used to evaluate both the lidar-camera pairs and depth-camera. How to annotate: click to add points in the currently selected pattern side (text in top left corner) if the pattern limit is viewed as a straight line in the image you may click only the corners, if needed you can click more points once a side is complete, move on to the n ext side by pressing \"n\" when the four sides are complete move to the next collection image by pressing \".\" The complete list of keys is printed when \"h\" is pressed. Be sure to label the corners in both intersected edges, ie, each corner should have two different coloured points. The result should be someting like this (for each image): Here is a video tutorial .","title":"Annotation of rgb images"},{"location":"evaluations/#rgb-to-rgb-camera-evaluation","text":"Evaluates de camera-to-camera reprojection error with the following metrics: X and Y errors Root mean squared error Translation and rotation errors usage: rgb_to_rgb_evaluation [-h] -train_json TRAIN_JSON_FILE -test_json TEST_JSON_FILE -ss SENSOR_SOURCE -st SENSOR_TARGET [-si] [-sg] optional arguments: -h, --help show this help message and exit -train_json TRAIN_JSON_FILE, --train_json_file TRAIN_JSON_FILE Json file containing train input dataset. -test_json TEST_JSON_FILE, --test_json_file TEST_JSON_FILE Json file containing test input dataset. -ss SENSOR_SOURCE, --sensor_source SENSOR_SOURCE Source transformation sensor. -st SENSOR_TARGET, --sensor_target SENSOR_TARGET Target transformation sensor. -si, --show_images If true the script shows images. -sg, --save_graphics Save reprojection error graphics. How to run: rosrun atom_evaluation rgb_to_rgb_evalutation.py -train_json <path_to_train_file> -test_json <path_to_test_file> -ss <source_sensor_name> -ts <target_sensor_name>","title":"RGB to RGB camera evaluation"},{"location":"evaluations/#lidar-to-depth-camera-evaluation","text":"How to run: rosrun atom_evaluation lidar_to_depth_evaluation.py -train_json <path_to_train_file> -test_json <path_to_test_file> -cs <camera_sensor_name> -rs <lidar_sensor_name> -si optional arguments: -h, --help show this help message and exit -train_json TRAIN_JSON_FILE, --train_json_file TRAIN_JSON_FILE Json file containing train input dataset. -test_json TEST_JSON_FILE, --test_json_file TEST_JSON_FILE Json file containing test input dataset. -ld SOURCE_SENSOR, --lidar_sensor SOURCE_SENSOR Source transformation sensor. -ds TARGET_SENSOR, --depth_sensor TARGET_SENSOR Target transformation sensor. -si, --show_images If true the script shows images.","title":"LiDAR to Depth Camera evaluation"},{"location":"evaluations/#rgb-to-depth-camera-evaluation","text":"How to run: rosrun atom_evaluation depth_sensor_to_camera_evaluation.py -train_json <path_to_train_file> -test_json <path_to_test_file> -cs <camera_sensor_name> -ds <depth_sensor_name> optional arguments: -h, --help show this help message and exit -train_json TRAIN_JSON_FILE, --train_json_file TRAIN_JSON_FILE Json file containing train input dataset. -test_json TEST_JSON_FILE, --test_json_file TEST_JSON_FILE Json file containing test input dataset. -cs SOURCE_SENSOR, --camera_sensor SOURCE_SENSOR Source transformation sensor. -ds TARGET_SENSOR, --depth_sensor TARGET_SENSOR Target transformation sensor. -si, --show_images If true the script shows images.","title":"RGB to Depth camera evaluation"},{"location":"evaluations/#lidar-to-lidar-evaluation","text":"How to run: rosrun atom_evaluation lidar_to_lidar.py -train_json <path_to_train_file> -test_json <path_to_test_file> -ld1 <source_lidar_sensor_name> -ld2 <target_lidar_sensor_name> optional arguments: -h, --help show this help message and exit -train_json TRAIN_JSON_FILE, --train_json_file TRAIN_JSON_FILE Json file containing train input dataset. -test_json TEST_JSON_FILE, --test_json_file TEST_JSON_FILE Json file containing test input dataset. -ld1 SOURCE_SENSOR, --lidar_sensor_1 SOURCE_SENSOR Source transformation sensor. -ld2 TARGET_SENSOR, --lidar_sensor_2 TARGET_SENSOR Target transformation sensor.","title":"LiDAR to LiDAR evaluation"},{"location":"evaluations/#lidar-to-rgb-camera-evaluation","text":"Evaluates the LiDAR-to-Camera calibration through the reprojection of the pattern limit 3D points into the image using the following metrics: X and Y errors Root mean squared error This process requires the annotation of the pattern limit points in the image. After annotating once, if the user wish to repeat the process, the saved json file with the annotations can be loaded. For that the -ua flag has to be disabled. usage: range_sensor_to_camera_evaluation.py [-h] -train_json TRAIN_JSON_FILE -test_json TEST_JSON_FILE -ss SOURCE_SENSOR -ts TARGET_SENSOR [-si] -ef EVAL_FILE [-ua] optional arguments: -h, --help show this help message and exit -train_json TRAIN_JSON_FILE, --train_json_file TRAIN_JSON_FILE Json file containing input training dataset. -test_json TEST_JSON_FILE, --test_json_file TEST_JSON_FILE Json file containing input testing dataset. -ss SOURCE_SENSOR, --source_sensor SOURCE_SENSOR Source transformation sensor. -ts TARGET_SENSOR, --target_sensor TARGET_SENSOR Target transformation sensor. -si, --show_images If true the script shows images. -ef EVAL_FILE, --eval_file EVAL_FILE Path to file to read and/or write the evalutation data. -ua, --use_annotation If true, the limit points will be manually annotated.","title":"LiDAR to RGB camera evaluation"},{"location":"evaluations/#point-cloud-image-projection","text":"atom_evaluation also allows the user to visualize the point cloud projected into an image to check the calibration. usage: point_cloud_to_image.py [-h] -json JSON_FILE -ls LIDAR_SENSOR -cs CAMERA_SENSOR optional arguments: -h, --help show this help message and exit -json JSON_FILE, --json_file JSON_FILE Json file containing input dataset. -ls LIDAR_SENSOR, --lidar_sensor LIDAR_SENSOR LiDAR sensor name. -cs CAMERA_SENSOR, --camera_sensor CAMERA_SENSOR Camera sensor name. How to run: rosrun atom_evaluation point_cloud_to_image.py -json <path_to_test_json> -ls <lidar_sensor_name> -cs <camera_sensor_name>","title":"Point cloud image projection"},{"location":"examples/","text":"Calibration Examples ATOM provides extensive visualization possibilities while running the calibration optimization procedure. To visualize in ROS Rviz use the -rv flag. So far, we have used ATOM to successfully calibrate several robotic platforms. Here are some examples: Atlascar2 Atlascar2 is an intelligent vehicle containing several cameras and 2D Lidars. IrisUA - ur10e The IrisUA - ur10e includes several variants of the hand-eye calibration problem. AgrobV2 AgrobV2 is a mobile robot with a stereo camera and a 3D Lidar designed for agriculture robotics. Calibration of AgrobV2. LARCC L aboratory of A utomation and R obotics C ollaborative C ell (LARCC) is included in a research project focusing of collaborative robotic industrial cells. The goal is to monitor in detail the volume of the cell in order to ensure safe collaboration between human operators and robots. For this, several sensors of different modalities are positioned everywhere in the cell, which makes the calibration of this robotic system a challenging task. Sensor fields of view in LARCC. Calibration of LARCC. MMTBot MMTBot is a simulated robotic system containing a manipulator, two rgb cameras and one 3D lidar, with the goal of reserching how ATOM can calibration hand-eye systems.","title":"Calibration Examples"},{"location":"examples/#calibration-examples","text":"ATOM provides extensive visualization possibilities while running the calibration optimization procedure. To visualize in ROS Rviz use the -rv flag. So far, we have used ATOM to successfully calibrate several robotic platforms. Here are some examples:","title":"Calibration Examples"},{"location":"examples/#atlascar2","text":"Atlascar2 is an intelligent vehicle containing several cameras and 2D Lidars.","title":"Atlascar2"},{"location":"examples/#irisua-ur10e","text":"The IrisUA - ur10e includes several variants of the hand-eye calibration problem.","title":"IrisUA - ur10e"},{"location":"examples/#agrobv2","text":"AgrobV2 is a mobile robot with a stereo camera and a 3D Lidar designed for agriculture robotics. Calibration of AgrobV2.","title":"AgrobV2"},{"location":"examples/#larcc","text":"L aboratory of A utomation and R obotics C ollaborative C ell (LARCC) is included in a research project focusing of collaborative robotic industrial cells. The goal is to monitor in detail the volume of the cell in order to ensure safe collaboration between human operators and robots. For this, several sensors of different modalities are positioned everywhere in the cell, which makes the calibration of this robotic system a challenging task. Sensor fields of view in LARCC. Calibration of LARCC.","title":"LARCC"},{"location":"examples/#mmtbot","text":"MMTBot is a simulated robotic system containing a manipulator, two rgb cameras and one 3D lidar, with the goal of reserching how ATOM can calibration hand-eye systems.","title":"MMTBot"},{"location":"publications/","text":"Publications Oliveira, M., A. Castro, T. Madeira, E. Pedrosa, P. Dias, and V. Santos, A ROS framework for the extrinsic calibration of intelligent vehicles: A multi-sensor, multi-modal approach, Robotics and Autonomous Systems (2020) p. 103558, ISSN: 0921-8890, DOI: https://doi.org/10.1016/j.robot.2020.103558, 2020. Bibtex . Pedrosa, E., M. Oliveira, N. Lau, and V. Santos, A General Approach to Hand\u2013Eye Calibration Through the Optimization of Atomic Transformations, IEEE Transactions on Robotics (2021) pp. 1\u201315, DOI: https://doi.org/10.1109/TRO.2021.3062306, 2021. Bibtex . Aguiar, A., M. Oliveira, E. Pedrosa, and F. Santos, A Camera to LiDAR calibration approach through the Optimization of Atomic Transformations, Expert Systems with Applications (2021) p. 114894, ISSN: 0957-4174, DOI: https://doi.org/10.1016/j.eswa.2021.114894, 2021. Bibtex","title":"Publications"},{"location":"publications/#publications","text":"Oliveira, M., A. Castro, T. Madeira, E. Pedrosa, P. Dias, and V. Santos, A ROS framework for the extrinsic calibration of intelligent vehicles: A multi-sensor, multi-modal approach, Robotics and Autonomous Systems (2020) p. 103558, ISSN: 0921-8890, DOI: https://doi.org/10.1016/j.robot.2020.103558, 2020. Bibtex . Pedrosa, E., M. Oliveira, N. Lau, and V. Santos, A General Approach to Hand\u2013Eye Calibration Through the Optimization of Atomic Transformations, IEEE Transactions on Robotics (2021) pp. 1\u201315, DOI: https://doi.org/10.1109/TRO.2021.3062306, 2021. Bibtex . Aguiar, A., M. Oliveira, E. Pedrosa, and F. Santos, A Camera to LiDAR calibration approach through the Optimization of Atomic Transformations, Expert Systems with Applications (2021) p. 114894, ISSN: 0957-4174, DOI: https://doi.org/10.1016/j.eswa.2021.114894, 2021. Bibtex","title":"Publications"},{"location":"quick_start/","text":"Quick Start Unlike most other calibration approaches, ATOM offers tools to address the complete calibration pipeline. These are instructions for quick starting your robotic system calibration. If you need more details read through the detailed description below. Create a calibration package rosrun atom_calibration create_calibration_pkg --name <your_robot_calibration> Configure a calibration package Edit the file: <your_robot_calibration>/calibration/config.yml_ with your system information. and then run: rosrun <your_robot_calibration> configure Set initial estimate ATOM provides interactive tools based on rviz that allow the user to set the pose of the sensors to be calibrated, while receiving visual feedback. Optional If you consider that your initial sensor poses are already accurate, you may skip this procedure. To use launch: roslaunch <your_robot_calibration> set_initial_estimate.launch Collect Data Collecting data produces an ATOM dataset, which is then used for calibrating the system. roslaunch <your_robot_calibration> collect_data.launch output_folder:=~/datasets/<my_dataset> Dataset playback Dataset playback offers the possibility to visualize and correct the labels automatically produced during the collection stage. Optional If you trust that the automatic labels are correct, you may skip this procedure. First launch the visualizer: roslaunch <your_robot_calibration> dataset_playback.launch and then: rosrun atom_calibration dataset_playback -json $ATOM_DATASETS/<your_robot_calibration>/<your_dataset>/dataset.json -uic -si -ow Calibrate sensors Finally run an optimization that will calibrate your sensors: roslaunch <your_robot_calibration> dataset_playback.launch","title":"Quick Start"},{"location":"quick_start/#quick-start","text":"Unlike most other calibration approaches, ATOM offers tools to address the complete calibration pipeline. These are instructions for quick starting your robotic system calibration. If you need more details read through the detailed description below.","title":"Quick Start"},{"location":"quick_start/#create-a-calibration-package","text":"rosrun atom_calibration create_calibration_pkg --name <your_robot_calibration>","title":"Create a calibration package"},{"location":"quick_start/#configure-a-calibration-package","text":"Edit the file: <your_robot_calibration>/calibration/config.yml_ with your system information. and then run: rosrun <your_robot_calibration> configure","title":"Configure a calibration package"},{"location":"quick_start/#set-initial-estimate","text":"ATOM provides interactive tools based on rviz that allow the user to set the pose of the sensors to be calibrated, while receiving visual feedback. Optional If you consider that your initial sensor poses are already accurate, you may skip this procedure. To use launch: roslaunch <your_robot_calibration> set_initial_estimate.launch","title":"Set initial estimate"},{"location":"quick_start/#collect-data","text":"Collecting data produces an ATOM dataset, which is then used for calibrating the system. roslaunch <your_robot_calibration> collect_data.launch output_folder:=~/datasets/<my_dataset>","title":"Collect Data"},{"location":"quick_start/#dataset-playback","text":"Dataset playback offers the possibility to visualize and correct the labels automatically produced during the collection stage. Optional If you trust that the automatic labels are correct, you may skip this procedure. First launch the visualizer: roslaunch <your_robot_calibration> dataset_playback.launch and then: rosrun atom_calibration dataset_playback -json $ATOM_DATASETS/<your_robot_calibration>/<your_dataset>/dataset.json -uic -si -ow","title":"Dataset playback"},{"location":"quick_start/#calibrate-sensors","text":"Finally run an optimization that will calibrate your sensors: roslaunch <your_robot_calibration> dataset_playback.launch","title":"Calibrate sensors"}]}