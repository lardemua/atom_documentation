{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"ATOM ATOM What is ATOM? Calibration Pipeline Robotic System Configuration Data Logging Initial Positioning of Sensors Data Collection and Labeling Dataset playback Calibration Calibration Evaluation Running the calibrated system What is a label? What is a collection? What is an ATOM dataset? ROS1 and ROS2 What is ATOM? ATOM is a calibration framework using the A tomic T ransformations O ptimization M ethod. https://github.com/lardemua/atom It contains a set of calibration tools for multi-sensor, multi-modal, robotic systems, based on the optimization of atomic transformations, as provided by a ROS based robot description. Moreover, it provides several scripts to facilitate all the steps of a calibration procedure. If this work is helpful for you please cite our publications . Calibration Pipeline In order to calibrate a robotic system one needs to carry out several tasks, such as acquiring data, labeling data, executing the calibration, interpreting the result of the calibration, etc. ATOM provides several scripts to address all the stages of a calibration procedure. These scripts are seamlessly integrated into ROS , and make use of RViz to provide extensive visualization functionalites. We have divided the calibration procedure into several stages, shown in the scheme below. ATOM calibration pipeline. The greyed out boxes are steps considered to be out of the scope of ATOM, i.e., these are tasks one should do in order to properly configure and run a robotic system in ROS, even if ATOM is not going to be used. Dashed line boxes represent steps which are optional, i.e., they may improve the calibration but are not essential to the procedure. Below we describe each of these stages, giving examples for the MMTBot . Robotic System Configuration Robotic System Configuration concerns the design and implementation of your robotic system in ROS. It generally involves the writing of an UDRF or a xacro file that describes the links and joints of your robotic system, among other things. If you are not familiar with this there are ROS tutorials to build your robot URDF and also to create your robot xacro . Note To calibrate your robot with ATOM, we recommend using xacro files instead of urdfs. This stage may also include de configuration of a simulation of your system in Gazebo . Typically one creates a couple of ros packages for our robot, as described below. <my_robot>_description ros package contains the xacro files that describe your robot. It sometimes also contains cad models inside a models folder. An example from MMTBot is here . <my_robot>_bringup ros package contains the launch files used to bringup your robotic system. Tipically there is a bringup.launch that starts the complete system. An example from the MMTBot . Data Logging Data Logging is the procedure by which a ros bagfile is recorded to be used in the calibration later on. You may record data by calling rosbag record directly, e.g.: rosbag record /topic1 ... /topicN -o output.bag We typically have a roslaunch file to record data and produce a bagfile for each robotic system. Here's the example for MMTBot . A bagfile should contain several topics, namely transformations and joint state messages, as well as messages produced by the sensors in the system. For example, in the case of MMTBot, which has sensors world_camera , hand_camera and lidar , we record the following topics: /tf /tf_static /joint_states /world_camera/rgb/image_raw/compressed /world_camera/rgb/camera_info /hand_camera/rgb/image_raw/compressed /hand_camera/rgb/camera_info /lidar/points Initial Positioning of Sensors The goal of this stage is to allow the user to define interactively the poses of each sensor, so that the optimization starts close to the optimal solution and thus avoids local minima. This stage may be skipped if the transformations from the URDF are believed to be \"sufficiently\" accurate. More details here . Data Collection and Labeling This stage reads the bagfile and allows the user to assist the labeling procedure, i.e., the identification of the calibration pattern in the data of the sensors, and then to decide when to save each collection. The output is an ATOM dataset . More details here . Dataset playback This stage is used to review ATOM datasets. The reviewing may identify incorrect labels, which can be corrected through manual annotation. It produces a corrected ATOM dataset . More details here . Calibration This is were finally the system is calibrated. ATOM provides extensive visualization functionalities so that it is possible to observe how the calibration is performing. More details here . Calibration Evaluation ATOM provides several scripts designed to measure the accuracy of the calibration. These tools are pairwise evaluations, which means it is possible to compare the accuracy of ATOM with other state of the art pairwise approaches. More details here . Running the calibrated system After calibration ATOM produces a calibrated URDF file which can be directly used in ROS. If you get here unscathed, you are a very lucky person :-) ... Enjoy! What is a label? A label is a set of data points selected from the complete sensor data, obtained through a labeling or pattern detection procedure. The representation of a label differs according to the sensor modality, as detailed next. RGB camera labels are a list of image pixel coordinates of the corners of the pattern. Labels for RGB modality. 3D Lidar labels contain the list of 3D point coordinates of the Lidar data points that belong to the pattern. In addition to this, 3D Lidar labels also contain a list of 3D points of the physical boundaries of the pattern. Labels for the 3D Lidar modality. Small green spheres are the pattern support points, larger green spheres are the points annotated as boundaries of the pattern. Depth modality labels contain a list of image pixel coordinates that are annotated to be inside the pattern. In addition to this, depth labels also contain a set of pixel coordinates of the boundaries of the pattern. Labels for the depth modality. Pattern support points in yellow, and boundaries in pink. What is a collection? A collection is a recording of the data from all the sensors in the system at a particular time instant selected by the user. Collections also contain information about labels for the sensor data, as well as the state of the robotic system at that time, i.e., all the transformations. Sensor data from an MMTBot collection. Incomplete collections A collection is referred to as an incomplete collection when it is not possible to detect the pattern in at least one of the sensors of the robotic system. What is an ATOM dataset? An ATOM dataset is a folder which contains data used for the calibration of a robotic system. Every ATOM dataset contains a dataset.json which provides details about the dataset, such as the defined configuration, the number of sensors, etc. Several scripts in the calibration pipeline require an ATOM dataset, but it is worth mentioning that the files are also human readable. Below you can see the structure of an ATOM dataset. Structure of an ATOM dataset json file. A dataset.json file contains a _metadata field, where details about the date, user and others are stored. It also contains a calibration_config , which is a copy of the state of the configuration file at the time of creation of the dataset. The sensors field describes each of the sensors in the system, in particular those selected to be calibrated. Finally, the collections field contains several collections, i.e. snapshots of data. Each collection contains a subfield data , that stores a dictionary obtained through the conversion of the ROS message to a python dictionary , the subfield labels contains information about annotations (automatic or manual) of each sensor data, and the subfield transforms contains all the transformations published at (or near) the time of the collection. In addition to the dataset.json file, ATOM datasets also contain dedicated files for larger data blobs, such as point clouds or images, which are saved separately in the same folder. Because the transformations are stored for each collection, it is possible to recover the complete state of the robotic system at the time of each collection. ATOM then provides visualization functionalities to display all collections at once. Below we can see the different poses of the manipulator and the calibration pattern for each collection of an MMTBot dataset. Several collections in an MMTBot dataset. Here is an ATOM dataset example from LARCC . ROS1 and ROS2 At the moment, ATOM is only supported in ROS1 (Noetic), Ubuntu 20.04 LTS. We have plans to work on a ROS2 version in the near future.","title":"Introduction"},{"location":"#atom","text":"ATOM What is ATOM? Calibration Pipeline Robotic System Configuration Data Logging Initial Positioning of Sensors Data Collection and Labeling Dataset playback Calibration Calibration Evaluation Running the calibrated system What is a label? What is a collection? What is an ATOM dataset? ROS1 and ROS2","title":"ATOM"},{"location":"#what-is-atom","text":"ATOM is a calibration framework using the A tomic T ransformations O ptimization M ethod. https://github.com/lardemua/atom It contains a set of calibration tools for multi-sensor, multi-modal, robotic systems, based on the optimization of atomic transformations, as provided by a ROS based robot description. Moreover, it provides several scripts to facilitate all the steps of a calibration procedure. If this work is helpful for you please cite our publications .","title":"What is ATOM?"},{"location":"#calibration-pipeline","text":"In order to calibrate a robotic system one needs to carry out several tasks, such as acquiring data, labeling data, executing the calibration, interpreting the result of the calibration, etc. ATOM provides several scripts to address all the stages of a calibration procedure. These scripts are seamlessly integrated into ROS , and make use of RViz to provide extensive visualization functionalites. We have divided the calibration procedure into several stages, shown in the scheme below. ATOM calibration pipeline. The greyed out boxes are steps considered to be out of the scope of ATOM, i.e., these are tasks one should do in order to properly configure and run a robotic system in ROS, even if ATOM is not going to be used. Dashed line boxes represent steps which are optional, i.e., they may improve the calibration but are not essential to the procedure. Below we describe each of these stages, giving examples for the MMTBot .","title":"Calibration Pipeline"},{"location":"#robotic-system-configuration","text":"Robotic System Configuration concerns the design and implementation of your robotic system in ROS. It generally involves the writing of an UDRF or a xacro file that describes the links and joints of your robotic system, among other things. If you are not familiar with this there are ROS tutorials to build your robot URDF and also to create your robot xacro . Note To calibrate your robot with ATOM, we recommend using xacro files instead of urdfs. This stage may also include de configuration of a simulation of your system in Gazebo . Typically one creates a couple of ros packages for our robot, as described below. <my_robot>_description ros package contains the xacro files that describe your robot. It sometimes also contains cad models inside a models folder. An example from MMTBot is here . <my_robot>_bringup ros package contains the launch files used to bringup your robotic system. Tipically there is a bringup.launch that starts the complete system. An example from the MMTBot .","title":"Robotic System Configuration"},{"location":"#data-logging","text":"Data Logging is the procedure by which a ros bagfile is recorded to be used in the calibration later on. You may record data by calling rosbag record directly, e.g.: rosbag record /topic1 ... /topicN -o output.bag We typically have a roslaunch file to record data and produce a bagfile for each robotic system. Here's the example for MMTBot . A bagfile should contain several topics, namely transformations and joint state messages, as well as messages produced by the sensors in the system. For example, in the case of MMTBot, which has sensors world_camera , hand_camera and lidar , we record the following topics: /tf /tf_static /joint_states /world_camera/rgb/image_raw/compressed /world_camera/rgb/camera_info /hand_camera/rgb/image_raw/compressed /hand_camera/rgb/camera_info /lidar/points","title":"Data Logging"},{"location":"#initial-positioning-of-sensors","text":"The goal of this stage is to allow the user to define interactively the poses of each sensor, so that the optimization starts close to the optimal solution and thus avoids local minima. This stage may be skipped if the transformations from the URDF are believed to be \"sufficiently\" accurate. More details here .","title":"Initial Positioning of Sensors"},{"location":"#data-collection-and-labeling","text":"This stage reads the bagfile and allows the user to assist the labeling procedure, i.e., the identification of the calibration pattern in the data of the sensors, and then to decide when to save each collection. The output is an ATOM dataset . More details here .","title":"Data Collection and Labeling"},{"location":"#dataset-playback","text":"This stage is used to review ATOM datasets. The reviewing may identify incorrect labels, which can be corrected through manual annotation. It produces a corrected ATOM dataset . More details here .","title":"Dataset playback"},{"location":"#calibration","text":"This is were finally the system is calibrated. ATOM provides extensive visualization functionalities so that it is possible to observe how the calibration is performing. More details here .","title":"Calibration"},{"location":"#calibration-evaluation","text":"ATOM provides several scripts designed to measure the accuracy of the calibration. These tools are pairwise evaluations, which means it is possible to compare the accuracy of ATOM with other state of the art pairwise approaches. More details here .","title":"Calibration Evaluation"},{"location":"#running-the-calibrated-system","text":"After calibration ATOM produces a calibrated URDF file which can be directly used in ROS. If you get here unscathed, you are a very lucky person :-) ... Enjoy!","title":"Running the calibrated system"},{"location":"#what-is-a-label","text":"A label is a set of data points selected from the complete sensor data, obtained through a labeling or pattern detection procedure. The representation of a label differs according to the sensor modality, as detailed next. RGB camera labels are a list of image pixel coordinates of the corners of the pattern. Labels for RGB modality. 3D Lidar labels contain the list of 3D point coordinates of the Lidar data points that belong to the pattern. In addition to this, 3D Lidar labels also contain a list of 3D points of the physical boundaries of the pattern. Labels for the 3D Lidar modality. Small green spheres are the pattern support points, larger green spheres are the points annotated as boundaries of the pattern. Depth modality labels contain a list of image pixel coordinates that are annotated to be inside the pattern. In addition to this, depth labels also contain a set of pixel coordinates of the boundaries of the pattern. Labels for the depth modality. Pattern support points in yellow, and boundaries in pink.","title":"What is a label?"},{"location":"#what-is-a-collection","text":"A collection is a recording of the data from all the sensors in the system at a particular time instant selected by the user. Collections also contain information about labels for the sensor data, as well as the state of the robotic system at that time, i.e., all the transformations. Sensor data from an MMTBot collection. Incomplete collections A collection is referred to as an incomplete collection when it is not possible to detect the pattern in at least one of the sensors of the robotic system.","title":"What is a collection?"},{"location":"#what-is-an-atom-dataset","text":"An ATOM dataset is a folder which contains data used for the calibration of a robotic system. Every ATOM dataset contains a dataset.json which provides details about the dataset, such as the defined configuration, the number of sensors, etc. Several scripts in the calibration pipeline require an ATOM dataset, but it is worth mentioning that the files are also human readable. Below you can see the structure of an ATOM dataset. Structure of an ATOM dataset json file. A dataset.json file contains a _metadata field, where details about the date, user and others are stored. It also contains a calibration_config , which is a copy of the state of the configuration file at the time of creation of the dataset. The sensors field describes each of the sensors in the system, in particular those selected to be calibrated. Finally, the collections field contains several collections, i.e. snapshots of data. Each collection contains a subfield data , that stores a dictionary obtained through the conversion of the ROS message to a python dictionary , the subfield labels contains information about annotations (automatic or manual) of each sensor data, and the subfield transforms contains all the transformations published at (or near) the time of the collection. In addition to the dataset.json file, ATOM datasets also contain dedicated files for larger data blobs, such as point clouds or images, which are saved separately in the same folder. Because the transformations are stored for each collection, it is possible to recover the complete state of the robotic system at the time of each collection. ATOM then provides visualization functionalities to display all collections at once. Below we can see the different poses of the manipulator and the calibration pattern for each collection of an MMTBot dataset. Several collections in an MMTBot dataset. Here is an ATOM dataset example from LARCC .","title":"What is an ATOM dataset?"},{"location":"#ros1-and-ros2","text":"At the moment, ATOM is only supported in ROS1 (Noetic), Ubuntu 20.04 LTS. We have plans to work on a ROS2 version in the near future.","title":"ROS1 and ROS2"},{"location":"acknowledgment/","text":"Acknowledgements Citing ATOM Contributors Current Maintainers Citing ATOM Oliveira, M., E. Pedrosa, A. Aguiar, D. Rato, F. Santos, P. Dias, V. Santos, ATOM: A general calibration framework for multi-modal, multi-sensor systems, Expert Systems with Applications (2022) , 118000, ISSN 0957-4174, https://doi.org/10.1016/j.eswa.2022.118000 . Bibtex . Rato, D., M. Oliveira, V. Santos, M. Gomes, A. Sappa, A sensor-to-pattern calibration framework for multi-modal industrial collaborative cells, Journal of Manufacturing Systems (2022) , Volume 64, Pages 497-507, ISSN 0278-6125, https://doi.org/10.1016/j.jmsy.2022.07.006 , 2022. Bibtex . Pedrosa, E., M. Oliveira, N. Lau, and V. Santos, A General Approach to Hand\u2013Eye Calibration Through the Optimization of Atomic Transformations, IEEE Transactions on Robotics (2021) pp. 1\u201315, DOI: https://doi.org/10.1109/TRO.2021.3062306 , 2021. Bibtex . Aguiar, A., M. Oliveira, E. Pedrosa, and F. Santos, A Camera to LiDAR calibration approach through the Optimization of Atomic Transformations, Expert Systems with Applications (2021) p. 114894, ISSN: 0957-4174, DOI: https://doi.org/10.1016/j.eswa.2021.114894 , 2021. Bibtex . Oliveira, M., A. Castro, T. Madeira, E. Pedrosa, P. Dias, and V. Santos, A ROS framework for the extrinsic calibration of intelligent vehicles: A multi-sensor, multi-modal approach, Robotics and Autonomous Systems (2020) p. 103558, ISSN: 0921-8890, DOI: https://doi.org/10.1016/j.robot.2020.103558 , 2020. Bibtex . Oliveira, M., A. Castro, T. Madeira, P. Dias,V. Santos, (2019). A General Approach to the Extrinsic Calibration of Intelligent Vehicles Using ROS. ROBOT. bibtex Contributors This project started in 2017, and since then many people have contributed to a lesser or larger degree. We list all of them here in alphabetical order: Andr\u00e9 Aguiar, Afonso Castro, Daniel Coelho, Paulo Dias, Manuel Gomes, Tiago Madeira, Miguel Riem Oliveira, Eurico Pedrosa, Daniela Rato, Vitor Santos. Current Maintainers Miguel Riem Oliveira - University of Aveiro Daniela Rato - University of Aveiro Manuel Gomes - University of Aveiro","title":"Acknowledgment"},{"location":"acknowledgment/#acknowledgements","text":"Citing ATOM Contributors Current Maintainers","title":"Acknowledgements"},{"location":"acknowledgment/#citing-atom","text":"Oliveira, M., E. Pedrosa, A. Aguiar, D. Rato, F. Santos, P. Dias, V. Santos, ATOM: A general calibration framework for multi-modal, multi-sensor systems, Expert Systems with Applications (2022) , 118000, ISSN 0957-4174, https://doi.org/10.1016/j.eswa.2022.118000 . Bibtex . Rato, D., M. Oliveira, V. Santos, M. Gomes, A. Sappa, A sensor-to-pattern calibration framework for multi-modal industrial collaborative cells, Journal of Manufacturing Systems (2022) , Volume 64, Pages 497-507, ISSN 0278-6125, https://doi.org/10.1016/j.jmsy.2022.07.006 , 2022. Bibtex . Pedrosa, E., M. Oliveira, N. Lau, and V. Santos, A General Approach to Hand\u2013Eye Calibration Through the Optimization of Atomic Transformations, IEEE Transactions on Robotics (2021) pp. 1\u201315, DOI: https://doi.org/10.1109/TRO.2021.3062306 , 2021. Bibtex . Aguiar, A., M. Oliveira, E. Pedrosa, and F. Santos, A Camera to LiDAR calibration approach through the Optimization of Atomic Transformations, Expert Systems with Applications (2021) p. 114894, ISSN: 0957-4174, DOI: https://doi.org/10.1016/j.eswa.2021.114894 , 2021. Bibtex . Oliveira, M., A. Castro, T. Madeira, E. Pedrosa, P. Dias, and V. Santos, A ROS framework for the extrinsic calibration of intelligent vehicles: A multi-sensor, multi-modal approach, Robotics and Autonomous Systems (2020) p. 103558, ISSN: 0921-8890, DOI: https://doi.org/10.1016/j.robot.2020.103558 , 2020. Bibtex . Oliveira, M., A. Castro, T. Madeira, P. Dias,V. Santos, (2019). A General Approach to the Extrinsic Calibration of Intelligent Vehicles Using ROS. ROBOT. bibtex","title":"Citing ATOM"},{"location":"acknowledgment/#contributors","text":"This project started in 2017, and since then many people have contributed to a lesser or larger degree. We list all of them here in alphabetical order: Andr\u00e9 Aguiar, Afonso Castro, Daniel Coelho, Paulo Dias, Manuel Gomes, Tiago Madeira, Miguel Riem Oliveira, Eurico Pedrosa, Daniela Rato, Vitor Santos.","title":"Contributors"},{"location":"acknowledgment/#current-maintainers","text":"Miguel Riem Oliveira - University of Aveiro Daniela Rato - University of Aveiro Manuel Gomes - University of Aveiro","title":"Current Maintainers"},{"location":"evaluations/","text":"Evaluation Procedures Evaluation Procedures Annotation of rgb images RGB to RGB camera evaluation LiDAR to Depth Camera evaluation RGB to Depth camera evaluation LiDAR to LiDAR evaluation LiDAR to RGB camera evaluation Point cloud image projection Ground truth frame evaluation After the system is calibrated one common concern is to be able to assess the accuracy of the produced calibration. ATOM provides several evaluation scripts for this purpose. Unlike ATOM which calibrates all sensors simultaneously, evaluations are performed in pairs of sensors, which facilitates comparisons with other calibration approaches (which are mostly pairwise), e.g. opencv's stereo calibration . Thus, there is a different evaluation script for each combination of modalities. The ATOM evaluation scripts often request for a train dataset and a test dataset. The train dataset is the dataset which is used to conduct the calibration, while the test dataset is a dataset which was not used for calibration. Results are reported for the test dataset using the geometric transformations estimated during the calibration which were copied to the train dataset json. These are meant to be two separate datasets, but if you can also use a single dataset by indicating it both as the train and the test dataset. In this case, however, you should be aware that the results are being computed with data that was used for training. Annotation of rgb images To evaluate calibration between range sensors and cameras, it is necessary to annotate the physical limits on the calibration pattern in the images of the collection, to allow a comparison with physical labelings as measured by range sensors . rosrun atom_evaluation annotate_pattern_borders_in_rgb.py [-h] -d DATASET_FILE -cs CAMERA_SENSOR [-si] [-ww WINDOW_WIDTH] [-ps POINT_SIZE] [-ppp POINTS_PER_PIXEL] optional arguments: -h, --help show this help message and exit -d DATASET_FILE, --dataset_file DATASET_FILE Json file containing input dataset. -cs CAMERA_SENSOR, --camera_sensor CAMERA_SENSOR Source transformation sensor. -si, --show_images If true the script shows images. -ww WINDOW_WIDTH, --window_width WINDOW_WIDTH Width of the window. -ps POINT_SIZE, --point_size POINT_SIZE Size of points to draw on image. -ppp POINTS_PER_PIXEL, --points_per_pixel POINTS_PER_PIXEL How many points per pixel to sample between annotated points. Note: you must annotate each camera sensor present in your calibration system. These annotation will be used to evaluate both the lidar-camera pairs and depth-camera. How to annotate: click to add points in the currently selected pattern side (text in top left corner) if the pattern limit is viewed as a straight line in the image you may click only the corners, if needed you can click more points once a side is complete, move on to the n ext side by pressing \"n\" when the four sides are complete move to the next collection image by pressing \".\" The complete list of keys is printed when \"h\" is pressed. Be sure to label the corners in both intersected edges, ie, each corner should have two different coloured points. The result should be something like this (for each image): Annotation of the boundaries of the pattern in RGB images. Here is a video tutorial . RGB to RGB camera evaluation Evaluates de camera-to-camera reprojection error with the following metrics: X and Y errors Root mean squared error Translation and rotation errors usage: rgb_to_rgb_evaluation [-h] -train_json TRAIN_JSON_FILE -test_json TEST_JSON_FILE -ss SENSOR_SOURCE -st SENSOR_TARGET [-si] [-sg] optional arguments: -h, --help show this help message and exit -train_json TRAIN_JSON_FILE, --train_json_file TRAIN_JSON_FILE Json file containing train input dataset. -test_json TEST_JSON_FILE, --test_json_file TEST_JSON_FILE Json file containing test input dataset. -ss SENSOR_SOURCE, --sensor_source SENSOR_SOURCE Source transformation sensor. -st SENSOR_TARGET, --sensor_target SENSOR_TARGET Target transformation sensor. -si, --show_images If true the script shows images. -sg, --save_graphics Save reprojection error graphics. How to run: rosrun atom_evaluation rgb_to_rgb_evaluation.py -train_json <path_to_train_file> -test_json <path_to_test_file> -ss <source_sensor_name> -ts <target_sensor_name> LiDAR to Depth Camera evaluation How to run: rosrun atom_evaluation lidar_to_depth_evaluation.py -train_json <path_to_train_file> -test_json <path_to_test_file> -cs <camera_sensor_name> -rs <lidar_sensor_name> -si optional arguments: -h, --help show this help message and exit -train_json TRAIN_JSON_FILE, --train_json_file TRAIN_JSON_FILE Json file containing train input dataset. -test_json TEST_JSON_FILE, --test_json_file TEST_JSON_FILE Json file containing test input dataset. -ld SOURCE_SENSOR, --lidar_sensor SOURCE_SENSOR Source transformation sensor. -ds TARGET_SENSOR, --depth_sensor TARGET_SENSOR Target transformation sensor. -si, --show_images If true the script shows images. RGB to Depth camera evaluation How to run: rosrun atom_evaluation depth_sensor_to_camera_evaluation.py -train_json <path_to_train_file> -test_json <path_to_test_file> -cs <camera_sensor_name> -ds <depth_sensor_name> optional arguments: -h, --help show this help message and exit -train_json TRAIN_JSON_FILE, --train_json_file TRAIN_JSON_FILE Json file containing train input dataset. -test_json TEST_JSON_FILE, --test_json_file TEST_JSON_FILE Json file containing test input dataset. -cs SOURCE_SENSOR, --camera_sensor SOURCE_SENSOR Source transformation sensor. -ds TARGET_SENSOR, --depth_sensor TARGET_SENSOR Target transformation sensor. -si, --show_images If true the script shows images. LiDAR to LiDAR evaluation How to run: rosrun atom_evaluation lidar_to_lidar.py -train_json <path_to_train_file> -test_json <path_to_test_file> -ld1 <source_lidar_sensor_name> -ld2 <target_lidar_sensor_name> optional arguments: -h, --help show this help message and exit -train_json TRAIN_JSON_FILE, --train_json_file TRAIN_JSON_FILE Json file containing train input dataset. -test_json TEST_JSON_FILE, --test_json_file TEST_JSON_FILE Json file containing test input dataset. -ld1 SOURCE_SENSOR, --lidar_sensor_1 SOURCE_SENSOR Source transformation sensor. -ld2 TARGET_SENSOR, --lidar_sensor_2 TARGET_SENSOR Target transformation sensor. LiDAR to RGB camera evaluation Evaluates the LiDAR-to-Camera calibration through the reprojection of the pattern limit 3D points into the image using the following metrics: X and Y errors Root mean squared error This process requires the annotation of the pattern limit points in the image. After annotating once, if the user wish to repeat the process, the saved json file with the annotations can be loaded. For that the -ua flag has to be disabled. usage: range_sensor_to_camera_evaluation.py [-h] -train_json TRAIN_JSON_FILE -test_json TEST_JSON_FILE -ss SOURCE_SENSOR -ts TARGET_SENSOR [-si] -ef EVAL_FILE [-ua] optional arguments: -h, --help show this help message and exit -train_json TRAIN_JSON_FILE, --train_json_file TRAIN_JSON_FILE Json file containing input training dataset. -test_json TEST_JSON_FILE, --test_json_file TEST_JSON_FILE Json file containing input testing dataset. -ss SOURCE_SENSOR, --source_sensor SOURCE_SENSOR Source transformation sensor. -ts TARGET_SENSOR, --target_sensor TARGET_SENSOR Target transformation sensor. -si, --show_images If true the script shows images. -ef EVAL_FILE, --eval_file EVAL_FILE Path to file to read and/or write the evaluation data. -ua, --use_annotation If true, the limit points will be manually annotated. Point cloud image projection atom_evaluation also allows the user to visualize the point cloud projected into an image to check the calibration. usage: point_cloud_to_image.py [-h] -json JSON_FILE -ls LIDAR_SENSOR -cs CAMERA_SENSOR optional arguments: -h, --help show this help message and exit -json JSON_FILE, --json_file JSON_FILE Json file containing input dataset. -ls LIDAR_SENSOR, --lidar_sensor LIDAR_SENSOR LiDAR sensor name. -cs CAMERA_SENSOR, --camera_sensor CAMERA_SENSOR Camera sensor name. How to run: rosrun atom_evaluation point_cloud_to_image.py -json <path_to_test_json> -ls <lidar_sensor_name> -cs <camera_sensor_name> Ground Truth Frame Evaluation When using simulation it is possible to know that a given dataset contains \"perfect\" value for the transformations to be estimated. Using this, we can run a comparison between the ground_truth dataset and the calibrated dataset, and assess if the estimated transformations are accurate. To run use: rosrun atom_evaluation ground_truth_frame_evaluation -train_json -test_json The script produces a table where you can inspect the errors. It is also possible to configure which frames are analyzed. Use --help to see the options.","title":"Evaluation Procedures"},{"location":"evaluations/#evaluation-procedures","text":"Evaluation Procedures Annotation of rgb images RGB to RGB camera evaluation LiDAR to Depth Camera evaluation RGB to Depth camera evaluation LiDAR to LiDAR evaluation LiDAR to RGB camera evaluation Point cloud image projection Ground truth frame evaluation After the system is calibrated one common concern is to be able to assess the accuracy of the produced calibration. ATOM provides several evaluation scripts for this purpose. Unlike ATOM which calibrates all sensors simultaneously, evaluations are performed in pairs of sensors, which facilitates comparisons with other calibration approaches (which are mostly pairwise), e.g. opencv's stereo calibration . Thus, there is a different evaluation script for each combination of modalities. The ATOM evaluation scripts often request for a train dataset and a test dataset. The train dataset is the dataset which is used to conduct the calibration, while the test dataset is a dataset which was not used for calibration. Results are reported for the test dataset using the geometric transformations estimated during the calibration which were copied to the train dataset json. These are meant to be two separate datasets, but if you can also use a single dataset by indicating it both as the train and the test dataset. In this case, however, you should be aware that the results are being computed with data that was used for training.","title":"Evaluation Procedures"},{"location":"evaluations/#annotation-of-rgb-images","text":"To evaluate calibration between range sensors and cameras, it is necessary to annotate the physical limits on the calibration pattern in the images of the collection, to allow a comparison with physical labelings as measured by range sensors . rosrun atom_evaluation annotate_pattern_borders_in_rgb.py [-h] -d DATASET_FILE -cs CAMERA_SENSOR [-si] [-ww WINDOW_WIDTH] [-ps POINT_SIZE] [-ppp POINTS_PER_PIXEL] optional arguments: -h, --help show this help message and exit -d DATASET_FILE, --dataset_file DATASET_FILE Json file containing input dataset. -cs CAMERA_SENSOR, --camera_sensor CAMERA_SENSOR Source transformation sensor. -si, --show_images If true the script shows images. -ww WINDOW_WIDTH, --window_width WINDOW_WIDTH Width of the window. -ps POINT_SIZE, --point_size POINT_SIZE Size of points to draw on image. -ppp POINTS_PER_PIXEL, --points_per_pixel POINTS_PER_PIXEL How many points per pixel to sample between annotated points. Note: you must annotate each camera sensor present in your calibration system. These annotation will be used to evaluate both the lidar-camera pairs and depth-camera. How to annotate: click to add points in the currently selected pattern side (text in top left corner) if the pattern limit is viewed as a straight line in the image you may click only the corners, if needed you can click more points once a side is complete, move on to the n ext side by pressing \"n\" when the four sides are complete move to the next collection image by pressing \".\" The complete list of keys is printed when \"h\" is pressed. Be sure to label the corners in both intersected edges, ie, each corner should have two different coloured points. The result should be something like this (for each image): Annotation of the boundaries of the pattern in RGB images. Here is a video tutorial .","title":"Annotation of rgb images"},{"location":"evaluations/#rgb-to-rgb-camera-evaluation","text":"Evaluates de camera-to-camera reprojection error with the following metrics: X and Y errors Root mean squared error Translation and rotation errors usage: rgb_to_rgb_evaluation [-h] -train_json TRAIN_JSON_FILE -test_json TEST_JSON_FILE -ss SENSOR_SOURCE -st SENSOR_TARGET [-si] [-sg] optional arguments: -h, --help show this help message and exit -train_json TRAIN_JSON_FILE, --train_json_file TRAIN_JSON_FILE Json file containing train input dataset. -test_json TEST_JSON_FILE, --test_json_file TEST_JSON_FILE Json file containing test input dataset. -ss SENSOR_SOURCE, --sensor_source SENSOR_SOURCE Source transformation sensor. -st SENSOR_TARGET, --sensor_target SENSOR_TARGET Target transformation sensor. -si, --show_images If true the script shows images. -sg, --save_graphics Save reprojection error graphics. How to run: rosrun atom_evaluation rgb_to_rgb_evaluation.py -train_json <path_to_train_file> -test_json <path_to_test_file> -ss <source_sensor_name> -ts <target_sensor_name>","title":"RGB to RGB camera evaluation"},{"location":"evaluations/#lidar-to-depth-camera-evaluation","text":"How to run: rosrun atom_evaluation lidar_to_depth_evaluation.py -train_json <path_to_train_file> -test_json <path_to_test_file> -cs <camera_sensor_name> -rs <lidar_sensor_name> -si optional arguments: -h, --help show this help message and exit -train_json TRAIN_JSON_FILE, --train_json_file TRAIN_JSON_FILE Json file containing train input dataset. -test_json TEST_JSON_FILE, --test_json_file TEST_JSON_FILE Json file containing test input dataset. -ld SOURCE_SENSOR, --lidar_sensor SOURCE_SENSOR Source transformation sensor. -ds TARGET_SENSOR, --depth_sensor TARGET_SENSOR Target transformation sensor. -si, --show_images If true the script shows images.","title":"LiDAR to Depth Camera evaluation"},{"location":"evaluations/#rgb-to-depth-camera-evaluation","text":"How to run: rosrun atom_evaluation depth_sensor_to_camera_evaluation.py -train_json <path_to_train_file> -test_json <path_to_test_file> -cs <camera_sensor_name> -ds <depth_sensor_name> optional arguments: -h, --help show this help message and exit -train_json TRAIN_JSON_FILE, --train_json_file TRAIN_JSON_FILE Json file containing train input dataset. -test_json TEST_JSON_FILE, --test_json_file TEST_JSON_FILE Json file containing test input dataset. -cs SOURCE_SENSOR, --camera_sensor SOURCE_SENSOR Source transformation sensor. -ds TARGET_SENSOR, --depth_sensor TARGET_SENSOR Target transformation sensor. -si, --show_images If true the script shows images.","title":"RGB to Depth camera evaluation"},{"location":"evaluations/#lidar-to-lidar-evaluation","text":"How to run: rosrun atom_evaluation lidar_to_lidar.py -train_json <path_to_train_file> -test_json <path_to_test_file> -ld1 <source_lidar_sensor_name> -ld2 <target_lidar_sensor_name> optional arguments: -h, --help show this help message and exit -train_json TRAIN_JSON_FILE, --train_json_file TRAIN_JSON_FILE Json file containing train input dataset. -test_json TEST_JSON_FILE, --test_json_file TEST_JSON_FILE Json file containing test input dataset. -ld1 SOURCE_SENSOR, --lidar_sensor_1 SOURCE_SENSOR Source transformation sensor. -ld2 TARGET_SENSOR, --lidar_sensor_2 TARGET_SENSOR Target transformation sensor.","title":"LiDAR to LiDAR evaluation"},{"location":"evaluations/#lidar-to-rgb-camera-evaluation","text":"Evaluates the LiDAR-to-Camera calibration through the reprojection of the pattern limit 3D points into the image using the following metrics: X and Y errors Root mean squared error This process requires the annotation of the pattern limit points in the image. After annotating once, if the user wish to repeat the process, the saved json file with the annotations can be loaded. For that the -ua flag has to be disabled. usage: range_sensor_to_camera_evaluation.py [-h] -train_json TRAIN_JSON_FILE -test_json TEST_JSON_FILE -ss SOURCE_SENSOR -ts TARGET_SENSOR [-si] -ef EVAL_FILE [-ua] optional arguments: -h, --help show this help message and exit -train_json TRAIN_JSON_FILE, --train_json_file TRAIN_JSON_FILE Json file containing input training dataset. -test_json TEST_JSON_FILE, --test_json_file TEST_JSON_FILE Json file containing input testing dataset. -ss SOURCE_SENSOR, --source_sensor SOURCE_SENSOR Source transformation sensor. -ts TARGET_SENSOR, --target_sensor TARGET_SENSOR Target transformation sensor. -si, --show_images If true the script shows images. -ef EVAL_FILE, --eval_file EVAL_FILE Path to file to read and/or write the evaluation data. -ua, --use_annotation If true, the limit points will be manually annotated.","title":"LiDAR to RGB camera evaluation"},{"location":"evaluations/#point-cloud-image-projection","text":"atom_evaluation also allows the user to visualize the point cloud projected into an image to check the calibration. usage: point_cloud_to_image.py [-h] -json JSON_FILE -ls LIDAR_SENSOR -cs CAMERA_SENSOR optional arguments: -h, --help show this help message and exit -json JSON_FILE, --json_file JSON_FILE Json file containing input dataset. -ls LIDAR_SENSOR, --lidar_sensor LIDAR_SENSOR LiDAR sensor name. -cs CAMERA_SENSOR, --camera_sensor CAMERA_SENSOR Camera sensor name. How to run: rosrun atom_evaluation point_cloud_to_image.py -json <path_to_test_json> -ls <lidar_sensor_name> -cs <camera_sensor_name>","title":"Point cloud image projection"},{"location":"evaluations/#ground-truth-frame-evaluation","text":"When using simulation it is possible to know that a given dataset contains \"perfect\" value for the transformations to be estimated. Using this, we can run a comparison between the ground_truth dataset and the calibrated dataset, and assess if the estimated transformations are accurate. To run use: rosrun atom_evaluation ground_truth_frame_evaluation -train_json -test_json The script produces a table where you can inspect the errors. It is also possible to configure which frames are analyzed. Use --help to see the options.","title":"Ground Truth Frame Evaluation"},{"location":"examples/","text":"Calibration Examples MMTBot Atlascar2 IrisUA - ur10e AgrobV2 LARCC ATOM provides extensive visualization possibilities while running the calibration optimization procedure. To visualize in ROS Rviz use the -rv flag. So far, we have used ATOM to successfully calibrate several robotic platforms. Here are some examples: MMTBot MMTBot is a simulated robotic system containing a manipulator, two rgb cameras and one 3D lidar, with the goal of reserching how ATOM can calibration hand-eye systems. The respositories for this system are here: https://github.com/miguelriemoliveira/mmtbot A 3D Model of the MMTBot. Further details on this system can be read in the paper published in ESWA 2022 . Atlascar2 The Atlascar2 is an intelligent vehicle containing several cameras and 2D Lidars. This was the first platform we have calibrated using ATOM. The repositories containing the atlascar packages are here: https://github.com/lardemua/atlascar2 A photograph of the AtlasCar2. A 3D model of the AtlasCar2. Further details on this system can be read in the papers published in ROBOT 2019 and RAS 2020 . IrisUA - ur10e The IrisUA - ur10e includes several variants of the hand-eye calibration problem. The repositories containing the calibration ros package for this system are here: https://github.com/iris-ua/iris_ur10e_calibration A photograph of the IRIS UA UR10e. A 3D Model of the IRIS UA UR10e. Further details on this system can be read in the paper published in T-Ro 2021 . AgrobV2 AgrobV2 is a mobile robot with a stereo camera and a 3D Lidar designed for agriculture robotics. https://github.com/aaguiar96/agrob A photograph of the AgrobV2. A 3D Model of the AgrobV2. Further details on this system can be read in the paper published in ESWA 2021 . LARCC L aboratory of A utomation and R obotics C ollaborative C ell (LARCC) is included in a research project focusing of collaborative robotic industrial cells. The goal is to monitor in detail the volume of the cell in order to ensure safe collaboration between human operators and robots. For this, several sensors of different modalities are positioned everywhere in the cell, which makes the calibration of this robotic system a challenging task. The repostiories of LARCC are not publicly available. A photograph of LARCC. A 3D Model of the LARCC.. Sensor fields of view in LARCC. Calibration of LARCC.","title":"Calibration Examples"},{"location":"examples/#calibration-examples","text":"MMTBot Atlascar2 IrisUA - ur10e AgrobV2 LARCC ATOM provides extensive visualization possibilities while running the calibration optimization procedure. To visualize in ROS Rviz use the -rv flag. So far, we have used ATOM to successfully calibrate several robotic platforms. Here are some examples:","title":"Calibration Examples"},{"location":"examples/#mmtbot","text":"MMTBot is a simulated robotic system containing a manipulator, two rgb cameras and one 3D lidar, with the goal of reserching how ATOM can calibration hand-eye systems. The respositories for this system are here: https://github.com/miguelriemoliveira/mmtbot A 3D Model of the MMTBot. Further details on this system can be read in the paper published in ESWA 2022 .","title":"MMTBot"},{"location":"examples/#atlascar2","text":"The Atlascar2 is an intelligent vehicle containing several cameras and 2D Lidars. This was the first platform we have calibrated using ATOM. The repositories containing the atlascar packages are here: https://github.com/lardemua/atlascar2 A photograph of the AtlasCar2. A 3D model of the AtlasCar2. Further details on this system can be read in the papers published in ROBOT 2019 and RAS 2020 .","title":"Atlascar2"},{"location":"examples/#irisua-ur10e","text":"The IrisUA - ur10e includes several variants of the hand-eye calibration problem. The repositories containing the calibration ros package for this system are here: https://github.com/iris-ua/iris_ur10e_calibration A photograph of the IRIS UA UR10e. A 3D Model of the IRIS UA UR10e. Further details on this system can be read in the paper published in T-Ro 2021 .","title":"IrisUA - ur10e"},{"location":"examples/#agrobv2","text":"AgrobV2 is a mobile robot with a stereo camera and a 3D Lidar designed for agriculture robotics. https://github.com/aaguiar96/agrob A photograph of the AgrobV2. A 3D Model of the AgrobV2. Further details on this system can be read in the paper published in ESWA 2021 .","title":"AgrobV2"},{"location":"examples/#larcc","text":"L aboratory of A utomation and R obotics C ollaborative C ell (LARCC) is included in a research project focusing of collaborative robotic industrial cells. The goal is to monitor in detail the volume of the cell in order to ensure safe collaboration between human operators and robots. For this, several sensors of different modalities are positioned everywhere in the cell, which makes the calibration of this robotic system a challenging task. The repostiories of LARCC are not publicly available. A photograph of LARCC. A 3D Model of the LARCC.. Sensor fields of view in LARCC. Calibration of LARCC.","title":"LARCC"},{"location":"getting_started/","text":"Getting started Installation Clone ATOM repository and install dependencies Clone RViz fork Set environment variables Calibration patterns Basic usage Create a calibration package Configure a calibration package Set initial estimate Collect data Dataset playback Calibrate sensors Installation Clone ATOM repository and install dependencies Clone the atom repository to a directory somewhere inside your catkin workspace: git clone https://github.com/lardemua/atom then install requirements: sudo pip3 install -r requirements.txt Also, you need to install the following packages using apt-get sudo apt-get install ros-noetic-ros-numpy Clone RViz fork Some of the functionalities in ATOM, i.e., depth semi-automatic labeling and depth manual labeling, make use of a special RViz fork that allows for mouse clicking functionality. If you are interested in using one of these functionalities you have to clone it to your catkin workspace: git clone https://github.com/miguelriemoliveira/rviz To be integrated into RVIZ The functionality extends the image display to suport mouse clicking. We are working on integrating this in the RViz main branch, but this is not available yet. More information here: https://github.com/ros-visualization/rviz/issues/916 https://github.com/ros-visualization/rviz/pull/1737 Set environment variables We often use two environment variables to allow for easy cross machine access to bagfiles and datasets. If you want to use these you can also add these lines to your .bashrc or .zhsrc , adjusting the paths according to your case: export ROS_BAGS=\"$HOME/bagfiles\" export ATOM_DATASETS=\"$HOME/datasets\" and then you can refer to these environment variables when providing paths to atom scripts, e.g.: roslaunch <my_robot_calibration> calibrate.launch dataset_file:=$ATOM_DATASETS/<my_dataset>/dataset.json and you can also refer to them inside the calibration configuration file Calibration patterns ATOM can be used with chessboard or charuco calibration patterns. To calibrate our systems we purchased charuco calibration patterns from [calib.io](https://calib.io/products/charuco-targets?variant=9400455004207. An example of a Charuco calibration pattern. Note Charuco boards are preferable to chessboard patterns, because of two main reasons: the first is that the charuco detection is more efficient when compared to the chessboard detection; the second is that the charuco pattern is detected even if it is only partially visible in the image, which is very usefull when the sensors in your system have small overlapping fields of view. Basic usage Unlike most other calibration approaches, ATOM offers tools to address the complete calibration pipeline. These are instructions for quick starting your robotic system calibration. If you need more details read through the Calibration Procedures . Create a calibration package rosrun atom_calibration create_calibration_pkg --name <my_robot_calibration> Configure a calibration package Edit the file: <my_robot_calibration>/calibration/config.yml_ with your system information. and then run: rosrun <my_robot_calibration> configure Set initial estimate ATOM provides interactive tools based on rviz that allow the user to set the pose of the sensors to be calibrated, while receiving visual feedback. Optional If you consider that your initial sensor poses are already accurate, you may skip this procedure. To use launch: roslaunch <my_robot_calibration> set_initial_estimate.launch Collect data Collecting data produces an ATOM dataset, which is then used for calibrating the system. roslaunch <my_robot_calibration> collect_data.launch output_folder:=~/datasets/<my_dataset> Dataset playback Dataset playback offers the possibility to visualize and correct the labels automatically produced during the collection stage. Optional If you trust that the automatic labels are correct, you may skip this procedure. First launch the visualizer: roslaunch <my_robot_calibration> dataset_playback.launch and then: rosrun atom_calibration dataset_playback -json $ATOM_DATASETS/<my_robot_calibration>/<your_dataset>/dataset.json -uic -si -ow Calibrate sensors Finally, run an optimization that will calibrate your sensors. First launch: roslaunch <my_robot_calibration> calibrate.launch Then, in a second terminal, run the calibrate script: rosrun atom_calibration calibrate -json $ATOM_DATASETS/<my_robot_dataset>/dataset.json -v -rv -si There are many different options to customize the calibration. Check the detailed description to know more.","title":"Getting Started"},{"location":"getting_started/#getting-started","text":"Installation Clone ATOM repository and install dependencies Clone RViz fork Set environment variables Calibration patterns Basic usage Create a calibration package Configure a calibration package Set initial estimate Collect data Dataset playback Calibrate sensors","title":"Getting started"},{"location":"getting_started/#installation","text":"","title":"Installation"},{"location":"getting_started/#clone-atom-repository-and-install-dependencies","text":"Clone the atom repository to a directory somewhere inside your catkin workspace: git clone https://github.com/lardemua/atom then install requirements: sudo pip3 install -r requirements.txt Also, you need to install the following packages using apt-get sudo apt-get install ros-noetic-ros-numpy","title":"Clone ATOM repository and install dependencies"},{"location":"getting_started/#clone-rviz-fork","text":"Some of the functionalities in ATOM, i.e., depth semi-automatic labeling and depth manual labeling, make use of a special RViz fork that allows for mouse clicking functionality. If you are interested in using one of these functionalities you have to clone it to your catkin workspace: git clone https://github.com/miguelriemoliveira/rviz To be integrated into RVIZ The functionality extends the image display to suport mouse clicking. We are working on integrating this in the RViz main branch, but this is not available yet. More information here: https://github.com/ros-visualization/rviz/issues/916 https://github.com/ros-visualization/rviz/pull/1737","title":"Clone RViz fork"},{"location":"getting_started/#set-environment-variables","text":"We often use two environment variables to allow for easy cross machine access to bagfiles and datasets. If you want to use these you can also add these lines to your .bashrc or .zhsrc , adjusting the paths according to your case: export ROS_BAGS=\"$HOME/bagfiles\" export ATOM_DATASETS=\"$HOME/datasets\" and then you can refer to these environment variables when providing paths to atom scripts, e.g.: roslaunch <my_robot_calibration> calibrate.launch dataset_file:=$ATOM_DATASETS/<my_dataset>/dataset.json and you can also refer to them inside the calibration configuration file","title":"Set environment variables"},{"location":"getting_started/#calibration-patterns","text":"ATOM can be used with chessboard or charuco calibration patterns. To calibrate our systems we purchased charuco calibration patterns from [calib.io](https://calib.io/products/charuco-targets?variant=9400455004207. An example of a Charuco calibration pattern. Note Charuco boards are preferable to chessboard patterns, because of two main reasons: the first is that the charuco detection is more efficient when compared to the chessboard detection; the second is that the charuco pattern is detected even if it is only partially visible in the image, which is very usefull when the sensors in your system have small overlapping fields of view.","title":"Calibration patterns"},{"location":"getting_started/#basic-usage","text":"Unlike most other calibration approaches, ATOM offers tools to address the complete calibration pipeline. These are instructions for quick starting your robotic system calibration. If you need more details read through the Calibration Procedures .","title":"Basic usage"},{"location":"getting_started/#create-a-calibration-package","text":"rosrun atom_calibration create_calibration_pkg --name <my_robot_calibration>","title":"Create a calibration package"},{"location":"getting_started/#configure-a-calibration-package","text":"Edit the file: <my_robot_calibration>/calibration/config.yml_ with your system information. and then run: rosrun <my_robot_calibration> configure","title":"Configure a calibration package"},{"location":"getting_started/#set-initial-estimate","text":"ATOM provides interactive tools based on rviz that allow the user to set the pose of the sensors to be calibrated, while receiving visual feedback. Optional If you consider that your initial sensor poses are already accurate, you may skip this procedure. To use launch: roslaunch <my_robot_calibration> set_initial_estimate.launch","title":"Set initial estimate"},{"location":"getting_started/#collect-data","text":"Collecting data produces an ATOM dataset, which is then used for calibrating the system. roslaunch <my_robot_calibration> collect_data.launch output_folder:=~/datasets/<my_dataset>","title":"Collect data"},{"location":"getting_started/#dataset-playback","text":"Dataset playback offers the possibility to visualize and correct the labels automatically produced during the collection stage. Optional If you trust that the automatic labels are correct, you may skip this procedure. First launch the visualizer: roslaunch <my_robot_calibration> dataset_playback.launch and then: rosrun atom_calibration dataset_playback -json $ATOM_DATASETS/<my_robot_calibration>/<your_dataset>/dataset.json -uic -si -ow","title":"Dataset playback"},{"location":"getting_started/#calibrate-sensors","text":"Finally, run an optimization that will calibrate your sensors. First launch: roslaunch <my_robot_calibration> calibrate.launch Then, in a second terminal, run the calibrate script: rosrun atom_calibration calibrate -json $ATOM_DATASETS/<my_robot_dataset>/dataset.json -v -rv -si There are many different options to customize the calibration. Check the detailed description to know more.","title":"Calibrate sensors"},{"location":"multimedia/","text":"Multimedia Take a look at the ATOM youtube playlist . Tutorials dataset playback Navigating collections Correcting 3D Lidar labels Correcting depth labels","title":"Multimedia"},{"location":"multimedia/#multimedia","text":"Take a look at the ATOM youtube playlist .","title":"Multimedia"},{"location":"multimedia/#tutorials-dataset-playback","text":"","title":"Tutorials dataset playback"},{"location":"multimedia/#navigating-collections","text":"","title":"Navigating collections"},{"location":"multimedia/#correcting-3d-lidar-labels","text":"","title":"Correcting 3D Lidar labels"},{"location":"multimedia/#correcting-depth-labels","text":"","title":"Correcting depth labels"},{"location":"procedures/","text":"Calibration procedures Calibration procedures Create a calibration package Configure a calibration package Recording compressed images Throttling topics Using a different configuration file Using tfs instead of the xacro file Set an initial estimate Visualizing sensor frustums Collect data RGB camera labeling 3D Lidar labeling Depth camera labeling 2D Lidar labeling Dataset playback Correcting 3D Lidar labels Correcting Depth labels Calibrate Calibrating intrinsic parameters Two stage calibration for robotic systems with an anchored sensor To calibrate your robot you must define your robotic system, (e.g. <my_robot>). You should also have a system description in the form of an urdf or a xacro file(s). This is normally stored in a ros package named <my_robot>_description . In addition to this, ATOM requires a bagfile with a recording of the data from the sensors you wish to calibrate. This was covered in detail in here . Transformations in the bagfile (i.e. topics /tf and /tf_static) will be ignored, so that they do not collide with the ones being published by the robot_state_publisher . Thus, if your robotic system contains moving parts, the bagfile should also record the sensor_msgs/JointState message. Note It is also possible to use the transformations in the bagfile instead of using the xacro description and the robot state publisher to produce them. See section on using tfs instead of a xacro file . To reduce the bag size, it may contain compressed images instead of raw images, since ATOM can decompress them while playing back the bagfile. Here is an example of a launch file which records compressed images. We consider this to be part of the normal configuration of your robotic system in ROS, so ATOM assumes this is already done. In any case if you need inspiration you can take a look at the calibration examples and how we configured our systems. Create a calibration package Assuming you have your robotic system setup, you can start creating the calibration package. You should create a calibration ros package specific for your robotic system. ATOM provides a script for this: rosrun atom_calibration create_calibration_pkg --name <my_robot_calibration> This will create the ros package in the current folder, but you can also specify another folder, e.g.: rosrun atom_calibration create_calibration_pkg --name ~/my/path/<my_robot_calibration> Configure a calibration package Once your calibration package is created you will have to configure the calibration procedure by editing the /calibration/config.yml file with your system information. The file contains several comments to provide clues on how to configure it. Here are examples of calibration config.yml files for an autonomous vehicle and for MMTBot , also shown below: # # \u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2557 # \u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u255a\u2550\u2550\u2588\u2588\u2554\u2550\u2550\u255d\u2588\u2588\u2554\u2550\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2551 # \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2551 \u2588\u2588\u2551 \u2588\u2588\u2551 \u2588\u2588\u2551\u2588\u2588\u2554\u2588\u2588\u2588\u2588\u2554\u2588\u2588\u2551 # \u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2551 \u2588\u2588\u2551 \u2588\u2588\u2551 \u2588\u2588\u2551\u2588\u2588\u2551\u255a\u2588\u2588\u2554\u255d\u2588\u2588\u2551 # __ \u2588\u2588\u2551 \u2588\u2588\u2551 \u2588\u2588\u2551 \u255a\u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u2588\u2588\u2551 \u255a\u2550\u255d \u2588\u2588\u2551 _ # / _| \u255a\u2550\u255d \u255a\u2550\u255d \u255a\u2550\u255d \u255a\u2550\u2550\u2550\u2550\u2550\u255d \u255a\u2550\u255d \u255a\u2550\u255d | | # | |_ _ __ __ _ _ __ ___ _____ _____ _ __| | __ # | _| '__/ _` | '_ ` _ \\ / _ \\ \\ /\\ / / _ \\| '__| |/ / # | | | | | (_| | | | | | | __/\\ V V / (_) | | | < # |_| |_| \\__,_|_| |_| |_|\\___| \\_/\\_/ \\___/|_| |_|\\_\\ # https://github.com/lardemua/atom # This yaml file describes your calibration! # You can start by defining your robotic system. # This is the URDF file (or xacro) that describes your robot. # Every time a path to a file is requested you can use # # - Absolute Path # Example 1: /home/user/ros_workspace/your_package/urdf/description.urdf.xacro # Example 2: file://home/user/ros_workspace/your_package/urdf/description.urdf.xacro # # - Path Expansion # Example 1: ${HOME}/user/${YOUR_VARIABLE}/your_package/urdf/description.urdf.xacro # Example 2: ~/user/ros_workspace/your_package/urdf/description.urdf.xacro # # NOTE: It is up to you to guarantee the environment variable exists. # # - ROS Package Reference # Example: package://your_package/urdf/description.urdf.xacro # description_file: \"package://mmtbot_gazebo/urdf/mmtbot.urdf.xacro\" #description_file: \"package://mmtbot_gazebo/urdf/mmtbot.urdf.xacro\" # The calibration framework requires a bagfile to extract the necessary data for the calibration. bag_file: \"$ROS_BAGS/mmtbot/test_depth2.bag\" # You must define a frame of reference for the optimization process. # It must exist in the transformation chains of all the sensors which are being calibrated. world_link: \"world\" # ATOM will calibrate the extrinsic parameters of your sensors. # In this section you should discriminate the sensors that will be part of the calibrations. sensors: # Each key will define a sensor and its name, which will be use throughout the calibration. # Each sensor definition must have the following properties: # link: # The frame of the sensor's data (i.e. the header.frame_id). # parent_link: # The parent link of the transformation (i.e. link) to be calibrated. # child_link: # This is the transformation (i.e. link) that we be optimized. # topic_name: # Name of the ROS topic that contains the data produced by this sensor. # If you are calibrating an camera, you should use the raw image produced by the # sensors. Additionally, it the topic is an image it will automatically use the # respective `camera_info` topic. hand_camera: link: \"hand_camera_rgb_optical_frame\" parent_link: \"flange\" child_link: \"hand_camera_link\" topic_name: \"/hand_camera/rgb/image_raw\" throttle: 5 modality: \"rgb\" world_camera_rgb: link: \"world_camera_rgb_optical_frame\" parent_link: \"world_camera_link\" child_link: \"world_camera_rgb_frame\" topic_name: \"/world_camera/rgb/image_raw\" modality: \"rgb\" lidar: link: \"lidar\" parent_link: \"tripod_left_support\" child_link: \"lidar_base_link\" topic_name: \"/lidar/points\" modality: \"lidar3d\" # The calibration requires a detectable pattern. # This section describes the properties of the calibration pattern used in th calibration. calibration_pattern: # The frame id (or link) of the pattern. # This link/transformation will be optimized. link: \"pattern_link\" # The parent frame id (or link) of the pattern. # For example, in hand-eye calibration the parent link # of the pattern can be the end-effector or the base of the arm parent_link: \"world\" # Defines if the pattern link is the same in all collections (i.e. fixed=true), # or each collection will have its own estimate of the link transformation. # Note: if you plan to have the pattern fixed, while the moving the rigidly attached sensors, # this is equivalent to having the sensors fixed and the pattern moving, so you should use fixed=false. fixed: false # The type of pattern used for the calibration. # Supported pattern are: chessboard, charuco pattern_type: \"charuco\" # If the pattern type is \"charuco\" you need to define # the aruco dictionary used by the pattern. # See https://docs.opencv.org/trunk/dc/df7/dictionary_8hpp.html dictionary: \"DICT_5X5_100\" # Mesh file (collada.dae or stl) for showing pattern on rviz. URI or regular path. mesh_file: \"package://mmtbot_gazebo/models/charuco_800x600/charuco_800x600.dae\" # The border width from the edge corner to the pattern physical edge. # Used for 3D sensors and lidars. # It can be a scalar (same border in x and y directions), or it can be {'x': ..., 'y': ,,,} border_size: { \"x\": 0.04, \"y\": 0.03 } # The number of corners the pattern has in the X and Y dimensions. # Note: The charuco detector uses the number of squares per dimension in its detector. # Internally we add a +1 to Y and X dimensions to account for that. # Therefore, the number of corners should be used even for the charuco pattern. dimension: { \"x\": 11, \"y\": 8 } # The length of the square edge. size: 0.06 # The length of the charuco inner marker. inner_size: 0.045 # Miscellaneous configuration # If your calibration problem is not fully constrained you should anchored one of the sensors. # This makes it immovable during the optimization. # This is typically referred to as gauge freedom. anchored_sensor: \"world_camera_rgb\" # Max time delta (in milliseconds) between sensor data messages when creating a collection. max_duration_between_msgs: 1000 After filling the config.yml file, you should run the package configuration: rosrun <my_robot_calibration> configure This will go through a series of verifications to check if the configuration is valid, if the bagfile exists and contains the necessary transformations, among many others. Once the verifications signal a correct calibration configuration, a set of files is automatically created inside your <my_robot>_calibration ros package, as shown below: \u251c<my_robot>_calibration \u251c\u2500\u2500 launch \u2502 \u251c\u2500\u2500 calibrate.launch \u2502 \u251c\u2500\u2500 collect_data.launch \u2502 \u251c\u2500\u2500 dataset_playback.launch \u2502 \u251c\u2500\u2500 playbag.launch \u2502 \u2514\u2500\u2500 set_initial_estimate.launch \u251c\u2500\u2500 rviz \u2502 \u251c\u2500\u2500 calibrate.rviz \u2502 \u251c\u2500\u2500 collect_data.rviz \u2502 \u251c\u2500\u2500 dataset_playback.rviz \u2502 \u2514\u2500\u2500 set_initial_estimate.rviz \u251c\u2500\u2500 scripts \u2502 \u2514\u2500\u2500 configure \u2514\u2500\u2500 urdf \u251c\u2500\u2500 initial_estimate.urdf.xacro \u251c\u2500\u2500 calibration \u2502 \u251c\u2500\u2500 config.yml \u2502 \u2514\u2500\u2500 summary.pdf The launch directory contains automatically created launch files used to launch each of the calibration stages. The rviz folder contains several rviz configuration files used to setup the visualization of each stage. The urdf folder contains a symbolic link to the xacro file of <my_robot> and, after calibration, will contain a calibrated urdf. Finally, the calibration folder contains the configuration file (config.yml). This folder also contains an automatically produced schematic of the configuration of the calibration named summary.pdf . It is used to inspect the configuration of the calibration and assess if the configuration is doing what we intended. Atomic transformations to be estimated are marked in blue, sensor coordinate systems, i.e. the coordinate systems in which sensors produce their data are marked in green, and the selected world coordinate frame is highlighted in red. Summary of a calibration configuration (MMTBot). Recording compressed images In case the topics from RGB camera(s) are published using the ROS image transport , it is possible to record the compressed image topic instead of the raw image topic as in this example . During the configuration of your calibration package, ATOM will detect that these are compressed topics and automatically generate a decompressing mechanism in the launch files. The advantage is that you get much smaller bag files since they contain compressed images. This may be critical for large systems such as LARCC , where the data output may reach over 1GB per minute. Throttling topics For an offline calibration procedure, speed is not critical. Thus, it is possible to configure a ROS topic throttler for your sensor messages, if they have been recorded in a higher than necessary frequency. For that just add a field throttle to the section of the corresponding sensor in your calibration config.yaml file. Using a different configuration file It is also possible to configure your calibration package with a different configuration file, in the case you have multiple configurations with multiple config.yml files. There are also other options to run a custom configuration, i.e.: usage: rosrun <my_robot_calibration> configure -cfg CONFIG_FILE Using tfs instead of the xacro file Sometimes it may be preferable to use the transformations in the bagfile instead of the ones produced by the xacro description. To do this use the --use_tfs option when configuring your package: usage: rosrun <my_robot_calibration> configure -utf Note A previous version of the configure script used a bash based script (currently it is python based), which may not have all the options listed. If the package was created some time ago, you may still have the bash based script. In that case you can update the configure script by running: rosrun atom_calibration update_configure_script -n <my_robot_calibration> Set an initial estimate Iterative optimization methods such as ATOM are sensitive to the initial parameter configuration. ATOM uses several groups of optimization parameters and uses distinct strategies to initialize each class. The most important group, sensor poses, may be manually initialized to ensure that the initial estimate is plausible and thus reduce the likelihood of encountering a local minima. ATOM provides an interactive framework based on rviz which allows the user to set the pose of the sensors while having immediate visual feedback. The system configures an RViz interactive marker for each sensor to be calibrated, so that it is possible to move the sensor in RViz. During this procedure one may use several visual clues to better position the sensors, i.e. it is possible to see the CAD model of the sensors and the rest of the structure of the robotic system or one could observe the amount of overlap between sensors. To set an initial estimate run: roslaunch <my_robot_calibration> set_initial_estimate.launch Here are a couple of examples of setting the initial estimate: Setting initial estimate of sensor poses in the AgrobV2. Setting initial estimate of sensor poses in the AtlasCar2. Setting initial estimate of sensor poses in the IRIS UR10e. Visualizing sensor frustums ATOM provides a way to visualize the frustums of RGB and Depth cameras. These may be useful to get a clue about the overlap between sensors, or the amount of coverage of a work volume. Below you can see the frustum of two rgb cameras. One of the cameras is positioned on the end-effector of the manipulator, and when it moves, so does its frustum. This functionality is only available in the set initial estimate stage. Visualizing frustum of RGB sensors in the MMTBot. Collect data To run a system calibration, one requires data from the sensors collected at different time instants. We refer to these snapshots of data as collections , and a set of collections as an ATOM dataset . To collect data, use: roslaunch <my_robot_calibration> collect_data.launch output_folder:=$ATOM_DATASETS/<your_dataset_folder> The script launches a fully configured rviz window. The user observes the data playback and decides when a collection should be saved by clicking a green sphere that appears in the scene. How many collections do I need? The number of collections required to accurately calibrate a system varies according to the number, modality and positioning of the sensors. Empirically we found that a figure around 20 or 30 collections is usually sufficient for estimating an accurate calibration. That's about the same number of images you need when calibrating a stereo system with OpenCV. Of course this highly depends on your system and the amount of overlap there is between the sensors. What is a good bagfile for recording an ATOM dataset? Each collection in an ATOM dataset will contain data from all the sensors in the system. All this data is assumed to be captured at the reference time stamp of that collection. In other words, ATOM assumes that the data from the sensors in a collection is synchronized. There are some robotic systems where it is straightforward to implement synchronization mechanisms, e.g., image acquisition hardware triggers in stereo camera systems. However, these solutions are very specialized and cannot be relied upon for the general case of multi-sensor, multi-modal systems. For example, it is not so easy to synchronize the acquisition of images from a camera with range data from a LiDAR. The solution adopted in ATOM is to transfer the responsibility of selection of these moments when a collection should be recorded to the user. More often than not, sensor data is not synchronized. However, if the sensors are observing a static scene, it is possible to safely assume that the sensor data is \"virtually synchronized\" because, even if the data is collected at different instants in time, it will observe the same static scenario. To ensure this, what we do during the recording of the bagfile for calibration is to plan in advance the moments where we will record collections, and ensure that at those moments the scene has been static for a duration longer than the estimated largest possible sensor data time differential. In practice, in our bagfiles, we move the pattern from position A to position B (moving the pattern at normal speed), but then hold the pattern static in position B for some time, e.g 3 to 5 seconds, in anticipation that the collection will be recorded once these seconds elapse, a time in which is is safe to assume that the scene has remained static for a long enough duration. Here is an example of a calibration bagfile where the described motion of the calibration pattern is visible. Having the user holding the pattern static will always results is small displacements of the calibration pattern. Thus, for more complex systems, or when a high accuracy is required, we use a tripod to hold the pattern still . Should the pattern be held in a special pose? Short answer: Yes , if LiDAR or depth sensors are used the pattern should be held somewhat diagonally. Long answer: In order to calibrate LiDAR or depth sensor modalities ATOM uses a cost function which uses the lateral physical boundaries of the pattern as landmarks for guiding the calibration. If the pattern is held in a straight up vertical pose, the lateral alignment will be equally valued regardless of the height at which the pattern is held. This results in redundancy in the estimation of the pattern pose, which pollutes the calibration procedure and often ends up in non-accurate calibrations. ATOM tackles this with a simple solution: we avoid holding the calibration pattern in a straight up pose , and hold it in one (or more) diagonal pose(s) when recording calibration bagfiles. It is also possible to add additional parameters to configure several aspects of the script. See below all the options. Additional parameters for collect_data.launch Argument Function overwrite overwrites previous dataset without asking for confirmation bag_rate Defines the playback rate of the bagfile bag_start Start time for playback bag_file Name of bagfile to playback ssl A string to be evaluated that indicates if a sensor should be labelled. One example using all the parameters above: roslaunch <my_robot_calibration> collect_data.launch output_folder:=$ATOM_DATASETS/<your_dataset_folder> overwrite:=true bag_rate:=0.5 bag_start:=10 ssl:='lambda name: name in [\"s1\", \"s2\"]' When you launch the data collection script, it automatically starts data labeling processes adequate for each sensor in your robotic system. As such, the data is being continuously labeled as the bagfile is played. Depending on the modality of the sensors in the system the labeling may be automatic or fully automatic. Below we detail how each of the labelers operate. RGB camera labeling RGB cameras have a fully automatic pattern detection. ATOM uses off the shelf chessboard or charuco calibration pattern detectors. Also, it provides an rviz configuration which subscribes annotated images received from the pattern detectors. You can check if the detection is working by observing the overlays of top of the images. Automatic labeling of RGB images (IrisUA - UR10e). Use charuco boards Charuco boards are preferable to chessboard patterns, because of two main reasons: the first is that the charuco detection is more more efficient when compared to the chessboard detection; the second is that the charuco pattern is detected even if it is only partially visible in the image, which is very useful when the sensors in your system have small overlapping fields of view. 3D Lidar labeling 3D Lidar labeling is a semi-automatic procedure. The idea is that the user moves an rviz marker close to where the pattern is present in the lidar point cloud. Setting the seed point in 3D Lidar data for semi-automatic labeling (MMTBot). After setting this seed position, the system continues to track the patterns pose over the next frames, even if it moves, as you can see below: Automatic 3D Lidar labeling and creating an ATOM dataset (AgrobV2). Tracking limitations The tracking procedure may fail if the pattern is too close to another object, as for example the ground plane. This can be solved by making sure the pattern is sufficiently far from all other objects, or during the dataset playback stage. Depth camera labeling The labeling of depth cameras is a semi-automatic procedure. It is done by clicking the depth image in rviz. The user should click somewhere inside the pattern, and then the system carries on the tracking of the pattern even if it moves. The user may reset the procedure by clicking again on the image. Labeling of depth data (LARCC). RViz fork required Learn about this here . 2D Lidar labeling The labeling of the 2D Lidars is very similar to the labeling of 3D Lidars. The user sets the seed point where the lidar points are observing the pattern, and then the pattern is tracked. Setting the seed point in 2D Lidar data for semi-automatic labeling (AtlasCar2). May be deprecated The 2D Lidar semi-automatic labeling was last used in 2019, so it may be deprecated. If you are interested on having this functionality create an issue with a request. Dataset playback The dataset playback is used to review and eventually correct the automatic labels produced during the collection of data . Ideally, the bulk of the labels produced automatically during the collect data stage should be correct, but a few incorrect annotations will disrupt the calibration. As such, a review of the annotations is recommended. You may skip it if you feel that the automatic labeling went very well. To run the dataset playback, first launch the visualization: roslaunch <my_robot_calibration> dataset_playback.launch and then the dataset_playback node: clear && rosrun atom_calibration dataset_playback -json <my_dataset_file>.json -ow This will create a new json file called .json. There are other available options listed below: usage: dataset_playback [-h] -json JSON_FILE [-csf COLLECTION_SELECTION_FUNCTION] [-ow] optional arguments: -h, --help show this help message and exit -json JSON_FILE, --json_file JSON_FILE Json file containing input dataset. -csf COLLECTION_SELECTION_FUNCTION, --collection_selection_function COLLECTION_SELECTION_FUNCTION A string to be evaluated into a lambda function that receives a collection name as input and returns True or False to indicate if the collection should be loaded (and used in the optimization). The Syntax is lambda name: f(x), where f(x) is the function in python language. Example: lambda name: int(name) > 5 , to load only collections 6, 7, and onward. -ow, --overwrite Overwrites the data_corrected.json without asking for permission Check the video tutorial . Navigating through collections in a dataset (LARCC). Correcting 3D Lidar labels Correcting 3D Lidar labels is done by selecting points in the point cloud displayed by RViz and pressing keys in order to add these points as pattern points or boundary points. Correcting 3D Lidar labels in a dataset (LARCC). Check the video tutorial . Do not forget to compile your catkin workspace For selecting points from point clouds, we use an rviz plugin in cpp that must be compiled. Correcting Depth labels To correct depth modality labels the user draws a polygon around the pattern in the depth image. Correcting depth labels in a dataset (LARCC). Check the video tutorial . RViz fork required Learn about this here . Calibrate Finally, a system calibration is called through: roslaunch <my_robot_calibration> calibrate.launch Then, in a second terminal, run the calibrate script: rosrun atom_calibration calibrate -json $ATOM_DATASETS/<my_robot_dataset>/dataset_corrected.json -v -rv -si There are several options to use in the calibrate script, one common usage is: rosrun atom_calibration calibrate -json $ATOM_DATASETS/<my_robot_dataset>/dataset_corrected.json -v -rv -si -uic -csf 'lambda x: int(x)< 5' -ssf 'lambda name: name in [\"camera_2\",\"camera_3\"]' which would run a calibration in verbose mode (-v), using ros visualization (-rv), showing images (-si), using incomplete collections, using collections with index smaller than 5, considering only sensors camera_2 and camera_3. You can see all the options listed below: usage: calibrate [-h] [-vo] -json JSON_FILE [-v] [-rv] [-si] [-oi] [-sr SAMPLE_RESIDUALS] [-ss SAMPLE_SEED] [-slr SAMPLE_LONGITUDINAL_RESIDUALS] [-ajf] [-oas] [-ap] [-uic] [-ias] [-rpd] [-nig translation rotation] [-ssf SENSOR_SELECTION_FUNCTION] [-csf COLLECTION_SELECTION_FUNCTION] [-phased] [-ipg] [-oj OUTPUT_JSON] optional arguments: -h, --help show this help message and exit -vo, --view_optimization ... -json JSON_FILE, --json_file JSON_FILE Json file containing input dataset. -v, --verbose Be verbose -rv, --ros_visualization Publish ros visualization markers. -si, --show_images shows images for each camera -oi, --optimize_intrinsics Adds camera intrinsics to the optimization -sr SAMPLE_RESIDUALS, --sample_residuals SAMPLE_RESIDUALS Samples residuals -ss SAMPLE_SEED, --sample_seed SAMPLE_SEED Sampling seed -slr SAMPLE_LONGITUDINAL_RESIDUALS, --sample_longitudinal_residuals SAMPLE_LONGITUDINAL_RESIDUALS Samples residuals -ajf, --all_joints_fixed Assume all joints are fixed and because of that draw a single robot mesh.Overrides automatic detection of static robot. -oas, --only_anchored_sensor Runs optimization only using the anchored sensor and discarding all others. -ap, --anchor_patterns Runs optimization without changing the poses of the patterns. -uic, --use_incomplete_collections Remove any collection which does not have a detection for all sensors. -ias, --ignore_anchored_sensor Ignore the anchored sensor information in the dataset. -rpd, --remove_partial_detections Remove detected labels which are only partial.Used or the Charuco. -nig translation rotation, --noisy_initial_guess translation rotation Percentage of noise to add to the initial guess atomic transformations set before. -ssf SENSOR_SELECTION_FUNCTION, --sensor_selection_function SENSOR_SELECTION_FUNCTION A string to be evaluated into a lambda function that receives a sensor name as input and returns True or False to indicate if the sensor should be loaded (and used in the optimization). The Syntax is lambda name: f(x), where f(x) is the function in python language. Example: lambda name: name in [\"left_laser\", \"frontal_camera\"] , to load only sensors left_laser and frontal_camera -csf COLLECTION_SELECTION_FUNCTION, --collection_selection_function COLLECTION_SELECTION_FUNCTION A string to be evaluated into a lambda function that receives a collection name as input and returns True or False to indicate if the collection should be loaded (and used in the optimization). The Syntax is lambda name: f(x), where f(x) is the function in python language. Example: lambda name: int(name) > 5 , to load only collections 6, 7, and onward. -phased, --phased_execution Stay in a loop before calling optimization, and in another after calling the optimization. Good for debugging. -ipg, --initial_pose_ghost Draw a ghost mesh with the systems initial pose. Good for debugging. -oj OUTPUT_JSON, --output_json OUTPUT_JSON Full path to output json file. If you use the --verbose option, the script will periodically print a table containing information about the errors per sensor and per collection, e.g.: Errors per collection (anchored sensor, max error per sensor, not detected as \"---\") +------------+----------+----------+----------+----------------+---------+---------+---------+ | Collection | camera_2 | camera_3 | camera_4 | depth_camera_1 | lidar_1 | lidar_2 | lidar_3 | +------------+----------+----------+----------+----------------+---------+---------+---------+ | 11 | 191.1838 | --- | 0.3000 | 0.1321 | 0.2606 | 0.1153 | 1.4850 | | 12 | 194.3039 | 98.7729 | 0.4031 | 0.1591 | 0.1043 | 0.0924 | 0.5050 | | 13 | --- | 94.6346 | 0.3795 | 0.1942 | 0.1657 | 0.1191 | 0.4210 | | 14 | 199.6314 | --- | 0.4001 | 0.1569 | 0.1897 | 0.0685 | 0.7067 | | 15 | 199.8989 | --- | 0.5995 | 0.1561 | 0.1956 | 0.0670 | 0.7074 | | 16 | --- | 146.3350 | 0.2370 | 0.1516 | 0.4412 | 0.1316 | 0.3175 | | 17 | --- | 150.6509 | 0.2204 | 0.1924 | 0.1747 | 0.0298 | 0.4698 | | 18 | 202.8877 | --- | 0.5371 | 0.0978 | 0.2200 | 0.1526 | 0.8007 | | 19 | 211.7066 | 133.3410 | 0.3598 | 0.2179 | 0.0909 | 0.0367 | 0.6065 | | 20 | 213.7645 | --- | 0.5272 | 0.2152 | 0.2647 | 0.2008 | 0.7670 | | 21 | 212.6482 | --- | 0.6358 | 0.2444 | 0.3512 | 0.0518 | 1.7215 | | 22 | 212.3743 | --- | 0.5967 | 0.2321 | 0.3342 | 0.0591 | 0.7347 | | 23 | 209.1305 | --- | 0.4488 | 0.1161 | 0.1601 | 0.0431 | 0.7198 | | 24 | 201.1378 | 139.1949 | 0.2539 | 0.1802 | 0.1114 | 0.0517 | 0.5524 | | 25 | --- | 139.2066 | 0.2994 | 0.0869 | 0.4260 | 0.1768 | 0.1162 | | 26 | 209.7005 | --- | 0.4192 | 0.1315 | 0.1400 | 0.0443 | 0.6932 | | 27 | 210.5517 | --- | 0.4042 | 0.2189 | 0.2926 | 0.0495 | 0.7068 | | 28 | --- | 142.4085 | 0.2628 | 0.1500 | 0.2662 | 0.1419 | 0.3405 | | Averages | 205.3015 | 130.5680 | 0.4047 | 0.1685 | 0.2327 | 0.0907 | 0.6873 | +------------+----------+----------+----------+----------------+---------+---------+---------+ Here's an example of a system being calibrated. Calibration of AgrobV2. Calibrating intrinsic parameters ATOM also supports intrinsic camera calibration (for now just RGB modality), but requires a first guess for these parameters. We compute a first guess using the ROS monocular camera calibration for rgb cameras, and the depth camera intrinsic calibration for depth cameras. After these calibrations are carried out, the sensor drivers publish calibrated camera_info messages which are saved to the bag files that is used for calibration, and then used a as first guess for ATOM's intrinsic parameter estimation. Intrinsic calibration runs simultaneous with the extrinsic calibration. Two stage calibration for robotic systems with an anchored sensor When one sensor is set to be anchored in the calibration/config.yml file, i.e. this file for the AtlaCar2, we recommend a two stage procedure to achieve a more accurate calibration: First, run a calibration using parameter --only_anchored_sensor ( -oas ) which will exclude from the optimization all sensors which are not the anchored one. This optimization will position the patterns correctly w.r.t. the anchored sensor. For example: rosrun atom_calibration calibrate -json $ATOM_DATASETS/larcc_real/ dataset_train/dataset_corrected.json -uic -nig 0.0 0.0 -ipg -si -rv -v -oas The output is stored in the atom_calibration.json , which is used and the input for the second stage, where all sensors are used. In this second stage the poses of the patterns are frozen using the parameter --anchor_patterns ( -ap ). To avoid overwriting atom_calibration.json, you should also define the output json file ( -oj ). For example: rosrun atom_calibration calibrate -json $ATOM_DATASETS/larcc_real/ dataset_train/atom_calibration.json -uic -nig 0.0 0.0 -ipg -si -rv -v -ap -oj atom_anchored_calibration.json","title":"Calibration Procedures"},{"location":"procedures/#calibration-procedures","text":"Calibration procedures Create a calibration package Configure a calibration package Recording compressed images Throttling topics Using a different configuration file Using tfs instead of the xacro file Set an initial estimate Visualizing sensor frustums Collect data RGB camera labeling 3D Lidar labeling Depth camera labeling 2D Lidar labeling Dataset playback Correcting 3D Lidar labels Correcting Depth labels Calibrate Calibrating intrinsic parameters Two stage calibration for robotic systems with an anchored sensor To calibrate your robot you must define your robotic system, (e.g. <my_robot>). You should also have a system description in the form of an urdf or a xacro file(s). This is normally stored in a ros package named <my_robot>_description . In addition to this, ATOM requires a bagfile with a recording of the data from the sensors you wish to calibrate. This was covered in detail in here . Transformations in the bagfile (i.e. topics /tf and /tf_static) will be ignored, so that they do not collide with the ones being published by the robot_state_publisher . Thus, if your robotic system contains moving parts, the bagfile should also record the sensor_msgs/JointState message. Note It is also possible to use the transformations in the bagfile instead of using the xacro description and the robot state publisher to produce them. See section on using tfs instead of a xacro file . To reduce the bag size, it may contain compressed images instead of raw images, since ATOM can decompress them while playing back the bagfile. Here is an example of a launch file which records compressed images. We consider this to be part of the normal configuration of your robotic system in ROS, so ATOM assumes this is already done. In any case if you need inspiration you can take a look at the calibration examples and how we configured our systems.","title":"Calibration procedures"},{"location":"procedures/#create-a-calibration-package","text":"Assuming you have your robotic system setup, you can start creating the calibration package. You should create a calibration ros package specific for your robotic system. ATOM provides a script for this: rosrun atom_calibration create_calibration_pkg --name <my_robot_calibration> This will create the ros package in the current folder, but you can also specify another folder, e.g.: rosrun atom_calibration create_calibration_pkg --name ~/my/path/<my_robot_calibration>","title":"Create a calibration package"},{"location":"procedures/#configure-a-calibration-package","text":"Once your calibration package is created you will have to configure the calibration procedure by editing the /calibration/config.yml file with your system information. The file contains several comments to provide clues on how to configure it. Here are examples of calibration config.yml files for an autonomous vehicle and for MMTBot , also shown below: # # \u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2557 # \u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u255a\u2550\u2550\u2588\u2588\u2554\u2550\u2550\u255d\u2588\u2588\u2554\u2550\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2551 # \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2551 \u2588\u2588\u2551 \u2588\u2588\u2551 \u2588\u2588\u2551\u2588\u2588\u2554\u2588\u2588\u2588\u2588\u2554\u2588\u2588\u2551 # \u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2551 \u2588\u2588\u2551 \u2588\u2588\u2551 \u2588\u2588\u2551\u2588\u2588\u2551\u255a\u2588\u2588\u2554\u255d\u2588\u2588\u2551 # __ \u2588\u2588\u2551 \u2588\u2588\u2551 \u2588\u2588\u2551 \u255a\u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u2588\u2588\u2551 \u255a\u2550\u255d \u2588\u2588\u2551 _ # / _| \u255a\u2550\u255d \u255a\u2550\u255d \u255a\u2550\u255d \u255a\u2550\u2550\u2550\u2550\u2550\u255d \u255a\u2550\u255d \u255a\u2550\u255d | | # | |_ _ __ __ _ _ __ ___ _____ _____ _ __| | __ # | _| '__/ _` | '_ ` _ \\ / _ \\ \\ /\\ / / _ \\| '__| |/ / # | | | | | (_| | | | | | | __/\\ V V / (_) | | | < # |_| |_| \\__,_|_| |_| |_|\\___| \\_/\\_/ \\___/|_| |_|\\_\\ # https://github.com/lardemua/atom # This yaml file describes your calibration! # You can start by defining your robotic system. # This is the URDF file (or xacro) that describes your robot. # Every time a path to a file is requested you can use # # - Absolute Path # Example 1: /home/user/ros_workspace/your_package/urdf/description.urdf.xacro # Example 2: file://home/user/ros_workspace/your_package/urdf/description.urdf.xacro # # - Path Expansion # Example 1: ${HOME}/user/${YOUR_VARIABLE}/your_package/urdf/description.urdf.xacro # Example 2: ~/user/ros_workspace/your_package/urdf/description.urdf.xacro # # NOTE: It is up to you to guarantee the environment variable exists. # # - ROS Package Reference # Example: package://your_package/urdf/description.urdf.xacro # description_file: \"package://mmtbot_gazebo/urdf/mmtbot.urdf.xacro\" #description_file: \"package://mmtbot_gazebo/urdf/mmtbot.urdf.xacro\" # The calibration framework requires a bagfile to extract the necessary data for the calibration. bag_file: \"$ROS_BAGS/mmtbot/test_depth2.bag\" # You must define a frame of reference for the optimization process. # It must exist in the transformation chains of all the sensors which are being calibrated. world_link: \"world\" # ATOM will calibrate the extrinsic parameters of your sensors. # In this section you should discriminate the sensors that will be part of the calibrations. sensors: # Each key will define a sensor and its name, which will be use throughout the calibration. # Each sensor definition must have the following properties: # link: # The frame of the sensor's data (i.e. the header.frame_id). # parent_link: # The parent link of the transformation (i.e. link) to be calibrated. # child_link: # This is the transformation (i.e. link) that we be optimized. # topic_name: # Name of the ROS topic that contains the data produced by this sensor. # If you are calibrating an camera, you should use the raw image produced by the # sensors. Additionally, it the topic is an image it will automatically use the # respective `camera_info` topic. hand_camera: link: \"hand_camera_rgb_optical_frame\" parent_link: \"flange\" child_link: \"hand_camera_link\" topic_name: \"/hand_camera/rgb/image_raw\" throttle: 5 modality: \"rgb\" world_camera_rgb: link: \"world_camera_rgb_optical_frame\" parent_link: \"world_camera_link\" child_link: \"world_camera_rgb_frame\" topic_name: \"/world_camera/rgb/image_raw\" modality: \"rgb\" lidar: link: \"lidar\" parent_link: \"tripod_left_support\" child_link: \"lidar_base_link\" topic_name: \"/lidar/points\" modality: \"lidar3d\" # The calibration requires a detectable pattern. # This section describes the properties of the calibration pattern used in th calibration. calibration_pattern: # The frame id (or link) of the pattern. # This link/transformation will be optimized. link: \"pattern_link\" # The parent frame id (or link) of the pattern. # For example, in hand-eye calibration the parent link # of the pattern can be the end-effector or the base of the arm parent_link: \"world\" # Defines if the pattern link is the same in all collections (i.e. fixed=true), # or each collection will have its own estimate of the link transformation. # Note: if you plan to have the pattern fixed, while the moving the rigidly attached sensors, # this is equivalent to having the sensors fixed and the pattern moving, so you should use fixed=false. fixed: false # The type of pattern used for the calibration. # Supported pattern are: chessboard, charuco pattern_type: \"charuco\" # If the pattern type is \"charuco\" you need to define # the aruco dictionary used by the pattern. # See https://docs.opencv.org/trunk/dc/df7/dictionary_8hpp.html dictionary: \"DICT_5X5_100\" # Mesh file (collada.dae or stl) for showing pattern on rviz. URI or regular path. mesh_file: \"package://mmtbot_gazebo/models/charuco_800x600/charuco_800x600.dae\" # The border width from the edge corner to the pattern physical edge. # Used for 3D sensors and lidars. # It can be a scalar (same border in x and y directions), or it can be {'x': ..., 'y': ,,,} border_size: { \"x\": 0.04, \"y\": 0.03 } # The number of corners the pattern has in the X and Y dimensions. # Note: The charuco detector uses the number of squares per dimension in its detector. # Internally we add a +1 to Y and X dimensions to account for that. # Therefore, the number of corners should be used even for the charuco pattern. dimension: { \"x\": 11, \"y\": 8 } # The length of the square edge. size: 0.06 # The length of the charuco inner marker. inner_size: 0.045 # Miscellaneous configuration # If your calibration problem is not fully constrained you should anchored one of the sensors. # This makes it immovable during the optimization. # This is typically referred to as gauge freedom. anchored_sensor: \"world_camera_rgb\" # Max time delta (in milliseconds) between sensor data messages when creating a collection. max_duration_between_msgs: 1000 After filling the config.yml file, you should run the package configuration: rosrun <my_robot_calibration> configure This will go through a series of verifications to check if the configuration is valid, if the bagfile exists and contains the necessary transformations, among many others. Once the verifications signal a correct calibration configuration, a set of files is automatically created inside your <my_robot>_calibration ros package, as shown below: \u251c<my_robot>_calibration \u251c\u2500\u2500 launch \u2502 \u251c\u2500\u2500 calibrate.launch \u2502 \u251c\u2500\u2500 collect_data.launch \u2502 \u251c\u2500\u2500 dataset_playback.launch \u2502 \u251c\u2500\u2500 playbag.launch \u2502 \u2514\u2500\u2500 set_initial_estimate.launch \u251c\u2500\u2500 rviz \u2502 \u251c\u2500\u2500 calibrate.rviz \u2502 \u251c\u2500\u2500 collect_data.rviz \u2502 \u251c\u2500\u2500 dataset_playback.rviz \u2502 \u2514\u2500\u2500 set_initial_estimate.rviz \u251c\u2500\u2500 scripts \u2502 \u2514\u2500\u2500 configure \u2514\u2500\u2500 urdf \u251c\u2500\u2500 initial_estimate.urdf.xacro \u251c\u2500\u2500 calibration \u2502 \u251c\u2500\u2500 config.yml \u2502 \u2514\u2500\u2500 summary.pdf The launch directory contains automatically created launch files used to launch each of the calibration stages. The rviz folder contains several rviz configuration files used to setup the visualization of each stage. The urdf folder contains a symbolic link to the xacro file of <my_robot> and, after calibration, will contain a calibrated urdf. Finally, the calibration folder contains the configuration file (config.yml). This folder also contains an automatically produced schematic of the configuration of the calibration named summary.pdf . It is used to inspect the configuration of the calibration and assess if the configuration is doing what we intended. Atomic transformations to be estimated are marked in blue, sensor coordinate systems, i.e. the coordinate systems in which sensors produce their data are marked in green, and the selected world coordinate frame is highlighted in red. Summary of a calibration configuration (MMTBot).","title":"Configure a calibration package"},{"location":"procedures/#recording-compressed-images","text":"In case the topics from RGB camera(s) are published using the ROS image transport , it is possible to record the compressed image topic instead of the raw image topic as in this example . During the configuration of your calibration package, ATOM will detect that these are compressed topics and automatically generate a decompressing mechanism in the launch files. The advantage is that you get much smaller bag files since they contain compressed images. This may be critical for large systems such as LARCC , where the data output may reach over 1GB per minute.","title":"Recording compressed images"},{"location":"procedures/#throttling-topics","text":"For an offline calibration procedure, speed is not critical. Thus, it is possible to configure a ROS topic throttler for your sensor messages, if they have been recorded in a higher than necessary frequency. For that just add a field throttle to the section of the corresponding sensor in your calibration config.yaml file.","title":"Throttling topics"},{"location":"procedures/#using-a-different-configuration-file","text":"It is also possible to configure your calibration package with a different configuration file, in the case you have multiple configurations with multiple config.yml files. There are also other options to run a custom configuration, i.e.: usage: rosrun <my_robot_calibration> configure -cfg CONFIG_FILE","title":"Using a different configuration file"},{"location":"procedures/#using-tfs-instead-of-the-xacro-file","text":"Sometimes it may be preferable to use the transformations in the bagfile instead of the ones produced by the xacro description. To do this use the --use_tfs option when configuring your package: usage: rosrun <my_robot_calibration> configure -utf Note A previous version of the configure script used a bash based script (currently it is python based), which may not have all the options listed. If the package was created some time ago, you may still have the bash based script. In that case you can update the configure script by running: rosrun atom_calibration update_configure_script -n <my_robot_calibration>","title":"Using tfs instead of the xacro file"},{"location":"procedures/#set-an-initial-estimate","text":"Iterative optimization methods such as ATOM are sensitive to the initial parameter configuration. ATOM uses several groups of optimization parameters and uses distinct strategies to initialize each class. The most important group, sensor poses, may be manually initialized to ensure that the initial estimate is plausible and thus reduce the likelihood of encountering a local minima. ATOM provides an interactive framework based on rviz which allows the user to set the pose of the sensors while having immediate visual feedback. The system configures an RViz interactive marker for each sensor to be calibrated, so that it is possible to move the sensor in RViz. During this procedure one may use several visual clues to better position the sensors, i.e. it is possible to see the CAD model of the sensors and the rest of the structure of the robotic system or one could observe the amount of overlap between sensors. To set an initial estimate run: roslaunch <my_robot_calibration> set_initial_estimate.launch Here are a couple of examples of setting the initial estimate: Setting initial estimate of sensor poses in the AgrobV2. Setting initial estimate of sensor poses in the AtlasCar2. Setting initial estimate of sensor poses in the IRIS UR10e.","title":"Set an initial estimate"},{"location":"procedures/#visualizing-sensor-frustums","text":"ATOM provides a way to visualize the frustums of RGB and Depth cameras. These may be useful to get a clue about the overlap between sensors, or the amount of coverage of a work volume. Below you can see the frustum of two rgb cameras. One of the cameras is positioned on the end-effector of the manipulator, and when it moves, so does its frustum. This functionality is only available in the set initial estimate stage. Visualizing frustum of RGB sensors in the MMTBot.","title":"Visualizing sensor frustums"},{"location":"procedures/#collect-data","text":"To run a system calibration, one requires data from the sensors collected at different time instants. We refer to these snapshots of data as collections , and a set of collections as an ATOM dataset . To collect data, use: roslaunch <my_robot_calibration> collect_data.launch output_folder:=$ATOM_DATASETS/<your_dataset_folder> The script launches a fully configured rviz window. The user observes the data playback and decides when a collection should be saved by clicking a green sphere that appears in the scene. How many collections do I need? The number of collections required to accurately calibrate a system varies according to the number, modality and positioning of the sensors. Empirically we found that a figure around 20 or 30 collections is usually sufficient for estimating an accurate calibration. That's about the same number of images you need when calibrating a stereo system with OpenCV. Of course this highly depends on your system and the amount of overlap there is between the sensors. What is a good bagfile for recording an ATOM dataset? Each collection in an ATOM dataset will contain data from all the sensors in the system. All this data is assumed to be captured at the reference time stamp of that collection. In other words, ATOM assumes that the data from the sensors in a collection is synchronized. There are some robotic systems where it is straightforward to implement synchronization mechanisms, e.g., image acquisition hardware triggers in stereo camera systems. However, these solutions are very specialized and cannot be relied upon for the general case of multi-sensor, multi-modal systems. For example, it is not so easy to synchronize the acquisition of images from a camera with range data from a LiDAR. The solution adopted in ATOM is to transfer the responsibility of selection of these moments when a collection should be recorded to the user. More often than not, sensor data is not synchronized. However, if the sensors are observing a static scene, it is possible to safely assume that the sensor data is \"virtually synchronized\" because, even if the data is collected at different instants in time, it will observe the same static scenario. To ensure this, what we do during the recording of the bagfile for calibration is to plan in advance the moments where we will record collections, and ensure that at those moments the scene has been static for a duration longer than the estimated largest possible sensor data time differential. In practice, in our bagfiles, we move the pattern from position A to position B (moving the pattern at normal speed), but then hold the pattern static in position B for some time, e.g 3 to 5 seconds, in anticipation that the collection will be recorded once these seconds elapse, a time in which is is safe to assume that the scene has remained static for a long enough duration. Here is an example of a calibration bagfile where the described motion of the calibration pattern is visible. Having the user holding the pattern static will always results is small displacements of the calibration pattern. Thus, for more complex systems, or when a high accuracy is required, we use a tripod to hold the pattern still . Should the pattern be held in a special pose? Short answer: Yes , if LiDAR or depth sensors are used the pattern should be held somewhat diagonally. Long answer: In order to calibrate LiDAR or depth sensor modalities ATOM uses a cost function which uses the lateral physical boundaries of the pattern as landmarks for guiding the calibration. If the pattern is held in a straight up vertical pose, the lateral alignment will be equally valued regardless of the height at which the pattern is held. This results in redundancy in the estimation of the pattern pose, which pollutes the calibration procedure and often ends up in non-accurate calibrations. ATOM tackles this with a simple solution: we avoid holding the calibration pattern in a straight up pose , and hold it in one (or more) diagonal pose(s) when recording calibration bagfiles. It is also possible to add additional parameters to configure several aspects of the script. See below all the options. Additional parameters for collect_data.launch Argument Function overwrite overwrites previous dataset without asking for confirmation bag_rate Defines the playback rate of the bagfile bag_start Start time for playback bag_file Name of bagfile to playback ssl A string to be evaluated that indicates if a sensor should be labelled. One example using all the parameters above: roslaunch <my_robot_calibration> collect_data.launch output_folder:=$ATOM_DATASETS/<your_dataset_folder> overwrite:=true bag_rate:=0.5 bag_start:=10 ssl:='lambda name: name in [\"s1\", \"s2\"]' When you launch the data collection script, it automatically starts data labeling processes adequate for each sensor in your robotic system. As such, the data is being continuously labeled as the bagfile is played. Depending on the modality of the sensors in the system the labeling may be automatic or fully automatic. Below we detail how each of the labelers operate.","title":"Collect data"},{"location":"procedures/#rgb-camera-labeling","text":"RGB cameras have a fully automatic pattern detection. ATOM uses off the shelf chessboard or charuco calibration pattern detectors. Also, it provides an rviz configuration which subscribes annotated images received from the pattern detectors. You can check if the detection is working by observing the overlays of top of the images. Automatic labeling of RGB images (IrisUA - UR10e). Use charuco boards Charuco boards are preferable to chessboard patterns, because of two main reasons: the first is that the charuco detection is more more efficient when compared to the chessboard detection; the second is that the charuco pattern is detected even if it is only partially visible in the image, which is very useful when the sensors in your system have small overlapping fields of view.","title":"RGB camera labeling"},{"location":"procedures/#3d-lidar-labeling","text":"3D Lidar labeling is a semi-automatic procedure. The idea is that the user moves an rviz marker close to where the pattern is present in the lidar point cloud. Setting the seed point in 3D Lidar data for semi-automatic labeling (MMTBot). After setting this seed position, the system continues to track the patterns pose over the next frames, even if it moves, as you can see below: Automatic 3D Lidar labeling and creating an ATOM dataset (AgrobV2). Tracking limitations The tracking procedure may fail if the pattern is too close to another object, as for example the ground plane. This can be solved by making sure the pattern is sufficiently far from all other objects, or during the dataset playback stage.","title":"3D Lidar labeling"},{"location":"procedures/#depth-camera-labeling","text":"The labeling of depth cameras is a semi-automatic procedure. It is done by clicking the depth image in rviz. The user should click somewhere inside the pattern, and then the system carries on the tracking of the pattern even if it moves. The user may reset the procedure by clicking again on the image. Labeling of depth data (LARCC). RViz fork required Learn about this here .","title":"Depth camera labeling"},{"location":"procedures/#2d-lidar-labeling","text":"The labeling of the 2D Lidars is very similar to the labeling of 3D Lidars. The user sets the seed point where the lidar points are observing the pattern, and then the pattern is tracked. Setting the seed point in 2D Lidar data for semi-automatic labeling (AtlasCar2). May be deprecated The 2D Lidar semi-automatic labeling was last used in 2019, so it may be deprecated. If you are interested on having this functionality create an issue with a request.","title":"2D Lidar labeling"},{"location":"procedures/#dataset-playback","text":"The dataset playback is used to review and eventually correct the automatic labels produced during the collection of data . Ideally, the bulk of the labels produced automatically during the collect data stage should be correct, but a few incorrect annotations will disrupt the calibration. As such, a review of the annotations is recommended. You may skip it if you feel that the automatic labeling went very well. To run the dataset playback, first launch the visualization: roslaunch <my_robot_calibration> dataset_playback.launch and then the dataset_playback node: clear && rosrun atom_calibration dataset_playback -json <my_dataset_file>.json -ow This will create a new json file called .json. There are other available options listed below: usage: dataset_playback [-h] -json JSON_FILE [-csf COLLECTION_SELECTION_FUNCTION] [-ow] optional arguments: -h, --help show this help message and exit -json JSON_FILE, --json_file JSON_FILE Json file containing input dataset. -csf COLLECTION_SELECTION_FUNCTION, --collection_selection_function COLLECTION_SELECTION_FUNCTION A string to be evaluated into a lambda function that receives a collection name as input and returns True or False to indicate if the collection should be loaded (and used in the optimization). The Syntax is lambda name: f(x), where f(x) is the function in python language. Example: lambda name: int(name) > 5 , to load only collections 6, 7, and onward. -ow, --overwrite Overwrites the data_corrected.json without asking for permission Check the video tutorial . Navigating through collections in a dataset (LARCC).","title":"Dataset playback"},{"location":"procedures/#correcting-3d-lidar-labels","text":"Correcting 3D Lidar labels is done by selecting points in the point cloud displayed by RViz and pressing keys in order to add these points as pattern points or boundary points. Correcting 3D Lidar labels in a dataset (LARCC). Check the video tutorial . Do not forget to compile your catkin workspace For selecting points from point clouds, we use an rviz plugin in cpp that must be compiled.","title":"Correcting 3D Lidar labels"},{"location":"procedures/#correcting-depth-labels","text":"To correct depth modality labels the user draws a polygon around the pattern in the depth image. Correcting depth labels in a dataset (LARCC). Check the video tutorial . RViz fork required Learn about this here .","title":"Correcting Depth labels"},{"location":"procedures/#calibrate","text":"Finally, a system calibration is called through: roslaunch <my_robot_calibration> calibrate.launch Then, in a second terminal, run the calibrate script: rosrun atom_calibration calibrate -json $ATOM_DATASETS/<my_robot_dataset>/dataset_corrected.json -v -rv -si There are several options to use in the calibrate script, one common usage is: rosrun atom_calibration calibrate -json $ATOM_DATASETS/<my_robot_dataset>/dataset_corrected.json -v -rv -si -uic -csf 'lambda x: int(x)< 5' -ssf 'lambda name: name in [\"camera_2\",\"camera_3\"]' which would run a calibration in verbose mode (-v), using ros visualization (-rv), showing images (-si), using incomplete collections, using collections with index smaller than 5, considering only sensors camera_2 and camera_3. You can see all the options listed below: usage: calibrate [-h] [-vo] -json JSON_FILE [-v] [-rv] [-si] [-oi] [-sr SAMPLE_RESIDUALS] [-ss SAMPLE_SEED] [-slr SAMPLE_LONGITUDINAL_RESIDUALS] [-ajf] [-oas] [-ap] [-uic] [-ias] [-rpd] [-nig translation rotation] [-ssf SENSOR_SELECTION_FUNCTION] [-csf COLLECTION_SELECTION_FUNCTION] [-phased] [-ipg] [-oj OUTPUT_JSON] optional arguments: -h, --help show this help message and exit -vo, --view_optimization ... -json JSON_FILE, --json_file JSON_FILE Json file containing input dataset. -v, --verbose Be verbose -rv, --ros_visualization Publish ros visualization markers. -si, --show_images shows images for each camera -oi, --optimize_intrinsics Adds camera intrinsics to the optimization -sr SAMPLE_RESIDUALS, --sample_residuals SAMPLE_RESIDUALS Samples residuals -ss SAMPLE_SEED, --sample_seed SAMPLE_SEED Sampling seed -slr SAMPLE_LONGITUDINAL_RESIDUALS, --sample_longitudinal_residuals SAMPLE_LONGITUDINAL_RESIDUALS Samples residuals -ajf, --all_joints_fixed Assume all joints are fixed and because of that draw a single robot mesh.Overrides automatic detection of static robot. -oas, --only_anchored_sensor Runs optimization only using the anchored sensor and discarding all others. -ap, --anchor_patterns Runs optimization without changing the poses of the patterns. -uic, --use_incomplete_collections Remove any collection which does not have a detection for all sensors. -ias, --ignore_anchored_sensor Ignore the anchored sensor information in the dataset. -rpd, --remove_partial_detections Remove detected labels which are only partial.Used or the Charuco. -nig translation rotation, --noisy_initial_guess translation rotation Percentage of noise to add to the initial guess atomic transformations set before. -ssf SENSOR_SELECTION_FUNCTION, --sensor_selection_function SENSOR_SELECTION_FUNCTION A string to be evaluated into a lambda function that receives a sensor name as input and returns True or False to indicate if the sensor should be loaded (and used in the optimization). The Syntax is lambda name: f(x), where f(x) is the function in python language. Example: lambda name: name in [\"left_laser\", \"frontal_camera\"] , to load only sensors left_laser and frontal_camera -csf COLLECTION_SELECTION_FUNCTION, --collection_selection_function COLLECTION_SELECTION_FUNCTION A string to be evaluated into a lambda function that receives a collection name as input and returns True or False to indicate if the collection should be loaded (and used in the optimization). The Syntax is lambda name: f(x), where f(x) is the function in python language. Example: lambda name: int(name) > 5 , to load only collections 6, 7, and onward. -phased, --phased_execution Stay in a loop before calling optimization, and in another after calling the optimization. Good for debugging. -ipg, --initial_pose_ghost Draw a ghost mesh with the systems initial pose. Good for debugging. -oj OUTPUT_JSON, --output_json OUTPUT_JSON Full path to output json file. If you use the --verbose option, the script will periodically print a table containing information about the errors per sensor and per collection, e.g.: Errors per collection (anchored sensor, max error per sensor, not detected as \"---\") +------------+----------+----------+----------+----------------+---------+---------+---------+ | Collection | camera_2 | camera_3 | camera_4 | depth_camera_1 | lidar_1 | lidar_2 | lidar_3 | +------------+----------+----------+----------+----------------+---------+---------+---------+ | 11 | 191.1838 | --- | 0.3000 | 0.1321 | 0.2606 | 0.1153 | 1.4850 | | 12 | 194.3039 | 98.7729 | 0.4031 | 0.1591 | 0.1043 | 0.0924 | 0.5050 | | 13 | --- | 94.6346 | 0.3795 | 0.1942 | 0.1657 | 0.1191 | 0.4210 | | 14 | 199.6314 | --- | 0.4001 | 0.1569 | 0.1897 | 0.0685 | 0.7067 | | 15 | 199.8989 | --- | 0.5995 | 0.1561 | 0.1956 | 0.0670 | 0.7074 | | 16 | --- | 146.3350 | 0.2370 | 0.1516 | 0.4412 | 0.1316 | 0.3175 | | 17 | --- | 150.6509 | 0.2204 | 0.1924 | 0.1747 | 0.0298 | 0.4698 | | 18 | 202.8877 | --- | 0.5371 | 0.0978 | 0.2200 | 0.1526 | 0.8007 | | 19 | 211.7066 | 133.3410 | 0.3598 | 0.2179 | 0.0909 | 0.0367 | 0.6065 | | 20 | 213.7645 | --- | 0.5272 | 0.2152 | 0.2647 | 0.2008 | 0.7670 | | 21 | 212.6482 | --- | 0.6358 | 0.2444 | 0.3512 | 0.0518 | 1.7215 | | 22 | 212.3743 | --- | 0.5967 | 0.2321 | 0.3342 | 0.0591 | 0.7347 | | 23 | 209.1305 | --- | 0.4488 | 0.1161 | 0.1601 | 0.0431 | 0.7198 | | 24 | 201.1378 | 139.1949 | 0.2539 | 0.1802 | 0.1114 | 0.0517 | 0.5524 | | 25 | --- | 139.2066 | 0.2994 | 0.0869 | 0.4260 | 0.1768 | 0.1162 | | 26 | 209.7005 | --- | 0.4192 | 0.1315 | 0.1400 | 0.0443 | 0.6932 | | 27 | 210.5517 | --- | 0.4042 | 0.2189 | 0.2926 | 0.0495 | 0.7068 | | 28 | --- | 142.4085 | 0.2628 | 0.1500 | 0.2662 | 0.1419 | 0.3405 | | Averages | 205.3015 | 130.5680 | 0.4047 | 0.1685 | 0.2327 | 0.0907 | 0.6873 | +------------+----------+----------+----------+----------------+---------+---------+---------+ Here's an example of a system being calibrated. Calibration of AgrobV2.","title":"Calibrate"},{"location":"procedures/#calibrating-intrinsic-parameters","text":"ATOM also supports intrinsic camera calibration (for now just RGB modality), but requires a first guess for these parameters. We compute a first guess using the ROS monocular camera calibration for rgb cameras, and the depth camera intrinsic calibration for depth cameras. After these calibrations are carried out, the sensor drivers publish calibrated camera_info messages which are saved to the bag files that is used for calibration, and then used a as first guess for ATOM's intrinsic parameter estimation. Intrinsic calibration runs simultaneous with the extrinsic calibration.","title":"Calibrating intrinsic parameters"},{"location":"procedures/#two-stage-calibration-for-robotic-systems-with-an-anchored-sensor","text":"When one sensor is set to be anchored in the calibration/config.yml file, i.e. this file for the AtlaCar2, we recommend a two stage procedure to achieve a more accurate calibration: First, run a calibration using parameter --only_anchored_sensor ( -oas ) which will exclude from the optimization all sensors which are not the anchored one. This optimization will position the patterns correctly w.r.t. the anchored sensor. For example: rosrun atom_calibration calibrate -json $ATOM_DATASETS/larcc_real/ dataset_train/dataset_corrected.json -uic -nig 0.0 0.0 -ipg -si -rv -v -oas The output is stored in the atom_calibration.json , which is used and the input for the second stage, where all sensors are used. In this second stage the poses of the patterns are frozen using the parameter --anchor_patterns ( -ap ). To avoid overwriting atom_calibration.json, you should also define the output json file ( -oj ). For example: rosrun atom_calibration calibrate -json $ATOM_DATASETS/larcc_real/ dataset_train/atom_calibration.json -uic -nig 0.0 0.0 -ipg -si -rv -v -ap -oj atom_anchored_calibration.json","title":"Two stage calibration for robotic systems with an anchored sensor"}]}