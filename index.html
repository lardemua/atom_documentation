<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="description" content="None" /><link rel="canonical" href="https://lardemua.github.io/atom_documentation/" />
      <link rel="shortcut icon" href="img/favicon.ico" />
    <title>ATOM Calibration Framework</title>
    <link rel="stylesheet" href="css/theme.css" />
    <link rel="stylesheet" href="css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/styles/github.min.css" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "Introduction";
        var mkdocs_page_input_path = "index.md";
        var mkdocs_page_url = "/atom_documentation/";
      </script>
    
    <script src="js/jquery-3.6.0.min.js" defer></script>
    <!--[if lt IE 9]>
      <script src="js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/highlight.min.js"></script>
      <script>hljs.initHighlightingOnLoad();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href="." class="icon icon-home"> ATOM Calibration Framework
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="./search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
                <li class="toctree-l1 current"><a class="reference internal current" href=".">Introduction</a>
    <ul class="current">
    <li class="toctree-l2"><a class="reference internal" href="#what-is-atom">What is ATOM?</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#calibration-pipeline">Calibration Pipeline</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#robotic-system-configuration">Robotic System Configuration</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#data-logging">Data Logging</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#initial-positioning-of-sensors">Initial Positioning of Sensors</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#data-collection-and-labeling">Data Collection and Labeling</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#dataset-playback">Dataset playback</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#calibration">Calibration</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#calibration-evaluation">Calibration Evaluation</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#running-the-calibrated-system">Running the calibrated system</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#what-is-a-label">What is a label?</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#what-is-a-collection">What is a collection?</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#what-is-an-atom-dataset">What is an ATOM dataset?</a>
    </li>
    </ul>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="getting_started/">Getting Started</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="procedures/">Calibration Procedures</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="examples/">Calibration Examples</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="evaluations/">Evaluation Procedures</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="multimedia/">Multimedia</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="acknowledgment/">Acknowledgment</a>
                </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href=".">ATOM Calibration Framework</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="." class="icon icon-home" alt="Docs"></a> &raquo;</li><li>Introduction</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>

          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <figure align="center">
<p><img alt="Image title" src="img/atom_logo.png" width="60%" />
  </p>
<figcaption align="center"></figcaption>
</figure>
<h2 id="atom">ATOM</h2>
<ul>
<li><a href="#what-is-atom">What is ATOM?</a></li>
<li><a href="#calibration-pipeline">Calibration Pipeline</a><ul>
<li><a href="#robotic-system-configuration">Robotic System Configuration</a></li>
<li><a href="#data-logging">Data Logging</a></li>
<li><a href="#initial-positioning-of-sensors">Initial Positioning of Sensors</a></li>
<li><a href="#data-collection-and-labeling">Data Collection and Labeling</a></li>
<li><a href="#dataset-playback">Dataset playback</a></li>
<li><a href="#calibration">Calibration</a></li>
<li><a href="#calibration-evaluation">Calibration Evaluation</a></li>
<li><a href="#running-the-calibrated-system">Running the calibrated system</a></li>
</ul>
</li>
<li><a href="#what-is-a-label">What is a label?</a></li>
<li><a href="#what-is-a-collection">What is a collection?</a></li>
<li><a href="#what-is-an-atom-dataset">What is an ATOM dataset?</a></li>
</ul>
<h3 id="what-is-atom">What is ATOM?</h3>
<p><a href="https://github.com/lardemua/atom">ATOM</a> is a calibration framework using the <strong>A</strong>tomic <strong>T</strong>ransformations <strong>O</strong>ptimization <strong>M</strong>ethod. </p>
<p align="center">
<a href="https://github.com/lardemua/atom">https://github.com/lardemua/atom</a>
</p>

<p>It contains a set of calibration tools for multi-sensor, multi-modal, robotic systems, based on the optimization of atomic transformations, as provided by a <a href="https://www.ros.org/">ROS</a> based robot description. Moreover, it provides several scripts to facilitate all the steps of a calibration procedure.</p>
<!-- ![type:video](https://www.youtube.com/watch?v=4B3X_NsX89M&list=PLQN09mzV5mbI4h5IQt3Eu9kugSk-08mnY&index=8) -->

<p>If this work is helpful for you please cite <a href="acknowledgment/#citing-atom">our publications</a>.</p>
<h3 id="calibration-pipeline">Calibration Pipeline</h3>
<p>In order to calibrate a robotic system one needs to carry out several tasks, such as acquiring data, labeling data, executing the calibration, interpreting the result of the calibration, etc.
ATOM provides several scripts to address all the stages of a calibration procedure. 
These scripts are seamlessly integrated into <a href="https://www.ros.org/">ROS</a>, and make use of <a href="http://wiki.ros.org/rviz">RViz</a> to provide extensive visualization functionalites.
We have divided the calibration procedure into several stages, shown in the scheme below.</p>
<figure align="center">
<p><img alt="Image title" src="img/architecture/atom_architecture.png" width="80%" />
  </p>
<figcaption align="center">ATOM calibration pipeline.</figcaption>
</figure>
<p>The <span style="color:gray">greyed out boxes </span> are steps considered to be out of the scope of ATOM, i.e., these are tasks one should do in order to properly configure a robotic system in ROS, even if ATOM is not going to be used. Dashed line boxes represent steps which are optional, i.e., they may improve the calibration but are not essential to the procedure. </p>
<p>Below we describe each of these stages, giving examples for the <a href="examples/#mmtbot">MMTBot</a>.</p>
<h4 id="robotic-system-configuration">Robotic System Configuration</h4>
<p>Robotic System Configuration concerns the design and implementation of your robotic system in ROS. It generally involves the writing of an <a href="http://wiki.ros.org/urdf">UDRF</a> or a <a href="http://wiki.ros.org/xacro">xacro</a> file that describes the links and joints of your robotic system, among other things.
If you are not familiar with this there are ROS tutorials to <a href="http://wiki.ros.org/urdf/Tutorials/Building%20a%20Visual%20Robot%20Model%20with%20URDF%20from%20Scratch">build your robot URDF</a> and also to <a href="http://wiki.ros.org/urdf/Tutorials/Using%20Xacro%20to%20Clean%20Up%20a%20URDF%20File">create your robot xacro</a>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>To calibrate your robot with ATOM, we recommend using xacro files instead of urdfs. </p>
</div>
<p>This stage may also include de configuration of a simulation of your system in <a href="https://gazebosim.org/home">Gazebo</a>.</p>
<p>Typically one creates a couple of ros packages for our robot, as described below.</p>
<p><strong>&lt;my_robot>_description</strong> ros package contains the xacro files that describe your robot. It sometimes also contains cad models inside a models folder. An example from <a href="https://github.com/miguelriemoliveira/mmtbot/tree/main/mmtbot_description">MMTBot is here</a>.</p>
<p><strong>&lt;my_robot>_bringup</strong> ros package contains the launch files used to bringup your robotic system. Tipically there is a <strong>bringup.launch</strong> that starts the complete system.
An example from the <a href="https://github.com/miguelriemoliveira/mmtbot/blob/main/mmtbot_bringup/launch/bringup.launch">MMTBot</a>.</p>
<h4 id="data-logging">Data Logging</h4>
<p>Data Logging is the procedure by which a <a href="http://wiki.ros.org/Bags">ros bagfile</a> is recorded to be used in the calibration later on.</p>
<p>You may record data by <a href="http://wiki.ros.org/rosbag/Tutorials/Recording%20and%20playing%20back%20data">calling rosbag record</a> directly, e.g.:</p>
<pre><code>rosbag record /topic1 ... /topicN -o output.bag
</code></pre>
<p>We tipically have a roslaunch file to record data and produce a bagfile for each robotic system. Here's the example for <a href="https://github.com/miguelriemoliveira/mmtbot/blob/main/mmtbot_bringup/launch/record.launch">MMTBot</a>.</p>
<p>A bagfile should contain several topics, namely transformations and joint state messages, as well as messages produced by the sensors in the system. For example, in the case of MMTBot, which has sensors <strong>world_camera</strong>, <strong>hand_camera</strong> and <strong>lidar</strong>, we record the following topics:</p>
<ul>
<li>/tf</li>
<li>/tf_static</li>
<li>/joint_states</li>
<li>/world_camera/rgb/image_raw/compressed</li>
<li>/world_camera/rgb/camera_info</li>
<li>/hand_camera/rgb/image_raw/compressed</li>
<li>/hand_camera/rgb/camera_info</li>
<li>/lidar/points</li>
</ul>
<h4 id="initial-positioning-of-sensors">Initial Positioning of Sensors</h4>
<p>The goal of this stage is to allow the user to define interactively the poses of each sensor, so that the optimization starts close to the optimal solution and thus avoids local minima. 
This stage may be skiped if the transformations from the URDF are believed to be "sufficiently" accurate.</p>
<p>More details <a href="procedures/#set-an-initial-estimate">here</a>.</p>
<h4 id="data-collection-and-labeling">Data Collection and Labeling</h4>
<p>This stage reads the bagfile and allows the user to assist the labeling procedure, i.e., the identification of the calibration pattern in the data of the sensors, and then to decide when to save each collection. The output is an <a href="#what-is-an-atom-dataset">ATOM dataset</a>.</p>
<p>More details <a href="procedures/#collect-data">here</a>.</p>
<h4 id="dataset-playback">Dataset playback</h4>
<p>This stage is used to review ATOM datasets. The reviewing may identify incorrect labels, which can be corrected through manual annotation. It produces a corrected <a href="#what-is-an-atom-dataset">ATOM dataset</a>.</p>
<p>More details <a href="procedures/#dataset-playback">here</a>.</p>
<h4 id="calibration">Calibration</h4>
<p>This is were finally the system is calibrated. ATOM provides extensive visualization functionalities so that it is possible to observe how the calibration is performing.</p>
<p>More details <a href="procedures/#calibrate">here</a>.</p>
<h4 id="calibration-evaluation">Calibration Evaluation</h4>
<p>ATOM provides several scripts designed to measure the accuracy of the calibration. 
These tools are pairwise evaluations, which means it is possible to compare the accuracy of ATOM with other state of the art pairwise approaches.</p>
<p>More details <a href="evaluations/">here</a>.</p>
<h4 id="running-the-calibrated-system">Running the calibrated system</h4>
<p>After calibration ATOM produces a calibrated URDF file which can be direcly used in ROS.</p>
<p>If you get here unscathed, <span style="color:orange">you are a very lucky person :-) </span> ... Enjoy!</p>
<h3 id="what-is-a-label">What is a label?</h3>
<p>A label is a set of data points selected from the complete sensor data, obtained through a labeling or pattern detection procedure.
The representation of a label differs according to the sensor modality, as detailed next.</p>
<p>RGB camera labels are a list of image pixel coordinates of the corners of the pattern.</p>
<figure align="center">
<p><img alt="Image title" src="img/rgb_labels.png" width="60%" />
  </p>
<figcaption align="center">Labels for RGB modality.</figcaption>
</figure>
<p>3D Lidar labels contain the list of 3D point coordinates of the Lidar data points that belong to the pattern.
In addition to this, 3D Lidar labels also contain a list of 3D points of the physical boundaries of the pattern.</p>
<figure align="center">
<p><img alt="Image title" src="img/lidar3d_labels.png" width="50%" />
  </p>
<figcaption align="center">Labels for the 3D Lidar modality. Small green spheres are the pattern support points, larger green spheres are the points annotated as boundaries of the pattern.</figcaption>
</figure>
<p>Depth modality labels contain a list of image pixel coordinates that are annotated to be inside the pattern. In addition to this, depth labels also contain a set of pixel coordinates of the boundaries of the pattern.</p>
<figure align="center">
<p><img alt="Image title" src="img/depth_label.png" width="50%" />
  </p>
<figcaption align="center">Labels for the depth modality. Pattern support points in yellow, and boundaries in pink.</figcaption>
</figure>
<h3 id="what-is-a-collection">What is a collection?</h3>
<p>A collection is a recording of the data from all the sensors in the system at a particular time instant selected by the user. Collections also contain information about labels for the sensor data, as well as the state of the robotic system at that time, i.e., all the transformations.</p>
<figure align="center">
<p><img alt="Image title" src="img/ATOMCollection.png" width="100%" />
  </p>
<figcaption align="center">Sensor data from an MMTBot collection.</figcaption>
</figure>
<div class="admonition info">
<p class="admonition-title">Incomplete collections</p>
<p>A collection is referred to as an <strong>incomplete collection</strong> when it is not possible to detect the pattern in at least one of the sensors of the robotic system.</p>
</div>
<h3 id="what-is-an-atom-dataset">What is an ATOM dataset?</h3>
<p>An ATOM dataset is a folder which contains data used for the calibration of a robotic system. Every ATOM dataset contains a <strong>dataset.json</strong> which provides details about the dataset, such as the defined configuration, the number of sensors, etc. </p>
<p>Several scripts in the calibration pipeline require an ATOM dataset, but is worth mentioning that the files are also human readable. </p>
<p>Below you can see the structure of an ATOM dataset. </p>
<figure align="center">
<p><img alt="Image title" src="img/ATOMDataset_tree.png" width="60%" />
  </p>
<figcaption align="center">Structure of an ATOM dataset json file.</figcaption>
</figure>
<p>A <strong>dataset.json</strong> file contains a <strong>_metadata</strong> field, where details about the date, user and others are stored. It also contains a <strong>calibration_config</strong>, which is a copy of the state of the configuration file at the time of creation of the dataset. The <strong>sensors</strong> field describes each of the sensors in the system, in particular those selected to be calibrated. </p>
<p>Finally, the <strong>collections</strong> field contains several collections, i.e. snapshots of data. Each collection contains a subfield <strong>data</strong>, that stores a dictionary obtained through the <a href="http://wiki.ros.org/rospy_message_converter">conversion of the ROS message to a python dictionary</a>, the subfield <strong>labels</strong> contains information about annotations (automatic or manual) of each sensor data, and the subfield <strong>transforms</strong> contains all the transformations published at (or near) the time of the collection.</p>
<p>In addition to the <strong>dataset.json</strong> file, ATOM datasets also contain dedicated files for larger data blobs, such as point clouds or images, which are saved separately in the same folder.</p>
<p>Because the transformations are stored for each collection, it is possible to recover the complete state of the robotic system at the time of each collection. 
ATOM then provides visualization functionalities to display all collections at once. Below we can see the different poses of the manipulator and the calibration pattern for each collection of an MMTBot dataset.</p>
<figure align="center">
<p><img alt="Image title" src="img/mmtbot_multiple_collections4.png" width="90%" />
  </p>
<figcaption align="center">Several collections in an MMTBot dataset.</figcaption>
</figure>
<p>Here is an <a href="https://jsoneditoronline.org/#left=cloud.d9efaa274cb44579ad73553dea513ed8">ATOM dataset example from LARCC</a>.</p>
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="getting_started/" class="btn btn-neutral float-right" title="Getting Started">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
    
      <span><a href="getting_started/" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script>var base_url = '.';</script>
    <script src="js/theme_extra.js" defer></script>
    <script src="js/theme.js" defer></script>
      <script src="search/main.js" defer></script>
    <script defer>
        window.onload = function () {
            SphinxRtdTheme.Navigation.enable(true);
        };
    </script>

</body>
</html>

<!--
MkDocs version : 1.3.0
Build Date UTC : 2022-07-02 05:40:42.340197+00:00
-->
